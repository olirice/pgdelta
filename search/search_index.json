{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pgdelta","text":"<p>A PostgreSQL schema differ and DDL generator that produces high-fidelity schema migrations.</p> <p>pgdelta is designed to generate precise, dependency-aware DDL migrations by comparing PostgreSQL schemas. It uses a three-phase approach (Extract \u2192 Diff \u2192 Generate) to ensure correctness and maintains roundtrip fidelity.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>High-fidelity migrations: Generates DDL that recreates schemas exactly</li> <li>Dependency resolution: Automatic ordering of DDL statements based on PostgreSQL dependencies</li> <li>Roundtrip fidelity: Extract \u2192 Diff \u2192 Generate \u2192 Apply cycles produce identical schemas</li> <li>Type-safe: Complete mypy coverage with structural pattern matching</li> <li>Real database testing: All tests use actual PostgreSQL instances via testcontainers</li> </ul>"},{"location":"#development-status","title":"Development Status","text":"<p>pgdelta is currently in early development (v0.1.0).</p> <p>The project currently supports basic schema and table operations with comprehensive constraint and index support planned for upcoming releases.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<p>Note: pgdelta is not yet published to PyPI. Install from source:</p> <pre><code>git clone https://github.com/olirice/pgdelta.git\ncd pgdelta\npip install -e \".[dev]\"\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from pgdelta import PgCatalog, generate_sql\nfrom pgdelta.catalog import extract_catalog\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session\n\n# Connect to databases\nsource_engine = create_engine(\"postgresql://user:pass@localhost/source_db\")\ntarget_engine = create_engine(\"postgresql://user:pass@localhost/target_db\")\n\nwith Session(source_engine) as source_session, Session(target_engine) as target_session:\n    # Extract schemas\n    source_catalog = extract_catalog(source_session)\n    target_catalog = extract_catalog(target_session)\n\n    # Generate migration from target to source\n    changes = target_catalog.diff(source_catalog)\n\n    # Generate SQL statements\n    sql_statements = [generate_sql(change) for change in changes]\n\n    for sql in sql_statements:\n        print(sql)\n</code></pre>"},{"location":"#example-output","title":"Example Output","text":"<pre><code>CREATE SCHEMA \"analytics\";\nCREATE TABLE \"analytics\".\"user_stats\" (\n  \"user_id\" integer,\n  \"post_count\" integer DEFAULT 0,\n  \"last_login\" timestamp without time zone\n);\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>pgdelta uses a three-phase approach designed for correctness and testability:</p>"},{"location":"#phase-1-extract","title":"Phase 1: Extract","text":"<ul> <li>SQL-only access: Database connections used exclusively during extraction</li> <li>Immutable snapshots: One-time catalog extraction into frozen dataclasses</li> <li>Field metadata: Distinguishes identity, data, and internal fields for semantic comparison</li> </ul>"},{"location":"#phase-2-diff","title":"Phase 2: Diff","text":"<ul> <li>Dependency resolution: Uses NetworkX to determine correct DDL ordering</li> <li>Change type inversion: Dependencies are inverted based on operation type (CREATE vs DROP)</li> <li>Pure comparison: No database access, operates on immutable snapshots</li> </ul>"},{"location":"#phase-3-generate","title":"Phase 3: Generate","text":"<ul> <li>Pure functions: SQL generation from change objects with no side effects</li> <li>Deterministic output: Same input always produces identical DDL</li> <li>Type-safe: Complete mypy coverage with structural pattern matching</li> </ul>"},{"location":"#whats-supported","title":"What's Supported","text":""},{"location":"#currently-supported","title":"Currently Supported","text":"<ul> <li>\u2705 Schemas: CREATE/DROP operations</li> <li>\u2705 Tables: CREATE/DROP/ALTER operations with full column support</li> <li>\u2705 Constraints: Primary keys, unique, foreign keys, check, exclusion constraints</li> <li>\u2705 Indexes: Complete index creation and deletion (all types, partial, functional)</li> <li>\u2705 Views: CREATE/DROP/REPLACE operations</li> <li>\u2705 Materialized Views: CREATE/DROP operations</li> <li>\u2705 Functions &amp; Procedures: Full lifecycle management including CREATE OR REPLACE</li> <li>\u2705 Triggers: CREATE/DROP operations</li> <li>\u2705 Sequences: CREATE/DROP operations with ownership tracking, ALTER SEQUENCE OWNED BY</li> <li>\u2705 Types: Enum, composite, and domain type support (CREATE/DROP)</li> <li>\u2705 RLS Policies: CREATE/DROP/ALTER policy management</li> <li>\u2705 Dependency Resolution: Constraint-based dependency ordering</li> <li>\u2705 Roundtrip Fidelity: Extract \u2192 Diff \u2192 Generate \u2192 Apply verification</li> </ul>"},{"location":"#planned-features","title":"Planned Features","text":"<ul> <li>\ud83d\udd04 ALTER operations: ALTER SEQUENCE, ALTER TYPE, ALTER FUNCTION/PROCEDURE, ALTER TRIGGER</li> <li>\ud83d\udd04 Schema modifications: ALTER SCHEMA operations</li> <li>\ud83d\udd04 View enhancements: RECURSIVE views, explicit column names, WITH CHECK OPTION</li> <li>\ud83d\udd04 Trigger management: ENABLE/DISABLE TRIGGER</li> <li>\ud83d\udd04 Security features: CREATE ROLE, GRANT/REVOKE, ALTER DEFAULT PRIVILEGES</li> <li>\ud83d\udd04 Metadata: Comments on objects</li> <li>\ud83d\udd04 Advanced features: Event triggers, Extensions</li> <li>\ud83d\udd04 Partitioning: Table partitioning support</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Understand the architecture</li> <li>Explore the API reference</li> <li>See supported entities</li> <li>Learn about dependency resolution</li> <li>Contribute to the project</li> </ul>"},{"location":"#license","title":"License","text":"<p>Apache 2.0 - see LICENSE file for details.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>pgdelta is designed around a three-phase architecture that separates concerns and ensures correctness through pure functions and immutable data structures.</p>"},{"location":"architecture/#design-principles","title":"Design Principles","text":""},{"location":"architecture/#1-pure-functions","title":"1. Pure Functions","text":"<p>All core logic uses pure functions with no side effects: - Extract: Read-only database operations - Diff: Pure comparison functions - Generate: Deterministic SQL generation</p>"},{"location":"architecture/#2-immutable-data","title":"2. Immutable Data","text":"<p>Once extracted, all data is immutable: - Catalogs: Frozen dataclasses that cannot be modified - Models: Immutable representations of PostgreSQL objects - Changes: Immutable change objects</p>"},{"location":"architecture/#3-separation-of-concerns","title":"3. Separation of Concerns","text":"<p>Each phase has a single responsibility: - Extract: Database interaction and data marshalling - Diff: Semantic comparison and change detection - Generate: SQL generation and dependency resolution</p>"},{"location":"architecture/#4-type-safety","title":"4. Type Safety","text":"<p>Complete type safety throughout the system: - mypy: 100% type coverage - Structural pattern matching: Type-safe dispatch - Generics: Type-safe collections</p>"},{"location":"architecture/#three-phase-architecture","title":"Three-Phase Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                                    pgdelta                                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502  \u2502    Phase 1      \u2502    \u2502    Phase 2      \u2502    \u2502    Phase 3      \u2502            \u2502\n\u2502  \u2502    Extract      \u2502\u2500\u2500\u2500\u25b6\u2502      Diff       \u2502\u2500\u2500\u2500\u25b6\u2502    Generate     \u2502            \u2502\n\u2502  \u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502            \u2502\n\u2502  \u2502 \u2022 Database      \u2502    \u2502 \u2022 Semantic      \u2502    \u2502 \u2022 SQL           \u2502            \u2502\n\u2502  \u2502   Connections   \u2502    \u2502   Comparison    \u2502    \u2502   Generation    \u2502            \u2502\n\u2502  \u2502 \u2022 Catalog       \u2502    \u2502 \u2022 Change        \u2502    \u2502 \u2022 Dependency    \u2502            \u2502\n\u2502  \u2502   Extraction    \u2502    \u2502   Detection     \u2502    \u2502   Resolution    \u2502            \u2502\n\u2502  \u2502 \u2022 Immutable     \u2502    \u2502 \u2022 Pure          \u2502    \u2502 \u2022 Pure          \u2502            \u2502\n\u2502  \u2502   Snapshots     \u2502    \u2502   Functions     \u2502    \u2502   Functions     \u2502            \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502                                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#phase-1-extract","title":"Phase 1: Extract","text":""},{"location":"architecture/#purpose","title":"Purpose","text":"<p>Extract PostgreSQL schema information into immutable dataclasses.</p>"},{"location":"architecture/#key-components","title":"Key Components","text":""},{"location":"architecture/#catalog-extraction","title":"Catalog Extraction","text":"<pre><code>@dataclass(frozen=True)\nclass PgCatalog:\n    \"\"\"Immutable PostgreSQL catalog snapshot.\"\"\"\n\n    namespaces: dict[str, PgNamespace]      # Schemas\n    classes: dict[str, PgClass]             # Tables, views, etc.\n    attributes: dict[str, PgAttribute]      # Columns\n    constraints: dict[str, PgConstraint]    # Constraints\n    indexes: dict[str, PgIndex]             # Indexes\n    sequences: dict[str, PgSequence]        # Sequences\n    policies: dict[str, PgPolicy]           # RLS policies\n    procedures: dict[str, PgProc]           # Functions\n    triggers: dict[str, PgTrigger]          # Triggers\n    types: dict[str, PgType]                # Custom types\n    depends: list[PgDepend]                 # Dependencies\n</code></pre>"},{"location":"architecture/#model-simplification","title":"Model Simplification","text":"<p>pgdelta uses simplified models that focus only on DDL-relevant information:</p> <pre><code># PostgreSQL's pg_class has 30+ fields\n# pgdelta's PgClass has 4 fields\n@dataclass(frozen=True)\nclass PgClass:\n    oid: int = field(metadata={\"tag\": \"internal\"})\n    relname: str = field(metadata={\"tag\": \"identity\"})\n    namespace: str = field(metadata={\"tag\": \"identity\"})\n    relkind: str = field(metadata={\"tag\": \"data\"})\n</code></pre>"},{"location":"architecture/#database-interaction","title":"Database Interaction","text":"<pre><code>def extract_catalog(session: Session) -&gt; PgCatalog:\n    \"\"\"Extract complete catalog from PostgreSQL session.\"\"\"\n\n    # Extract all object types in dependency order\n    namespaces = extract_namespaces(session)\n    classes = extract_classes(session)\n    attributes = extract_attributes(session)\n    constraints = extract_constraints(session)\n    indexes = extract_indexes(session)\n    sequences = extract_sequences(session)\n    policies = extract_policies(session)\n    procedures = extract_procedures(session)\n    triggers = extract_triggers(session)\n    types = extract_types(session)\n    depends = extract_depends(session, ...)\n\n    # Build immutable catalog\n    return PgCatalog(\n        namespaces={ns.stable_id: ns for ns in namespaces},\n        classes={cls.stable_id: cls for cls in classes},\n        # ... other collections\n    )\n</code></pre>"},{"location":"architecture/#phase-2-diff","title":"Phase 2: Diff","text":""},{"location":"architecture/#purpose_1","title":"Purpose","text":"<p>Compare two catalogs and generate change objects representing the differences.</p>"},{"location":"architecture/#key-components_1","title":"Key Components","text":""},{"location":"architecture/#semantic-equality","title":"Semantic Equality","text":"<pre><code>def semantic_equality(self, other: BasePgModel) -&gt; bool:\n    \"\"\"Compare objects based on identity and data fields only.\"\"\"\n    if type(self) != type(other):\n        return False\n\n    for field in fields(self):\n        if field.metadata.get(\"tag\") in (\"identity\", \"data\"):\n            if getattr(self, field.name) != getattr(other, field.name):\n                return False\n\n    return True\n</code></pre>"},{"location":"architecture/#change-detection","title":"Change Detection","text":"<pre><code>def diff_catalogs(master: PgCatalog, branch: PgCatalog) -&gt; list[DDL]:\n    \"\"\"Generate changes to transform master to branch.\"\"\"\n    changes = []\n\n    # Diff each object type\n    changes.extend(diff_namespaces(master.namespaces, branch.namespaces))\n    changes.extend(diff_classes(master.classes, branch.classes))\n    changes.extend(diff_attributes(master.attributes, branch.attributes))\n    changes.extend(diff_constraints(master.constraints, branch.constraints))\n    changes.extend(diff_indexes(master.indexes, branch.indexes))\n    # ... other types\n\n    return changes\n</code></pre>"},{"location":"architecture/#change-types","title":"Change Types","text":"<p>Each object type has corresponding change types:</p> <pre><code># Schema changes\n@dataclass(frozen=True)\nclass CreateSchema:\n    stable_id: str\n    nspname: str\n\n@dataclass(frozen=True)\nclass DropSchema:\n    stable_id: str\n    nspname: str\n\n# Table changes\n@dataclass(frozen=True)\nclass CreateTable:\n    stable_id: str\n    namespace: str\n    relname: str\n    columns: list[PgAttribute]\n\n@dataclass(frozen=True)\nclass AlterTable:\n    stable_id: str\n    namespace: str\n    relname: str\n    add_columns: list[PgAttribute]\n    drop_columns: list[str]\n    alter_columns: list[AlterColumn]\n</code></pre>"},{"location":"architecture/#diff-algorithms","title":"Diff Algorithms","text":""},{"location":"architecture/#object-level-diffing","title":"Object-Level Diffing","text":"<pre><code>def diff_objects(\n    master_objects: dict[str, T],\n    branch_objects: dict[str, T],\n    create_fn: Callable[[T], DDL],\n    drop_fn: Callable[[T], DDL],\n    alter_fn: Callable[[T, T], DDL | None],\n) -&gt; list[DDL]:\n    \"\"\"Generic object diffing algorithm.\"\"\"\n    changes = []\n\n    # Find objects to create (in branch but not master)\n    for stable_id, branch_obj in branch_objects.items():\n        if stable_id not in master_objects:\n            changes.append(create_fn(branch_obj))\n\n    # Find objects to drop (in master but not branch)\n    for stable_id, master_obj in master_objects.items():\n        if stable_id not in branch_objects:\n            changes.append(drop_fn(master_obj))\n\n    # Find objects to alter (in both but different)\n    for stable_id, master_obj in master_objects.items():\n        if stable_id in branch_objects:\n            branch_obj = branch_objects[stable_id]\n            if not master_obj.semantic_equality(branch_obj):\n                alter_change = alter_fn(master_obj, branch_obj)\n                if alter_change:\n                    changes.append(alter_change)\n\n    return changes\n</code></pre>"},{"location":"architecture/#field-level-diffing","title":"Field-Level Diffing","text":"<pre><code>def diff_table_columns(\n    master_table: PgClass,\n    branch_table: PgClass,\n    master_catalog: PgCatalog,\n    branch_catalog: PgCatalog,\n) -&gt; AlterTable | None:\n    \"\"\"Diff table columns to generate ALTER TABLE changes.\"\"\"\n\n    master_columns = master_catalog.get_class_attributes(master_table.stable_id)\n    branch_columns = branch_catalog.get_class_attributes(branch_table.stable_id)\n\n    # Find columns to add\n    add_columns = []\n    for branch_col in branch_columns:\n        if not any(col.attname == branch_col.attname for col in master_columns):\n            add_columns.append(branch_col)\n\n    # Find columns to drop\n    drop_columns = []\n    for master_col in master_columns:\n        if not any(col.attname == master_col.attname for col in branch_columns):\n            drop_columns.append(master_col.attname)\n\n    # Find columns to alter\n    alter_columns = []\n    for master_col in master_columns:\n        for branch_col in branch_columns:\n            if master_col.attname == branch_col.attname:\n                if not master_col.semantic_equality(branch_col):\n                    alter_columns.append(AlterColumn(master_col, branch_col))\n\n    # Create ALTER TABLE change if any modifications\n    if add_columns or drop_columns or alter_columns:\n        return AlterTable(\n            stable_id=master_table.stable_id,\n            namespace=master_table.namespace,\n            relname=master_table.relname,\n            add_columns=add_columns,\n            drop_columns=drop_columns,\n            alter_columns=alter_columns,\n        )\n\n    return None\n</code></pre>"},{"location":"architecture/#phase-3-generate","title":"Phase 3: Generate","text":""},{"location":"architecture/#purpose_2","title":"Purpose","text":"<p>Generate SQL DDL from change objects with proper dependency ordering.</p>"},{"location":"architecture/#key-components_2","title":"Key Components","text":""},{"location":"architecture/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_sql(change: DDL) -&gt; str:\n    \"\"\"Generate SQL for a change object using structural pattern matching.\"\"\"\n\n    match change:\n        case CreateSchema() as create_schema:\n            return generate_create_schema_sql(create_schema)\n\n        case CreateTable() as create_table:\n            return generate_create_table_sql(create_table)\n\n        case AlterTable() as alter_table:\n            return generate_alter_table_sql(alter_table)\n\n        case CreateIndex() as create_index:\n            return generate_create_index_sql(create_index)\n\n        case CreateConstraint() as create_constraint:\n            return generate_create_constraint_sql(create_constraint)\n\n        case _:\n            msg = f\"Unsupported change type: {type(change)}\"\n            raise NotImplementedError(msg)\n</code></pre>"},{"location":"architecture/#dependency-resolution","title":"Dependency Resolution","text":"<p>See the Dependency Resolution documentation for detailed information.</p>"},{"location":"architecture/#sql-generation-functions","title":"SQL Generation Functions","text":"<pre><code>def generate_create_table_sql(change: CreateTable) -&gt; str:\n    \"\"\"Generate CREATE TABLE SQL.\"\"\"\n    quoted_schema = f'\"{change.namespace}\"'\n    quoted_table = f'\"{change.relname}\"'\n\n    sql_parts = [f\"CREATE TABLE {quoted_schema}.{quoted_table} (\"]\n\n    # Add columns\n    column_defs = []\n    for col in change.columns:\n        col_def = f'  \"{col.attname}\" {col.formatted_type}'\n\n        if col.is_generated:\n            col_def += f\" GENERATED ALWAYS AS ({col.generated_expression}) STORED\"\n            if col.attnotnull:\n                col_def += \" NOT NULL\"\n        else:\n            if col.default_value:\n                col_def += f\" DEFAULT {col.default_value}\"\n            if col.attnotnull:\n                col_def += \" NOT NULL\"\n\n        column_defs.append(col_def)\n\n    sql_parts.append(\"\\n\" + \",\\n\".join(column_defs) + \"\\n\")\n    sql_parts.append(\")\")\n\n    return \"\".join(sql_parts) + \";\"\n</code></pre>"},{"location":"architecture/#model-architecture","title":"Model Architecture","text":""},{"location":"architecture/#base-model","title":"Base Model","text":"<pre><code>@dataclass(frozen=True)\nclass BasePgModel:\n    \"\"\"Base class for all PostgreSQL models.\"\"\"\n\n    def semantic_equality(self, other: BasePgModel) -&gt; bool:\n        \"\"\"Compare objects based on identity and data fields only.\"\"\"\n        if type(self) != type(other):\n            return False\n\n        for field in fields(self):\n            if field.metadata.get(\"tag\") in (\"identity\", \"data\"):\n                if getattr(self, field.name) != getattr(other, field.name):\n                    return False\n\n        return True\n\n    @property\n    def stable_id(self) -&gt; str:\n        \"\"\"Cross-database portable identifier.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"architecture/#field-metadata-system","title":"Field Metadata System","text":"<pre><code># Field metadata categories\nIDENTITY = {\"tag\": \"identity\"}    # Uniquely identifies object\nDATA = {\"tag\": \"data\"}           # Object data/configuration\nINTERNAL = {\"tag\": \"internal\"}   # PostgreSQL internal fields\n\n# Example usage\n@dataclass(frozen=True)\nclass PgAttribute:\n    # Identity fields (used in semantic comparison)\n    attname: str = field(metadata=IDENTITY)\n    class_stable_id: str = field(metadata=IDENTITY)\n\n    # Data fields (used in semantic comparison)\n    type_name: str = field(metadata=DATA)\n    attnotnull: bool = field(metadata=DATA)\n    default_value: str | None = field(metadata=DATA)\n\n    # Internal fields (ignored in semantic comparison)\n    oid: int = field(metadata=INTERNAL)\n    attnum: int = field(metadata=INTERNAL)\n</code></pre>"},{"location":"architecture/#identifier-system","title":"Identifier System","text":"<p>pgdelta uses a dual identifier system:</p>"},{"location":"architecture/#stable-id","title":"Stable ID","text":"<p>Cross-database portable identifier: <pre><code># Format: \"type_prefix:namespace.name\"\nstable_ids = {\n    \"schema\": \"s:schema_name\",\n    \"table\": \"t:schema.table_name\",\n    \"view\": \"v:schema.view_name\",\n    \"index\": \"i:schema.index_name\",\n    \"constraint\": \"c:schema.table.constraint_name\",\n    \"sequence\": \"S:schema.sequence_name\",\n    \"function\": \"f:schema.function_name\",\n    \"trigger\": \"tg:schema.table.trigger_name\",\n    \"type\": \"typ:schema.type_name\",\n    \"policy\": \"p:schema.table.policy_name\",\n}\n</code></pre></p>"},{"location":"architecture/#pg_depend_id","title":"pg_depend_id","text":"<p>PostgreSQL internal identifier for dependency tracking: <pre><code># Format: \"classid.objid.objsubid\"\npg_depend_id = f\"{classid}.{objid}.{objsubid}\"\n</code></pre></p>"},{"location":"architecture/#directory-structure","title":"Directory Structure","text":"<pre><code>src/pgdelta/\n\u251c\u2500\u2500 __init__.py                 # Public API\n\u251c\u2500\u2500 catalog.py                  # Catalog extraction and management\n\u251c\u2500\u2500 dependency_resolution.py    # Dependency resolution system\n\u251c\u2500\u2500 exceptions.py               # Custom exceptions\n\u2502\n\u251c\u2500\u2500 cli/                        # Command-line interface\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 main.py\n\u2502\n\u251c\u2500\u2500 model/                      # PostgreSQL object models\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py                 # Base model class\n\u2502   \u251c\u2500\u2500 pg_attribute.py         # Column model\n\u2502   \u251c\u2500\u2500 pg_class.py             # Table/view model\n\u2502   \u251c\u2500\u2500 pg_constraint.py        # Constraint model\n\u2502   \u251c\u2500\u2500 pg_depend.py            # Dependency model\n\u2502   \u251c\u2500\u2500 pg_index.py             # Index model\n\u2502   \u251c\u2500\u2500 pg_namespace.py         # Schema model\n\u2502   \u251c\u2500\u2500 pg_policy.py            # Policy model\n\u2502   \u251c\u2500\u2500 pg_proc.py              # Function model\n\u2502   \u251c\u2500\u2500 pg_sequence.py          # Sequence model\n\u2502   \u251c\u2500\u2500 pg_trigger.py           # Trigger model\n\u2502   \u2514\u2500\u2500 pg_type.py              # Type model\n\u2502\n\u251c\u2500\u2500 diff/                       # Diff algorithms\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 orchestrator.py         # Main diffing orchestrator\n\u2502\n\u2514\u2500\u2500 changes/                    # Change types and SQL generation\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 dispatcher.py           # SQL generation dispatcher\n    \u2502\n    \u251c\u2500\u2500 schema/                 # Schema changes\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 create.py\n    \u2502   \u2514\u2500\u2500 drop.py\n    \u2502\n    \u251c\u2500\u2500 table/                  # Table changes\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 create.py\n    \u2502   \u251c\u2500\u2500 drop.py\n    \u2502   \u2514\u2500\u2500 alter.py\n    \u2502\n    \u251c\u2500\u2500 index/                  # Index changes\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 create.py\n    \u2502   \u2514\u2500\u2500 drop.py\n    \u2502\n    \u2514\u2500\u2500 [other entity types]/   # Other change types\n</code></pre>"},{"location":"architecture/#testing-architecture","title":"Testing Architecture","text":""},{"location":"architecture/#test-categories","title":"Test Categories","text":""},{"location":"architecture/#unit-tests","title":"Unit Tests","text":"<pre><code># Test individual components in isolation\ndef test_create_table_sql_generation():\n    \"\"\"Test CREATE TABLE SQL generation.\"\"\"\n    change = CreateTable(\n        stable_id=\"t:public.users\",\n        namespace=\"public\",\n        relname=\"users\",\n        columns=[\n            PgAttribute(attname=\"id\", type_name=\"integer\", attnotnull=True),\n            PgAttribute(attname=\"email\", type_name=\"text\", attnotnull=True),\n        ]\n    )\n\n    sql = generate_create_table_sql(change)\n    assert \"CREATE TABLE \\\"public\\\".\\\"users\\\"\" in sql\n    assert \"\\\"id\\\" integer NOT NULL\" in sql\n    assert \"\\\"email\\\" text NOT NULL\" in sql\n</code></pre>"},{"location":"architecture/#integration-tests","title":"Integration Tests","text":"<pre><code># Test full workflows with real PostgreSQL\ndef test_full_diff_workflow(postgres_session):\n    \"\"\"Test complete extract-diff-generate workflow.\"\"\"\n    # Create initial schema\n    postgres_session.execute(text(\"CREATE TABLE users (id SERIAL PRIMARY KEY)\"))\n    postgres_session.commit()\n\n    # Extract master catalog\n    master_catalog = extract_catalog(postgres_session)\n\n    # Create target schema\n    postgres_session.execute(text(\"ALTER TABLE users ADD COLUMN email TEXT\"))\n    postgres_session.commit()\n\n    # Extract branch catalog\n    branch_catalog = extract_catalog(postgres_session)\n\n    # Generate changes\n    changes = master_catalog.diff(branch_catalog)\n\n    # Verify changes\n    assert len(changes) == 1\n    assert isinstance(changes[0], AlterTable)\n    assert changes[0].add_columns[0].attname == \"email\"\n</code></pre>"},{"location":"architecture/#roundtrip-tests","title":"Roundtrip Tests","text":"<pre><code># Test that Extract \u2192 Diff \u2192 Generate \u2192 Apply produces identical schemas\ndef test_roundtrip_fidelity(postgres_session):\n    \"\"\"Test roundtrip fidelity with complex schema.\"\"\"\n    # Create complex schema\n    setup_complex_schema(postgres_session)\n\n    # Extract catalog\n    original_catalog = extract_catalog(postgres_session)\n\n    # Generate recreation changes\n    empty_catalog = PgCatalog(...)\n    changes = empty_catalog.diff(original_catalog)\n\n    # Apply changes to empty database\n    apply_changes(changes, empty_postgres_session)\n\n    # Extract final catalog\n    final_catalog = extract_catalog(empty_postgres_session)\n\n    # Verify semantic equality\n    assert original_catalog.semantically_equals(final_catalog)\n</code></pre>"},{"location":"architecture/#test-infrastructure","title":"Test Infrastructure","text":""},{"location":"architecture/#test-containers","title":"Test Containers","text":"<pre><code># Use testcontainers for real PostgreSQL testing\n@pytest.fixture\ndef postgres_container():\n    \"\"\"PostgreSQL test container.\"\"\"\n    with PostgresContainer(\"postgres:17\") as container:\n        yield container\n\n@pytest.fixture\ndef postgres_session(postgres_container):\n    \"\"\"PostgreSQL session for testing.\"\"\"\n    engine = create_engine(postgres_container.get_connection_url())\n    with Session(engine) as session:\n        yield session\n</code></pre>"},{"location":"architecture/#test-data-generation","title":"Test Data Generation","text":"<pre><code># Generate test data programmatically\ndef generate_test_schema(complexity: str = \"simple\") -&gt; str:\n    \"\"\"Generate test schema SQL.\"\"\"\n    if complexity == \"simple\":\n        return \"\"\"\n        CREATE SCHEMA test;\n        CREATE TABLE test.users (\n            id SERIAL PRIMARY KEY,\n            email TEXT NOT NULL\n        );\n        \"\"\"\n    elif complexity == \"complex\":\n        return \"\"\"\n        CREATE SCHEMA app;\n        CREATE SEQUENCE app.user_id_seq;\n        CREATE TABLE app.users (\n            id BIGINT DEFAULT nextval('app.user_id_seq') PRIMARY KEY,\n            email TEXT NOT NULL UNIQUE\n        );\n        CREATE INDEX idx_users_email ON app.users (email);\n        CREATE VIEW app.active_users AS\n        SELECT * FROM app.users WHERE email IS NOT NULL;\n        \"\"\"\n</code></pre>"},{"location":"architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/#query-optimization","title":"Query Optimization","text":"<ul> <li>Batch extraction: Single queries for each object type</li> <li>Index usage: Leverage PostgreSQL indexes for fast extraction</li> <li>Minimal data: Extract only essential fields</li> </ul>"},{"location":"architecture/#dependency-resolution_1","title":"Dependency Resolution","text":"<ul> <li>Focused analysis: Only analyze objects relevant to changes</li> <li>Efficient algorithms: Use NetworkX for graph operations</li> <li>Constraint caching: Reuse constraint calculations</li> </ul>"},{"location":"architecture/#error-handling","title":"Error Handling","text":""},{"location":"architecture/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>class PgDeltaError(Exception):\n    \"\"\"Base exception for pgdelta errors.\"\"\"\n    pass\n\nclass DependencyResolutionError(PgDeltaError):\n    \"\"\"Error during dependency resolution.\"\"\"\n    pass\n\nclass CyclicDependencyError(DependencyResolutionError):\n    \"\"\"Cyclic dependency detected.\"\"\"\n    pass\n</code></pre>"},{"location":"architecture/#error-recovery","title":"Error Recovery","text":"<pre><code>def safe_extract_catalog(session: Session) -&gt; PgCatalog:\n    \"\"\"Extract catalog with error recovery.\"\"\"\n    try:\n        return extract_catalog(session)\n    except Exception as e:\n        logger.error(f\"Failed to extract catalog: {e}\")\n        # Try to extract partial catalog\n        return extract_partial_catalog(session)\n</code></pre>"},{"location":"architecture/#extension-points","title":"Extension Points","text":""},{"location":"architecture/#adding-new-object-types","title":"Adding New Object Types","text":"<ol> <li>Create model in <code>model/pg_*.py</code></li> <li>Add extraction logic</li> <li>Add diffing logic in <code>diff/</code></li> <li>Add change types in <code>changes/*/</code></li> <li>Add SQL generation functions</li> <li>Add tests</li> </ol>"},{"location":"architecture/#custom-change-types","title":"Custom Change Types","text":"<pre><code>@dataclass(frozen=True)\nclass CustomChange:\n    \"\"\"Custom change type.\"\"\"\n    stable_id: str\n    custom_field: str\n\ndef generate_custom_change_sql(change: CustomChange) -&gt; str:\n    \"\"\"Generate SQL for custom change.\"\"\"\n    return f\"-- Custom change: {change.custom_field}\"\n</code></pre>"},{"location":"architecture/#hooks-and-plugins","title":"Hooks and Plugins","text":"<pre><code># Future extension point for plugins\nclass PgDeltaPlugin:\n    \"\"\"Base class for pgdelta plugins.\"\"\"\n\n    def pre_extract(self, session: Session) -&gt; None:\n        \"\"\"Called before catalog extraction.\"\"\"\n        pass\n\n    def post_diff(self, changes: list[DDL]) -&gt; list[DDL]:\n        \"\"\"Called after diff generation.\"\"\"\n        return changes\n\n    def pre_generate(self, changes: list[DDL]) -&gt; list[DDL]:\n        \"\"\"Called before SQL generation.\"\"\"\n        return changes\n</code></pre>"},{"location":"architecture/#future-architecture-enhancements","title":"Future Architecture Enhancements","text":""},{"location":"architecture/#streaming-processing","title":"Streaming Processing","text":"<pre><code># Future: Stream large catalogs to reduce memory usage\nasync def stream_catalog_extraction(session: AsyncSession) -&gt; AsyncIterator[BasePgModel]:\n    \"\"\"Stream catalog objects for large databases.\"\"\"\n    async for obj in extract_objects_streaming(session):\n        yield obj\n</code></pre>"},{"location":"architecture/#parallel-processing","title":"Parallel Processing","text":"<pre><code># Future: Parallelize independent operations\nasync def parallel_sql_generation(changes: list[DDL]) -&gt; list[str]:\n    \"\"\"Generate SQL in parallel for independent changes.\"\"\"\n    tasks = []\n    for change in changes:\n        if can_generate_in_parallel(change):\n            tasks.append(asyncio.create_task(generate_sql_async(change)))\n\n    return await asyncio.gather(*tasks)\n</code></pre>"},{"location":"architecture/#caching-layer","title":"Caching Layer","text":"<pre><code># Future: Cache catalog extractions and diff results\nclass CachingCatalogExtractor:\n    \"\"\"Catalog extractor with caching.\"\"\"\n\n    def __init__(self, cache_backend: CacheBackend):\n        self.cache = cache_backend\n\n    def extract_catalog(self, session: Session) -&gt; PgCatalog:\n        cache_key = self._compute_cache_key(session)\n        cached_catalog = self.cache.get(cache_key)\n\n        if cached_catalog:\n            return cached_catalog\n\n        catalog = extract_catalog(session)\n        self.cache.set(cache_key, catalog)\n        return catalog\n</code></pre>"},{"location":"architecture/#summary","title":"Summary","text":"<p>pgdelta's architecture is designed for: - Correctness: Immutable data and pure functions prevent bugs - Performance: Efficient algorithms and minimal data structures - Maintainability: Clear separation of concerns and type safety - Extensibility: Plugin system and extension points for new features - Testability: Real PostgreSQL testing with comprehensive coverage</p> <p>The three-phase approach ensures that each component has a single responsibility and can be tested independently, while the immutable data structures prevent the complex state management bugs that plague many migration tools.</p>"},{"location":"architecture/#roundtrip-fidelity","title":"Roundtrip Fidelity","text":"<p>One of pgdelta's key guarantees is roundtrip fidelity:</p> <pre><code>Extract(DB1) \u2192 Diff \u2192 Generate(SQL) \u2192 Apply(SQL, DB2) \u2192 Extract(DB2)\n</code></pre> <p>The final Extract(DB2) should produce a catalog that is semantically identical to the original Extract(DB1).</p> <p>This ensures that: - No information is lost during the process - Generated DDL is complete and accurate - The tool can be used reliably for production migrations</p>"},{"location":"architecture/#testing-philosophy","title":"Testing Philosophy","text":"<p>pgdelta's testing approach emphasizes real-world accuracy:</p>"},{"location":"architecture/#real-postgresql-testing","title":"Real PostgreSQL Testing","text":"<ul> <li>All tests use actual PostgreSQL instances via testcontainers</li> <li>No mocks or simulated behavior</li> <li>Tests verify behavior against real database responses</li> </ul>"},{"location":"architecture/#roundtrip-testing","title":"Roundtrip Testing","text":"<ul> <li>Generic integration tests verify roundtrip fidelity</li> <li>Tests ensure Extract \u2192 Diff \u2192 Generate \u2192 Apply cycles work correctly</li> <li>Validates that generated DDL recreates schemas exactly</li> </ul>"},{"location":"architecture/#coverage-requirements","title":"Coverage Requirements","text":"<ul> <li>Minimum 85% test coverage required</li> <li>All code paths must be tested with real PostgreSQL behavior</li> <li>Edge cases are tested with actual database scenarios</li> </ul> <p>This testing approach ensures that pgdelta works correctly with real PostgreSQL databases and handles edge cases properly.</p>"},{"location":"dependency-resolution/","title":"Dependency Resolution","text":"<p>PostgreSQL objects have complex dependency relationships that must be respected when generating DDL migrations. pgdelta uses a sophisticated constraint-based dependency resolution system to ensure SQL statements are generated in the correct order.</p>"},{"location":"dependency-resolution/#the-challenge","title":"The Challenge","text":"<p>Consider this PostgreSQL schema:</p> <pre><code>-- 1. Create schema\nCREATE SCHEMA app;\n\n-- 2. Create sequence\nCREATE SEQUENCE app.user_id_seq;\n\n-- 3. Create table that uses sequence\nCREATE TABLE app.users (\n    id BIGINT DEFAULT nextval('app.user_id_seq') PRIMARY KEY,\n    email TEXT NOT NULL\n);\n\n-- 4. Create index on table\nCREATE INDEX idx_users_email ON app.users (email);\n\n-- 5. Create view that references table\nCREATE VIEW app.active_users AS\nSELECT * FROM app.users WHERE email IS NOT NULL;\n\n-- 6. Set sequence ownership\nALTER SEQUENCE app.user_id_seq OWNED BY app.users.id;\n</code></pre> <p>When generating DDL to create this schema, the operations must be performed in a specific order: 1. Schema must exist before any objects are created in it 2. Sequence must exist before table references it 3. Table must exist before index is created on it 4. Table must exist before view references it 5. Both sequence and table must exist before setting ownership</p> <p>The wrong order will cause PostgreSQL errors!</p>"},{"location":"dependency-resolution/#pgdeltas-solution-constraint-based-resolution","title":"pgdelta's Solution: Constraint-Based Resolution","text":"<p>pgdelta uses a modern constraint-based approach that separates concerns:</p> <ol> <li>Extract Dependencies: Find all relevant object relationships</li> <li>Generate Constraints: Apply semantic rules to create ordering constraints</li> <li>Solve Constraints: Use topological sorting to find valid ordering</li> </ol> <p>This approach is more maintainable and extensible than traditional conditional logic.</p>"},{"location":"dependency-resolution/#dependency-sources","title":"Dependency Sources","text":"<p>pgdelta identifies dependencies from multiple sources:</p>"},{"location":"dependency-resolution/#pg_depend","title":"pg_depend","text":"<p>PostgreSQL's internal dependency tracking system that records explicit dependencies between objects.</p>"},{"location":"dependency-resolution/#implicit-dependencies","title":"Implicit Dependencies","text":"<ul> <li>Schema ownership: Objects must be created in existing schemas</li> <li>Table-column relationships: Columns belong to tables</li> <li>Index-table relationships: Indexes are built on tables</li> </ul>"},{"location":"dependency-resolution/#constraint-dependencies","title":"Constraint Dependencies","text":"<ul> <li>Foreign key references: Foreign keys depend on referenced tables</li> <li>Check constraint dependencies: Check constraints may reference other objects</li> </ul>"},{"location":"dependency-resolution/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Master Catalog  \u2502    \u2502 Branch Catalog  \u2502    \u2502 Change List     \u2502\n\u2502 (current state) \u2502    \u2502 (target state)  \u2502    \u2502 (operations)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502    DependencyExtractor      \u2502\n                  \u2502                             \u2502\n                  \u2502 \u2022 Finds relevant objects    \u2502\n                  \u2502 \u2022 Extracts dependencies     \u2502\n                  \u2502 \u2022 Builds dependency model   \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502    OperationSemantics       \u2502\n                  \u2502                             \u2502\n                  \u2502 \u2022 Applies semantic rules    \u2502\n                  \u2502 \u2022 Generates constraints     \u2502\n                  \u2502 \u2022 Handles special cases     \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u25bc\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502    ConstraintSolver         \u2502\n                  \u2502                             \u2502\n                  \u2502 \u2022 Builds constraint graph   \u2502\n                  \u2502 \u2022 Performs topological sort \u2502\n                  \u2502 \u2022 Returns ordered changes   \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"dependency-resolution/#components-deep-dive","title":"Components Deep Dive","text":""},{"location":"dependency-resolution/#1-dependencyextractor","title":"1. DependencyExtractor","text":"<p>Purpose: Extract only the dependencies relevant to the current changeset.</p> <p>Key Insight: Instead of analyzing the entire database, focus only on objects that are being changed or could affect the changes.</p> <pre><code>class DependencyExtractor:\n    \"\"\"Extracts dependencies relevant to a changeset from catalogs.\"\"\"\n\n    def __init__(self, master_catalog: PgCatalog, branch_catalog: PgCatalog):\n        # Store both current state (master) and target state (branch)\n        self.master_catalog = master_catalog\n        self.branch_catalog = branch_catalog\n</code></pre>"},{"location":"dependency-resolution/#finding-relevant-objects","title":"Finding Relevant Objects","text":"<p>The extractor uses transitive discovery to find all objects that could be relevant:</p> <pre><code>def _find_relevant_objects(self, changes: list[DDL], max_depth: int = 2) -&gt; set[str]:\n    \"\"\"Find all objects relevant to the changeset using transitive discovery.\"\"\"\n    # Start with objects being directly changed\n    relevant = {change.stable_id for change in changes}\n\n    # Add transitive dependencies up to max_depth\n    for _ in range(max_depth):\n        new_objects = set()\n        for obj_id in relevant:\n            # Add dependencies from both catalogs\n            # (objects this object depends on)\n            new_objects.update(\n                self._get_direct_dependencies(obj_id, self.master_catalog)\n            )\n            new_objects.update(\n                self._get_direct_dependencies(obj_id, self.branch_catalog)\n            )\n\n            # Add dependents from both catalogs\n            # (objects that depend on this object)\n            new_objects.update(\n                self._get_direct_dependents(obj_id, self.master_catalog)\n            )\n            new_objects.update(\n                self._get_direct_dependents(obj_id, self.branch_catalog)\n            )\n\n        # Stop if no new objects found\n        if not new_objects - relevant:\n            break\n        relevant.update(new_objects)\n\n    return relevant\n</code></pre> <p>Why transitive discovery? If we're creating table A that depends on sequence B, and sequence B depends on schema C, we need to know about all three relationships even if we're only changing table A.</p>"},{"location":"dependency-resolution/#building-the-dependency-model","title":"Building the Dependency Model","text":"<p>The extractor builds a unified model containing dependencies from both catalogs:</p> <pre><code>def _extract_from_catalog(\n    self,\n    model: DependencyModel,\n    catalog: PgCatalog,\n    relevant_objects: set[str],\n    source: str,\n) -&gt; None:\n    \"\"\"Extract dependencies from a catalog for relevant objects only.\"\"\"\n    for depend in catalog.depends:\n        # Only include dependencies between relevant objects\n        if (\n            depend.dependent_stable_id in relevant_objects\n            and depend.referenced_stable_id in relevant_objects\n            and not depend.dependent_stable_id.startswith(\"unknown.\")\n            and not depend.referenced_stable_id.startswith(\"unknown.\")\n        ):\n            # Tag each dependency with its source catalog\n            model.add_dependency(\n                depend.dependent_stable_id,\n                depend.referenced_stable_id,\n                source  # \"master\" or \"branch\"\n            )\n</code></pre> <p>Why both catalogs? Dependencies exist in different states: - Master catalog: Shows current dependencies (needed for DROP operations) - Branch catalog: Shows target dependencies (needed for CREATE operations)</p>"},{"location":"dependency-resolution/#2-operationsemantics","title":"2. OperationSemantics","text":"<p>Purpose: Apply semantic rules to generate ordering constraints between operations.</p> <p>Key Insight: Different operation types (CREATE, DROP, ALTER) have different semantic requirements for dependency ordering.</p> <pre><code>class OperationSemantics:\n    \"\"\"Defines semantic rules for ordering operations based on dependencies.\"\"\"\n\n    def generate_constraints(\n        self, changes: list[DDL], model: DependencyModel\n    ) -&gt; list[Constraint]:\n        \"\"\"Generate ordering constraints from changes and dependency model.\"\"\"\n        constraints = []\n\n        # Add dependency-based constraints\n        # (operations must respect object dependencies)\n        constraints.extend(self._generate_dependency_constraints(changes, model))\n\n        # Add same-object operation constraints\n        # (operations on same object have specific ordering rules)\n        constraints.extend(self._generate_same_object_constraints(changes))\n\n        return constraints\n</code></pre>"},{"location":"dependency-resolution/#dependency-based-constraints","title":"Dependency-Based Constraints","text":"<p>The core logic analyzes pairs of operations and determines if they should be ordered:</p> <pre><code>def _analyze_dependency_constraint(\n    self, i: int, change_a: DDL, j: int, change_b: DDL, model: DependencyModel\n) -&gt; Constraint | None:\n    \"\"\"Analyze if two changes should be ordered based on dependencies.\"\"\"\n\n    # CRITICAL: Choose appropriate catalog state for each operation\n    # For CREATE operations, use branch catalog (where dependencies will exist after creation)\n    # For DROP operations, use master catalog (where dependencies exist before deletion)\n    source_a = \"master\" if is_drop_change(change_a) else \"branch\"\n    source_b = \"master\" if is_drop_change(change_b) else \"branch\"\n\n    # Check for dependencies in appropriate states\n    a_depends_on_b = model.has_dependency(\n        change_a.stable_id, change_b.stable_id, source_a\n    )\n    b_depends_on_a = model.has_dependency(\n        change_b.stable_id, change_a.stable_id, source_b\n    )\n\n    # Apply semantic rules based on discovered dependencies\n    if a_depends_on_b:\n        return self._dependency_semantic_rule(\n            i, change_a, j, change_b, \"a_depends_on_b\"\n        )\n    elif b_depends_on_a:\n        return self._dependency_semantic_rule(\n            j, change_b, i, change_a, \"b_depends_on_a\"\n        )\n\n    return None\n</code></pre> <p>Why different catalog states? Consider these scenarios:</p> <ul> <li>CREATE operations: Need to look at branch catalog to see what dependencies will exist</li> <li>DROP operations: Need to look at master catalog to see what dependencies currently exist</li> </ul>"},{"location":"dependency-resolution/#semantic-rules","title":"Semantic Rules","text":"<p>Once a dependency is found, different rules apply based on operation types:</p> <pre><code>def _dependency_semantic_rule(\n    self,\n    dep_idx: int,\n    dependent_change: DDL,\n    ref_idx: int,\n    referenced_change: DDL,\n    reason: str,\n) -&gt; Constraint | None:\n    \"\"\"Apply semantic rules when dependent_change depends on referenced_change.\"\"\"\n\n    # Rule 1: For DROP operations, drop dependents before dependencies\n    if is_drop_change(dependent_change) and is_drop_change(referenced_change):\n        return Constraint(\n            dep_idx,                    # Drop dependent first\n            ConstraintType.BEFORE,\n            ref_idx,                    # Before dropping dependency\n            f\"DROP dependent before dependency ({reason})\",\n        )\n\n    # Rule 2: For CREATE operations, create dependencies before dependents\n    elif is_create_change(dependent_change) and is_create_change(referenced_change):\n        return Constraint(\n            ref_idx,                    # Create dependency first\n            ConstraintType.BEFORE,\n            dep_idx,                    # Before creating dependent\n            f\"CREATE dependency before dependent ({reason})\",\n        )\n\n    # Rule 3: For mixed operations, ensure dependencies exist first\n    elif (\n        is_create_change(dependent_change)\n        or is_alter_change(dependent_change)\n        or is_replace_change(dependent_change)\n    ) and (\n        is_create_change(referenced_change)\n        or is_alter_change(referenced_change)\n        or is_replace_change(referenced_change)\n    ):\n        return Constraint(\n            ref_idx,                    # Ensure dependency exists first\n            ConstraintType.BEFORE,\n            dep_idx,                    # Before creating/altering dependent\n            f\"Ensure dependency exists before dependent ({reason})\",\n        )\n\n    # Rule 4: DROP before CREATE/ALTER/REPLACE\n    elif is_drop_change(referenced_change) and (\n        is_create_change(dependent_change)\n        or is_alter_change(dependent_change)\n        or is_replace_change(dependent_change)\n    ):\n        return Constraint(\n            ref_idx,                    # DROP first\n            ConstraintType.BEFORE,\n            dep_idx,                    # Before CREATE/ALTER/REPLACE\n            f\"DROP before CREATE/ALTER/REPLACE ({reason})\",\n        )\n\n    return None\n</code></pre>"},{"location":"dependency-resolution/#special-cases","title":"Special Cases","text":"<p>Some PostgreSQL relationships require special handling:</p> <pre><code># Special rule: For CREATE operations with sequences and tables\n# PostgreSQL reports sequence ownership (sequence depends on table)\n# But for creation, table depends on sequence (table needs sequence to exist first)\nif is_create_change(dependent_change) and is_create_change(referenced_change):\n    from pgdelta.changes.sequence import CreateSequence\n    from pgdelta.changes.table import CreateTable\n\n    # If sequence depends on table, invert for CREATE operations\n    if isinstance(dependent_change, CreateSequence) and isinstance(\n        referenced_change, CreateTable\n    ):\n        return Constraint(\n            dep_idx,  # CreateSequence should come first\n            ConstraintType.BEFORE,\n            ref_idx,  # Before CreateTable\n            f\"CREATE sequence before table that uses it ({reason})\",\n        )\n</code></pre> <p>Why this special case? PostgreSQL's dependency tracking shows that sequences are \"owned by\" table columns, making the sequence depend on the table. However, for DDL generation, the table needs the sequence to exist first, so we invert this relationship for CREATE operations.</p>"},{"location":"dependency-resolution/#same-object-constraints","title":"Same-Object Constraints","text":"<p>When multiple operations affect the same object, they need specific ordering:</p> <pre><code>def _generate_same_object_constraints(self, changes: list[DDL]) -&gt; list[Constraint]:\n    \"\"\"Generate constraints for operations on the same object.\"\"\"\n    constraints = []\n\n    # Group changes by object\n    object_groups = defaultdict(list)\n    for i, change in enumerate(changes):\n        object_groups[change.stable_id].append(i)\n\n    # Add ordering constraints within each group\n    for indices in object_groups.values():\n        if len(indices) &gt; 1:\n            # Sort by operation priority (DROP &lt; CREATE &lt; ALTER &lt; REPLACE)\n            sorted_indices = sorted(\n                indices, key=lambda i: self._get_operation_priority(changes[i])\n            )\n\n            # Add sequential constraints\n            for k in range(len(sorted_indices) - 1):\n                constraints.append(\n                    Constraint(\n                        sorted_indices[k],\n                        ConstraintType.BEFORE,\n                        sorted_indices[k + 1],\n                        \"Same object operation priority\",\n                    )\n                )\n\n    return constraints\n\ndef _get_operation_priority(self, change: DDL) -&gt; int:\n    \"\"\"Get operation priority (lower = earlier).\"\"\"\n    if is_drop_change(change):\n        return 0    # DROP first\n    elif is_create_change(change):\n        return 1    # CREATE second\n    elif is_alter_change(change):\n        return 2    # ALTER third\n    elif is_replace_change(change):\n        return 3    # REPLACE last\n    else:\n        return 4\n</code></pre>"},{"location":"dependency-resolution/#3-constraintsolver","title":"3. ConstraintSolver","text":"<p>Purpose: Solve the ordering constraints to produce a valid sequence of operations.</p> <p>Key Insight: This is a classic topological sorting problem that can be solved efficiently with graph algorithms.</p> <pre><code>class ConstraintSolver:\n    \"\"\"Solves ordering constraints to produce a valid sequence.\"\"\"\n\n    def solve(self, changes: list[DDL], constraints: list[Constraint]) -&gt; list[DDL]:\n        \"\"\"Solve constraints using topological sorting.\"\"\"\n        # Build constraint graph\n        graph = nx.DiGraph()\n\n        # Add all changes as nodes\n        for i in range(len(changes)):\n            graph.add_node(i)\n\n        # Add constraint edges\n        for constraint in constraints:\n            if constraint.constraint_type == ConstraintType.BEFORE:\n                # Add directed edge from A to B (A must come before B)\n                graph.add_edge(constraint.change_a_index, constraint.change_b_index)\n\n        # Topological sort\n        try:\n            ordered_indices = list(nx.topological_sort(graph))\n            return [changes[i] for i in ordered_indices]\n        except nx.NetworkXUnfeasible:\n            # This indicates a cyclic dependency\n            raise CyclicDependencyError(\n                \"Cyclic dependency detected in change constraints\"\n            ) from None\n</code></pre> <p>Why NetworkX? NetworkX provides robust graph algorithms including cycle detection and topological sorting, handling edge cases that would be complex to implement manually.</p>"},{"location":"dependency-resolution/#dependency-types","title":"Dependency Types","text":"<p>pgdelta tracks various types of PostgreSQL dependencies:</p>"},{"location":"dependency-resolution/#1-schema-dependencies","title":"1. Schema Dependencies","text":"<pre><code># Schema contains all objects created in it\n# Schema must be created before any objects in it\n# Schema must be dropped after all objects in it are dropped\n\n# Example dependency:\n# PgDepend(\n#     dependent_stable_id=\"t:app.users\",      # table depends on schema\n#     referenced_stable_id=\"s:app\",           # schema\n#     source=\"branch\"\n# )\n</code></pre>"},{"location":"dependency-resolution/#2-table-column-dependencies","title":"2. Table-Column Dependencies","text":"<pre><code># Columns belong to tables\n# Table must exist before columns can be added\n# Columns must be dropped before table can be dropped\n\n# Example dependency:\n# PgDepend(\n#     dependent_stable_id=\"c:app.users.email\",  # column depends on table\n#     referenced_stable_id=\"t:app.users\",       # table\n#     source=\"branch\"\n# )\n</code></pre>"},{"location":"dependency-resolution/#3-index-dependencies","title":"3. Index Dependencies","text":"<pre><code># Indexes are created on tables\n# Table must exist before index can be created\n# Index must be dropped before table can be dropped\n\n# Example dependency:\n# PgDepend(\n#     dependent_stable_id=\"i:app.idx_users_email\",  # index depends on table\n#     referenced_stable_id=\"t:app.users\",           # table\n#     source=\"branch\"\n# )\n</code></pre>"},{"location":"dependency-resolution/#4-constraint-dependencies","title":"4. Constraint Dependencies","text":"<pre><code># Constraints are applied to tables\n# Foreign key constraints reference other tables\n# Tables must exist before constraints can be added\n\n# Example foreign key dependency:\n# PgDepend(\n#     dependent_stable_id=\"c:app.orders_user_id_fkey\",  # constraint depends on referenced table\n#     referenced_stable_id=\"t:app.users\",               # referenced table\n#     source=\"branch\"\n# )\n</code></pre>"},{"location":"dependency-resolution/#5-sequence-dependencies","title":"5. Sequence Dependencies","text":"<pre><code># Sequences can be owned by table columns\n# For CREATE operations, sequence must exist before table\n# For DROP operations, ownership must be removed before dropping\n\n# Example dependency:\n# PgDepend(\n#     dependent_stable_id=\"s:app.user_id_seq\",  # sequence depends on table (ownership)\n#     referenced_stable_id=\"t:app.users\",       # table\n#     source=\"branch\"\n# )\n</code></pre>"},{"location":"dependency-resolution/#6-view-dependencies","title":"6. View Dependencies","text":"<pre><code># Views reference tables and other views\n# Referenced objects must exist before view can be created\n# Views must be dropped before referenced objects can be dropped\n\n# Example dependency:\n# PgDepend(\n#     dependent_stable_id=\"v:app.active_users\",  # view depends on table\n#     referenced_stable_id=\"t:app.users\",        # table\n#     source=\"branch\"\n# )\n</code></pre>"},{"location":"dependency-resolution/#7-function-dependencies","title":"7. Function Dependencies","text":"<pre><code># Functions can reference tables, types, other functions\n# Referenced objects must exist before function can be created\n\n# Example dependency:\n# PgDepend(\n#     dependent_stable_id=\"f:app.get_user_count\",  # function depends on table\n#     referenced_stable_id=\"t:app.users\",          # table\n#     source=\"branch\"\n# )\n</code></pre>"},{"location":"dependency-resolution/#8-trigger-dependencies","title":"8. Trigger Dependencies","text":"<pre><code># Triggers are attached to tables and call functions\n# Table and function must exist before trigger can be created\n\n# Example dependencies:\n# PgDepend(\n#     dependent_stable_id=\"tg:app.users.update_modified_time\",  # trigger depends on table\n#     referenced_stable_id=\"t:app.users\",                       # table\n#     source=\"branch\"\n# )\n# PgDepend(\n#     dependent_stable_id=\"tg:app.users.update_modified_time\",  # trigger depends on function\n#     referenced_stable_id=\"f:app.update_modified_time\",        # function\n#     source=\"branch\"\n# )\n</code></pre>"},{"location":"dependency-resolution/#catalog-state-management","title":"Catalog State Management","text":"<p>pgdelta maintains two catalog states to handle dependencies correctly:</p>"},{"location":"dependency-resolution/#master-catalog-current-state","title":"Master Catalog (Current State)","text":"<pre><code># Contains the current database state\n# Used for:\n# - DROP operations (what dependencies currently exist)\n# - Understanding what needs to be removed\n# - Validating that objects exist before dropping them\n\n# Example: When dropping a table, we need to know what indexes/constraints\n# currently exist on it so we can drop them first\n</code></pre>"},{"location":"dependency-resolution/#branch-catalog-target-state","title":"Branch Catalog (Target State)","text":"<pre><code># Contains the target database state\n# Used for:\n# - CREATE operations (what dependencies will exist)\n# - Understanding what needs to be created\n# - Validating that dependencies will be satisfied\n\n# Example: When creating a table, we need to know what schema it will be in\n# so we can ensure the schema exists first\n</code></pre>"},{"location":"dependency-resolution/#dual-state-dependency-analysis","title":"Dual-State Dependency Analysis","text":"<pre><code>def _analyze_dependency_constraint(\n    self, i: int, change_a: DDL, j: int, change_b: DDL, model: DependencyModel\n) -&gt; Constraint | None:\n    \"\"\"Analyze if two changes should be ordered based on dependencies.\"\"\"\n\n    # CRITICAL: Choose appropriate catalog state for each operation\n    source_a = \"master\" if is_drop_change(change_a) else \"branch\"\n    source_b = \"master\" if is_drop_change(change_b) else \"branch\"\n\n    # Check for dependencies in appropriate states\n    a_depends_on_b = model.has_dependency(\n        change_a.stable_id, change_b.stable_id, source_a\n    )\n    b_depends_on_a = model.has_dependency(\n        change_b.stable_id, change_a.stable_id, source_b\n    )\n\n    # Apply semantic rules...\n</code></pre> <p>Why this matters? Consider a scenario where: 1. We're dropping table A (exists in master, not in branch) 2. We're creating table B (doesn't exist in master, exists in branch) 3. Table B depends on table A in the branch</p> <p>Without dual-state analysis, we might try to drop table A before creating table B, which would fail because table B needs table A to exist when it's created.</p>"},{"location":"dependency-resolution/#error-handling","title":"Error Handling","text":""},{"location":"dependency-resolution/#cyclic-dependencies","title":"Cyclic Dependencies","text":"<pre><code># Cyclic dependencies are detected during topological sorting\ntry:\n    ordered_indices = list(nx.topological_sort(graph))\n    return [changes[i] for i in ordered_indices]\nexcept nx.NetworkXUnfeasible:\n    raise CyclicDependencyError(\n        \"Cyclic dependency detected in change constraints\"\n    ) from None\n</code></pre> <p>What causes cyclic dependencies? - Circular foreign key references - Mutual view dependencies - Complex constraint interdependencies</p> <p>How pgdelta handles them: 1. Detects cycles during topological sort 2. Raises a clear error message 3. Provides debugging information about the cycle</p>"},{"location":"dependency-resolution/#missing-dependencies","title":"Missing Dependencies","text":"<pre><code># Filter out unknown dependencies during extraction\nif (\n    depend.dependent_stable_id in relevant_objects\n    and depend.referenced_stable_id in relevant_objects\n    and not depend.dependent_stable_id.startswith(\"unknown.\")\n    and not depend.referenced_stable_id.startswith(\"unknown.\")\n):\n    model.add_dependency(\n        depend.dependent_stable_id,\n        depend.referenced_stable_id,\n        source\n    )\n</code></pre> <p>Unknown dependencies occur when: - System objects are referenced but not tracked - Complex PostgreSQL internals are involved - Extension objects are not fully catalogued</p>"},{"location":"dependency-resolution/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"dependency-resolution/#changeset-focused-analysis","title":"Changeset-Focused Analysis","text":"<pre><code>def extract_for_changeset(self, changes: list[DDL]) -&gt; DependencyModel:\n    \"\"\"Extract only dependencies relevant to the changeset.\"\"\"\n    model = DependencyModel()\n\n    # Find all objects relevant to the changeset\n    relevant_objects = self._find_relevant_objects(changes)\n\n    # Extract dependencies from both catalogs for relevant objects\n    self._extract_from_catalog(\n        model, self.master_catalog, relevant_objects, \"master\"\n    )\n    self._extract_from_catalog(\n        model, self.branch_catalog, relevant_objects, \"branch\"\n    )\n\n    return model\n</code></pre> <p>Why this matters? A full PostgreSQL catalog can contain thousands of objects. By focusing only on objects relevant to the current changeset, we: - Improve performance - Avoid unnecessary complexity</p>"},{"location":"dependency-resolution/#efficient-graph-algorithms","title":"Efficient Graph Algorithms","text":"<pre><code># Use NetworkX for robust graph algorithms\nimport networkx as nx\n\n# NetworkX provides:\n# - Efficient topological sorting\n# - Cycle detection\n# - Graph traversal algorithms\n# - Well-tested implementations\n</code></pre>"},{"location":"dependency-resolution/#dependency-indexing","title":"Dependency Indexing","text":"<pre><code>class DependencyModel:\n    def __init__(self) -&gt; None:\n        self.dependencies: set[ObjectDependency] = set()\n        # Build indexes for fast lookup\n        self._dependency_index: dict[str, set[str]] = defaultdict(set)\n        self._reverse_index: dict[str, set[str]] = defaultdict(set)\n\n    def add_dependency(self, dependent: str, referenced: str, source: str = \"\") -&gt; None:\n        \"\"\"Add a dependency between objects.\"\"\"\n        dep = ObjectDependency(dependent, referenced, source)\n        if dep not in self.dependencies:\n            self.dependencies.add(dep)\n            # Update indexes for O(1) lookup\n            self._dependency_index[dependent].add(referenced)\n            self._reverse_index[referenced].add(dependent)\n</code></pre>"},{"location":"dependency-resolution/#testing-and-validation","title":"Testing and Validation","text":""},{"location":"dependency-resolution/#unit-tests","title":"Unit Tests","text":"<p>pgdelta includes comprehensive unit tests for dependency resolution:</p> <pre><code>def test_create_sequence_before_table():\n    \"\"\"Test that sequences are created before tables that use them.\"\"\"\n    changes = [\n        CreateTable(stable_id=\"t:app.users\", ...),\n        CreateSequence(stable_id=\"s:app.user_id_seq\", ...),\n    ]\n\n    # Build dependency model\n    model = DependencyModel()\n    model.add_dependency(\"t:app.users\", \"s:app.user_id_seq\", \"branch\")\n\n    # Resolve dependencies\n    resolver = DependencyResolver(master_catalog, branch_catalog)\n    ordered_changes = resolver.resolve_dependencies(changes)\n\n    # Verify sequence comes before table\n    assert ordered_changes[0].stable_id == \"s:app.user_id_seq\"\n    assert ordered_changes[1].stable_id == \"t:app.users\"\n</code></pre>"},{"location":"dependency-resolution/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_complex_schema_dependencies():\n    \"\"\"Test dependency resolution with complex real-world schema.\"\"\"\n    # Create complex schema with multiple interdependencies\n    sql = \"\"\"\n    CREATE SCHEMA app;\n    CREATE SEQUENCE app.user_id_seq;\n    CREATE TABLE app.users (\n        id BIGINT DEFAULT nextval('app.user_id_seq') PRIMARY KEY,\n        email TEXT NOT NULL\n    );\n    CREATE INDEX idx_users_email ON app.users (email);\n    CREATE VIEW app.active_users AS\n    SELECT * FROM app.users WHERE email IS NOT NULL;\n    ALTER SEQUENCE app.user_id_seq OWNED BY app.users.id;\n    \"\"\"\n\n    # Generate changes\n    changes = generate_changes_from_sql(sql)\n\n    # Resolve dependencies\n    ordered_changes = resolve_dependencies(changes, master_catalog, branch_catalog)\n\n    # Verify correct ordering\n    assert_schema_created_first(ordered_changes)\n    assert_sequence_created_before_table(ordered_changes)\n    assert_table_created_before_index(ordered_changes)\n    assert_table_created_before_view(ordered_changes)\n</code></pre>"},{"location":"dependency-resolution/#roundtrip-fidelity-tests","title":"Roundtrip Fidelity Tests","text":"<pre><code>def test_dependency_roundtrip_fidelity():\n    \"\"\"Test that dependency resolution maintains roundtrip fidelity.\"\"\"\n    # Extract catalog from database\n    original_catalog = extract_catalog(session)\n\n    # Generate changes to recreate schema\n    changes = generate_recreation_changes(original_catalog)\n\n    # Resolve dependencies\n    ordered_changes = resolve_dependencies(changes, empty_catalog, original_catalog)\n\n    # Apply changes to empty database\n    apply_changes(ordered_changes, empty_session)\n\n    # Extract new catalog\n    new_catalog = extract_catalog(empty_session)\n\n    # Verify catalogs are semantically identical\n    assert original_catalog.semantically_equals(new_catalog)\n</code></pre>"},{"location":"dependency-resolution/#debugging-dependency-issues","title":"Debugging Dependency Issues","text":""},{"location":"dependency-resolution/#constraint-visualization","title":"Constraint Visualization","text":"<pre><code>def debug_constraints(changes: list[DDL], constraints: list[Constraint]):\n    \"\"\"Debug constraint generation.\"\"\"\n    print(\"Generated Constraints:\")\n    for constraint in constraints:\n        change_a = changes[constraint.change_a_index]\n        change_b = changes[constraint.change_b_index]\n        print(f\"  {change_a.stable_id} {constraint.constraint_type.value} {change_b.stable_id}\")\n        print(f\"    Reason: {constraint.reason}\")\n</code></pre>"},{"location":"dependency-resolution/#dependency-graph-export","title":"Dependency Graph Export","text":"<pre><code>def export_dependency_graph(model: DependencyModel, filename: str):\n    \"\"\"Export dependency graph for visualization.\"\"\"\n    graph = nx.DiGraph()\n\n    for dep in model.dependencies:\n        graph.add_edge(dep.referenced, dep.dependent, source=dep.source)\n\n    nx.write_graphml(graph, filename)\n    # Open in graph visualization tool like Gephi or yEd\n</code></pre>"},{"location":"dependency-resolution/#error-diagnosis","title":"Error Diagnosis","text":"<pre><code>def diagnose_cyclic_dependency(changes: list[DDL], constraints: list[Constraint]):\n    \"\"\"Diagnose cyclic dependency errors.\"\"\"\n    graph = nx.DiGraph()\n\n    for i in range(len(changes)):\n        graph.add_node(i)\n\n    for constraint in constraints:\n        graph.add_edge(constraint.change_a_index, constraint.change_b_index)\n\n    try:\n        cycles = list(nx.simple_cycles(graph))\n        print(f\"Found {len(cycles)} cycles:\")\n        for cycle in cycles:\n            print(\"  Cycle:\")\n            for i in cycle:\n                print(f\"    {changes[i].stable_id}\")\n    except nx.NetworkXNoCycle:\n        print(\"No cycles found\")\n</code></pre>"},{"location":"dependency-resolution/#future-enhancements","title":"Future Enhancements","text":""},{"location":"dependency-resolution/#parallel-execution","title":"Parallel Execution","text":"<pre><code># Future: Identify operations that can be executed in parallel\ndef identify_parallel_operations(ordered_changes: list[DDL]) -&gt; list[list[DDL]]:\n    \"\"\"Identify operations that can be executed in parallel.\"\"\"\n    # Operations with no dependencies between them can run in parallel\n    # This could significantly improve migration performance\n    pass\n</code></pre>"},{"location":"dependency-resolution/#dependency-optimization","title":"Dependency Optimization","text":"<pre><code># Future: Optimize dependency extraction for very large schemas\ndef optimize_dependency_extraction(changes: list[DDL]) -&gt; set[str]:\n    \"\"\"Optimize dependency extraction for large schemas.\"\"\"\n    # Use more sophisticated algorithms to minimize dependency analysis\n    # Consider dependency caching and incremental analysis\n    pass\n</code></pre>"},{"location":"dependency-resolution/#smart-batching","title":"Smart Batching","text":"<pre><code># Future: Batch related operations for better performance\ndef batch_related_operations(ordered_changes: list[DDL]) -&gt; list[list[DDL]]:\n    \"\"\"Batch related operations for better performance.\"\"\"\n    # Group operations that can be executed together\n    # For example, multiple column additions on the same table\n    pass\n</code></pre>"},{"location":"dependency-resolution/#best-practices","title":"Best Practices","text":""},{"location":"dependency-resolution/#1-design-for-dependency-resolution","title":"1. Design for Dependency Resolution","text":"<p>When designing PostgreSQL schemas:</p> <pre><code># Good: Clear dependency hierarchy\nCREATE SCHEMA app;\nCREATE SEQUENCE app.user_id_seq;\nCREATE TABLE app.users (id BIGINT DEFAULT nextval('app.user_id_seq'));\n\n# Avoid: Circular dependencies\nCREATE TABLE app.users (friend_id BIGINT REFERENCES app.users(id));\nCREATE TABLE app.friends (user_id BIGINT REFERENCES app.users(id));\n</code></pre>"},{"location":"dependency-resolution/#2-test-complex-dependencies","title":"2. Test Complex Dependencies","text":"<pre><code># Test dependency resolution with complex scenarios\ndef test_complex_foreign_key_dependencies():\n    \"\"\"Test complex foreign key dependency scenarios.\"\"\"\n    # Create scenario with multiple interdependent tables\n    # Verify correct ordering\n    pass\n</code></pre>"},{"location":"dependency-resolution/#3-monitor-performance","title":"3. Monitor Performance","text":"<pre><code># Monitor dependency resolution performance\ndef monitor_dependency_resolution_performance():\n    \"\"\"Monitor dependency resolution performance.\"\"\"\n    start_time = time.time()\n    ordered_changes = resolve_dependencies(changes, master_catalog, branch_catalog)\n    end_time = time.time()\n\n    print(f\"Resolved {len(changes)} changes in {end_time - start_time:.2f} seconds\")\n    print(f\"Found {len(ordered_changes)} ordered operations\")\n</code></pre>"},{"location":"dependency-resolution/#summary","title":"Summary","text":"<p>pgdelta's dependency resolution system ensures that DDL migrations are generated in the correct order by:</p> <ol> <li>Extracting relevant dependencies from both current and target database states</li> <li>Applying semantic rules to generate ordering constraints between operations</li> <li>Solving constraints using proven graph algorithms to produce valid operation sequences</li> </ol> <p>This approach is: - Correct: Handles complex PostgreSQL dependency scenarios - Efficient: Focuses only on relevant objects and uses optimized algorithms - Maintainable: Separates concerns and uses clear abstractions - Extensible: New operation types only require additional semantic rules</p> <p>The system has been tested with complex real-world schemas and maintains pgdelta's core guarantee of roundtrip fidelity while ensuring all generated DDL can be applied successfully to PostgreSQL databases.</p>"},{"location":"api/cli/","title":"CLI Interface","text":"<p>pgdelta provides a command-line interface for generating schema diffs and DDL migrations.</p>"},{"location":"api/cli/#installation","title":"Installation","text":"<p>Note: pgdelta is not yet published to PyPI. Install from source:</p> <pre><code>git clone https://github.com/olirice/pgdelta.git\ncd pgdelta\npip install -e \".[dev]\"\n</code></pre>"},{"location":"api/cli/#commands","title":"Commands","text":""},{"location":"api/cli/#pgdelta-diff-headless","title":"<code>pgdelta diff-headless</code>","text":"<p>Generate a diff between two schemas using isolated Docker containers.</p> <pre><code>pgdelta diff-headless [OPTIONS]\n</code></pre> <p>This command creates temporary PostgreSQL containers, applies the provided SQL to create schemas, and generates the DDL needed to transform one schema to match the other.</p>"},{"location":"api/cli/#options","title":"Options","text":"<p><code>--master-sql TEXT</code> : SQL statements to create the master (target) schema</p> <p><code>--branch-sql TEXT</code> : SQL statements to create the branch (source) schema</p> <p><code>--initial-sql TEXT</code> : Optional SQL to run in both databases before applying schema SQL (useful for extensions, custom types, etc.)</p> <p><code>-i, --postgres-image TEXT</code> : PostgreSQL Docker image to use (default: <code>postgres:17</code>)</p> <p><code>-o, --output PATH</code> : Output file path (default: stdout)</p> <p><code>--verify / --no-verify</code> : Verify generated SQL with roundtrip test (default: <code>--verify</code>)</p> <p><code>-v, --verbose</code> : Show verbose output</p>"},{"location":"api/cli/#examples","title":"Examples","text":"<p>Basic usage: <pre><code>pgdelta diff-headless \\\n    --initial-sql \"CREATE TABLE users (id SERIAL PRIMARY KEY);\" \\\n    --branch-sql \"ALTER TABLE users ADD COLUMN email TEXT;\"\n</code></pre></p> <p>With multiple schemas: <pre><code>pgdelta diff-headless \\\n    --initial-sql \"CREATE SCHEMA app; CREATE TABLE app.users (id SERIAL PRIMARY KEY);\" \\\n    --branch-sql \"ALTER TABLE app.users ADD COLUMN email TEXT;\"\n</code></pre></p> <p>Output to file: <pre><code>pgdelta diff-headless \\\n    --initial-sql \"CREATE SCHEMA app; CREATE TABLE app.users (id SERIAL);\" \\\n    --branch-sql \"ALTER TABLE app.users ADD COLUMN email TEXT;\" \\\n    --output migration.sql\n</code></pre></p> <p>Using different PostgreSQL version: <pre><code>pgdelta diff-headless \\\n    --postgres-image \"postgres:16\" \\\n    --initial-sql \"CREATE TABLE test (id INTEGER);\" \\\n    --branch-sql \"ALTER TABLE test ADD COLUMN name TEXT;\"\n</code></pre></p>"},{"location":"api/cli/#output","title":"Output","text":"<p>The command generates SQL DDL statements that apply the branch changes to the initial schema:</p> <pre><code>ALTER TABLE \"users\" ADD COLUMN \"email\" text;\n</code></pre>"},{"location":"api/cli/#verification","title":"Verification","text":"<p>When <code>--verify</code> is enabled (default), pgdelta performs a roundtrip test:</p> <ol> <li>Applies the generated SQL to the master database</li> <li>Extracts the resulting schema</li> <li>Compares it with the branch schema</li> <li>Reports success or failure</li> </ol> <p>Verification success: <pre><code>\u2705 Verification passed - generated SQL is correct\n</code></pre></p> <p>Verification failure: <pre><code>\u26a0\ufe0f Verification failed - generated SQL may not be complete\n</code></pre></p>"},{"location":"api/cli/#pgdelta-info","title":"<code>pgdelta info</code>","text":"<p>Display pgdelta and system information.</p> <pre><code>pgdelta info\n</code></pre> <p>Shows: - pgdelta version - Python version and implementation - Operating system details - System architecture - Hardware information</p>"},{"location":"api/cli/#example-output","title":"Example Output","text":"<pre><code>\ud83d\udc18 pgdelta Information\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Property            \u2502 Value                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Version             \u2502 0.1.0                    \u2502\n\u2502 Python Version      \u2502 3.13.0 (CPython)        \u2502\n\u2502 Python Executable   \u2502 /usr/bin/python3.13      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\ud83d\udcbb System Information\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Property            \u2502 Value                    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Operating System    \u2502 Linux 6.2.0             \u2502\n\u2502 OS Version          \u2502 Ubuntu 22.04.3 LTS      \u2502\n\u2502 Machine Type        \u2502 x86_64                   \u2502\n\u2502 Architecture        \u2502 64bit                    \u2502\n\u2502 Processor           \u2502 x86_64                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"api/cli/#pgdelta-version","title":"<code>pgdelta --version</code>","text":"<p>Show version information and exit.</p> <pre><code>pgdelta --version\n</code></pre> <p>Output: <pre><code>pgdelta version 0.1.0\n</code></pre></p>"},{"location":"api/cli/#exit-codes","title":"Exit Codes","text":"<ul> <li>0: Success</li> <li>1: Error occurred (invalid arguments, SQL errors, verification failures, etc.)</li> </ul>"},{"location":"api/cli/#dependencies","title":"Dependencies","text":"<p>The CLI requires additional dependencies for container-based diffing:</p> <pre><code>pip install pgdelta[dev]\n</code></pre> <p>This installs: - <code>testcontainers</code> for Docker container management - <code>psycopg2-binary</code> for PostgreSQL connectivity - <code>sqlalchemy</code> for database operations</p>"},{"location":"api/cli/#docker-requirements","title":"Docker Requirements","text":"<p>The <code>diff-headless</code> command requires Docker to be installed and running:</p> <pre><code># Check if Docker is available\ndocker --version\n\n# Ensure Docker daemon is running\ndocker ps\n</code></pre>"},{"location":"api/cli/#common-use-cases","title":"Common Use Cases","text":""},{"location":"api/cli/#schema-migration-generation","title":"Schema Migration Generation","text":"<p>Future Interface (Planned): pgdelta will connect directly to two databases and diff their catalogs:</p> <pre><code># Planned interface - not yet implemented\npgdelta diff \\\n    --source \"postgresql://user:pass@prod-host/myapp\" \\\n    --target \"postgresql://user:pass@dev-host/myapp\" \\\n    --output migration.sql\n</code></pre> <p>Current Workaround: Use pg_dump followed by diff-headless:</p> <pre><code># Export your schemas to SQL files first\npg_dump --schema-only --no-owner myapp_dev &gt; dev_schema.sql\npg_dump --schema-only --no-owner myapp_prod &gt; prod_schema.sql\n\n# Generate migration\npgdelta diff-headless \\\n    --master-sql \"$(cat prod_schema.sql)\" \\\n    --branch-sql \"$(cat dev_schema.sql)\" \\\n    --output migration.sql\n</code></pre> <p>Note: Due to limited entity support (extensions, partitioned tables, etc.), the pg_dump approach may currently fail with complex schemas. The direct database connection interface will handle these limitations better.</p>"},{"location":"api/cli/#testing-schema-changes","title":"Testing Schema Changes","text":"<p>Verify that your manual migration scripts work correctly:</p> <pre><code># Test if your migration transforms schema A to schema B\npgdelta diff-headless \\\n    --master-sql \"$(cat schema_a.sql)\" \\\n    --branch-sql \"$(cat schema_b.sql)\" \\\n    --verify\n</code></pre>"},{"location":"api/cli/#automated-cicd-integration","title":"Automated CI/CD Integration","text":"<p>Use in CI/CD pipelines to validate schema changes:</p> <pre><code>#!/bin/bash\n# Compare feature branch schema with main branch\npgdelta diff-headless \\\n    --master-sql \"$(cat main_schema.sql)\" \\\n    --branch-sql \"$(cat feature_schema.sql)\" \\\n    --verify \\\n    --output migration.sql\n\n# Exit with error if verification fails\nif [ $? -ne 0 ]; then\n    echo \"Schema migration verification failed\"\n    exit 1\nfi\n</code></pre>"},{"location":"api/cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/cli/#docker-issues","title":"Docker Issues","text":"<p>Error: Cannot connect to Docker daemon <pre><code># Ensure Docker is running\nsudo systemctl start docker  # Linux\nopen -a Docker                # macOS\n</code></pre></p> <p>Error: Image not found <pre><code># Pull the PostgreSQL image first\ndocker pull postgres:17\n</code></pre></p>"},{"location":"api/cli/#memory-issues","title":"Memory Issues","text":"<p>For large schemas, increase Docker memory limits:</p> <pre><code># Check current Docker settings\ndocker system info | grep -i memory\n\n# Large schemas may need more memory\ndocker run --memory=4g postgres:17\n</code></pre>"},{"location":"api/cli/#sql-syntax-errors","title":"SQL Syntax Errors","text":"<p>Ensure your SQL is valid PostgreSQL syntax:</p> <pre><code># Test SQL syntax separately\npsql -c \"$(cat your_schema.sql)\" --dry-run\n</code></pre> <p>If you get syntax errors, check: - SQL statement terminators (semicolons) - Quoted identifiers - PostgreSQL-specific syntax</p>"},{"location":"api/cli/#permission-errors","title":"Permission Errors","text":"<p>Error: Permission denied <pre><code># Ensure user is in docker group (Linux)\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre></p>"},{"location":"api/python/","title":"Python API","text":"<p>pgdelta provides a Python API for programmatic schema diffing and DDL generation.</p>"},{"location":"api/python/#installation","title":"Installation","text":"<pre><code>pip install pgdelta\n</code></pre>"},{"location":"api/python/#basic-usage","title":"Basic Usage","text":"<pre><code>from pgdelta import PgCatalog, generate_sql\nfrom pgdelta.catalog import extract_catalog\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session\n\n# Connect to databases\nsource_engine = create_engine(\"postgresql://user:pass@localhost/source_db\")\ntarget_engine = create_engine(\"postgresql://user:pass@localhost/target_db\")\n\nwith Session(source_engine) as source_session, Session(target_engine) as target_session:\n    # Extract schemas\n    source_catalog = extract_catalog(source_session)\n    target_catalog = extract_catalog(target_session)\n\n    # Generate migration from target to source\n    changes = target_catalog.diff(source_catalog)\n\n    # Generate SQL statements\n    sql_statements = [generate_sql(change) for change in changes]\n\n    for sql in sql_statements:\n        print(sql)\n</code></pre>"},{"location":"api/python/#api-reference","title":"API Reference","text":""},{"location":"api/python/#core-functions","title":"Core Functions","text":""},{"location":"api/python/#pgdelta.catalog.extract_catalog","title":"<code>pgdelta.catalog.extract_catalog(session)</code>","text":"<p>Extract catalog from PostgreSQL database session.</p> Source code in <code>src/pgdelta/catalog.py</code> <pre><code>def extract_catalog(session: Session) -&gt; PgCatalog:\n    \"\"\"Extract catalog from PostgreSQL database session.\"\"\"\n    # Extract namespaces (schemas)\n    namespaces = extract_namespaces(session)\n\n    # Extract classes (tables, views, etc.)\n    classes = extract_classes(session)\n\n    # Extract attributes (columns)\n    attributes = extract_attributes(session)\n\n    # Extract constraints\n    constraints = extract_constraints(session)\n\n    # Extract indexes\n    indexes = extract_indexes(session)\n\n    # Extract sequences\n    sequences = extract_sequences(session)\n\n    # Extract RLS policies\n    policies = extract_policies(session)\n\n    # Extract procedures/functions\n    procedures = extract_procedures(session)\n\n    # Extract triggers\n    triggers = extract_triggers(session)\n\n    # Extract types\n    namespace_oids = [ns.oid for ns in namespaces]\n    types = extract_types(session, namespace_oids)\n\n    # Extract dependencies from pg_depend\n    depends = extract_depends(\n        session,\n        namespaces,\n        classes,\n        constraints,\n        indexes,\n        sequences,\n        policies,\n        procedures,\n        triggers,\n        types,\n    )\n\n    # Extract view dependencies from pg_rewrite and add them to depends list\n    view_deps = extract_view_dependencies_as_pg_depend(session, classes)\n    depends.extend(view_deps)\n\n    return catalog(\n        namespaces=namespaces,\n        classes=classes,\n        attributes=attributes,\n        constraints=constraints,\n        indexes=indexes,\n        sequences=sequences,\n        policies=policies,\n        procedures=procedures,\n        triggers=triggers,\n        types=types,\n        depends=depends,\n    )\n</code></pre>"},{"location":"api/python/#classes","title":"Classes","text":""},{"location":"api/python/#pgdelta.PgCatalog","title":"<code>pgdelta.PgCatalog</code>  <code>dataclass</code>","text":"<p>Immutable PostgreSQL catalog snapshot.</p> Source code in <code>src/pgdelta/catalog.py</code> <pre><code>@dataclass(frozen=True)\nclass PgCatalog:\n    \"\"\"Immutable PostgreSQL catalog snapshot.\"\"\"\n\n    namespaces: dict[str, PgNamespace]  # Keyed by stable_id (nspname)\n    classes: dict[str, PgClass]  # Keyed by stable_id (relkind:namespace.relname)\n    attributes: dict[str, PgAttribute]  # Keyed by stable_id (namespace.table.column)\n    constraints: dict[\n        str, PgConstraint\n    ]  # Keyed by stable_id (namespace.table.constraint_name)\n    indexes: dict[str, PgIndex]  # Keyed by stable_id (i:namespace.index_name)\n    sequences: dict[str, PgSequence]  # Keyed by stable_id (S:namespace.seqname)\n    policies: dict[str, PgPolicy]  # Keyed by stable_id (P:namespace.table.policy)\n    procedures: dict[\n        str, PgProc\n    ]  # Keyed by stable_id (function:namespace.name(argtypes))\n    triggers: dict[str, PgTrigger]  # Keyed by stable_id (trigger:namespace.table.name)\n    types: dict[str, PgType]  # Keyed by stable_id (type:namespace.typename)\n    depends: list[PgDepend]  # All dependencies\n\n    def diff(self, branch: PgCatalog) -&gt; list[DDL]:\n        \"\"\"Generate changes to transform this catalog to the branch catalog.\"\"\"\n        from .diff.orchestrator import diff_catalogs\n\n        return diff_catalogs(self, branch)\n\n    def get_class_attributes(self, class_stable_id: str) -&gt; list[PgAttribute]:\n        \"\"\"Get all attributes for a class (table/view/etc).\"\"\"\n        attributes = []\n        for attr in self.attributes.values():\n            if attr.class_stable_id == class_stable_id:\n                attributes.append(attr)\n\n        # Sort by column number for consistent ordering\n        return sorted(attributes, key=lambda col: col.attnum)\n\n    def semantically_equals(self, other: PgCatalog) -&gt; bool:\n        \"\"\"\n        Check if two catalogs are semantically equal.\n\n        This compares the logical structure of the database catalogs,\n        ignoring implementation details like OIDs, file nodes, and statistics.\n        \"\"\"\n\n        def _compare(left: BasePgModel | None, right: BasePgModel | None) -&gt; bool:\n            if left is None or right is None:\n                return False\n            return left.semantic_equality(right)\n\n        def _key(item: BasePgModel) -&gt; str:\n            return item.__class__.__name__ + item.stable_id\n\n        left_entities: list[BasePgModel] = []\n        right_entities: list[BasePgModel] = []\n        for field in fields(self):\n            # We don't need to compare dependencies\n            if field.name != \"depends\":\n                field_values = getattr(self, field.name).values()\n                other_field_values = getattr(other, field.name).values()\n\n                left_entities += field_values\n                right_entities += other_field_values\n\n        return (\n            flu(left_entities)\n            .join_full(\n                right_entities,\n                key=_key,\n                other_key=_key,\n            )\n            .map(lambda x: _compare(x[0], x[1]))\n            .filter(lambda x: not x)\n            .first(default=None)\n        ) is None\n</code></pre>"},{"location":"api/python/#pgdelta.PgCatalog.diff","title":"<code>diff(branch)</code>","text":"<p>Generate changes to transform this catalog to the branch catalog.</p> Source code in <code>src/pgdelta/catalog.py</code> <pre><code>def diff(self, branch: PgCatalog) -&gt; list[DDL]:\n    \"\"\"Generate changes to transform this catalog to the branch catalog.\"\"\"\n    from .diff.orchestrator import diff_catalogs\n\n    return diff_catalogs(self, branch)\n</code></pre>"},{"location":"api/python/#pgdelta.PgCatalog.get_class_attributes","title":"<code>get_class_attributes(class_stable_id)</code>","text":"<p>Get all attributes for a class (table/view/etc).</p> Source code in <code>src/pgdelta/catalog.py</code> <pre><code>def get_class_attributes(self, class_stable_id: str) -&gt; list[PgAttribute]:\n    \"\"\"Get all attributes for a class (table/view/etc).\"\"\"\n    attributes = []\n    for attr in self.attributes.values():\n        if attr.class_stable_id == class_stable_id:\n            attributes.append(attr)\n\n    # Sort by column number for consistent ordering\n    return sorted(attributes, key=lambda col: col.attnum)\n</code></pre>"},{"location":"api/python/#pgdelta.PgCatalog.semantically_equals","title":"<code>semantically_equals(other)</code>","text":"<p>Check if two catalogs are semantically equal.</p> <p>This compares the logical structure of the database catalogs, ignoring implementation details like OIDs, file nodes, and statistics.</p> Source code in <code>src/pgdelta/catalog.py</code> <pre><code>def semantically_equals(self, other: PgCatalog) -&gt; bool:\n    \"\"\"\n    Check if two catalogs are semantically equal.\n\n    This compares the logical structure of the database catalogs,\n    ignoring implementation details like OIDs, file nodes, and statistics.\n    \"\"\"\n\n    def _compare(left: BasePgModel | None, right: BasePgModel | None) -&gt; bool:\n        if left is None or right is None:\n            return False\n        return left.semantic_equality(right)\n\n    def _key(item: BasePgModel) -&gt; str:\n        return item.__class__.__name__ + item.stable_id\n\n    left_entities: list[BasePgModel] = []\n    right_entities: list[BasePgModel] = []\n    for field in fields(self):\n        # We don't need to compare dependencies\n        if field.name != \"depends\":\n            field_values = getattr(self, field.name).values()\n            other_field_values = getattr(other, field.name).values()\n\n            left_entities += field_values\n            right_entities += other_field_values\n\n    return (\n        flu(left_entities)\n        .join_full(\n            right_entities,\n            key=_key,\n            other_key=_key,\n        )\n        .map(lambda x: _compare(x[0], x[1]))\n        .filter(lambda x: not x)\n        .first(default=None)\n    ) is None\n</code></pre>"},{"location":"api/python/#functions","title":"Functions","text":""},{"location":"api/python/#pgdelta.generate_sql","title":"<code>pgdelta.generate_sql(change)</code>","text":"<p>Generate SQL for a DDL change.</p> Source code in <code>src/pgdelta/changes/dispatcher.py</code> <pre><code>def generate_sql(change: DDL) -&gt; str:\n    \"\"\"Generate SQL for a DDL change.\"\"\"\n    sql = \"\"\n\n    match change:\n        case CreateSchema():\n            sql = generate_create_schema_sql(change)\n        case DropSchema():\n            sql = generate_drop_schema_sql(change)\n        case CreateTable():\n            sql = generate_create_table_sql(change)\n        case DropTable():\n            sql = generate_drop_table_sql(change)\n        case AlterTable():\n            sql = generate_alter_table_sql(change)\n        case CreateView():\n            sql = generate_create_view_sql(change)\n        case DropView():\n            sql = generate_drop_view_sql(change)\n        case ReplaceView():\n            sql = generate_replace_view_sql(change)\n        case CreateMaterializedView():\n            sql = generate_create_materialized_view_sql(change)\n        case DropMaterializedView():\n            sql = generate_drop_materialized_view_sql(change)\n        case ReplaceMaterializedView():\n            sql = generate_replace_materialized_view_sql(change)\n        case CreateConstraint():\n            sql = generate_create_constraint_sql(change)\n        case DropConstraint():\n            sql = generate_drop_constraint_sql(change)\n        case AlterConstraint():\n            sql = generate_alter_constraint_sql(change)\n        case CreateFunction():\n            sql = generate_create_function_sql(change)\n        case DropFunction():\n            sql = generate_drop_function_sql(change)\n        case ReplaceFunction():\n            sql = generate_replace_function_sql(change)\n        case CreateIndex():\n            sql = generate_create_index_sql(change)\n        case DropIndex():\n            sql = generate_drop_index_sql(change)\n        case AlterIndex():\n            raise NotImplementedError(\"ALTER INDEX operations are not yet implemented\")\n        case CreateSequence():\n            sql = generate_create_sequence_sql(change)\n        case DropSequence():\n            sql = generate_drop_sequence_sql(change)\n        case AlterSequence():\n            sql = generate_alter_sequence_sql(change)\n        case CreatePolicy():\n            sql = generate_create_policy_sql(change)\n        case DropPolicy():\n            sql = generate_drop_policy_sql(change)\n        case AlterPolicy():\n            sql = generate_alter_policy_sql(change)\n        case RenamePolicyTo():\n            sql = generate_rename_policy_sql(change)\n        case CreateTrigger():\n            sql = generate_create_trigger_sql(change)\n        case DropTrigger():\n            sql = generate_drop_trigger_sql(change)\n        case CreateType():\n            sql = generate_create_type_sql(change)\n        case DropType():\n            sql = generate_drop_type_sql(change)\n        case (\n            AlterTypeOwnerTo()\n            | AlterTypeRename()\n            | AlterTypeSetSchema()\n            | AlterTypeAddAttribute()\n            | AlterTypeDropAttribute()\n            | AlterTypeAlterAttribute()\n            | AlterTypeAddValue()\n            | AlterTypeRenameValue()\n        ):\n            sql = generate_alter_type_sql(change)\n        case _:\n            assert_never(change)\n\n    # Log SQL generation\n    logger.debug(\n        \"sql.generated\",\n        extra={\n            \"change_type\": type(change).__name__,\n            \"stable_id\": getattr(change, \"stable_id\", None),\n            \"sql\": sql,\n        },\n    )\n\n    return sql\n</code></pre>"},{"location":"api/python/#exceptions","title":"Exceptions","text":""},{"location":"api/python/#pgdelta.PgDeltaError","title":"<code>pgdelta.PgDeltaError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception class for all pgdelta-specific exceptions.</p> <p>This serves as the root of the exception hierarchy and allows users to catch all pgdelta-related exceptions with a single except clause.</p> Source code in <code>src/pgdelta/exceptions.py</code> <pre><code>class PgDeltaError(Exception):\n    \"\"\"\n    Base exception class for all pgdelta-specific exceptions.\n\n    This serves as the root of the exception hierarchy and allows users\n    to catch all pgdelta-related exceptions with a single except clause.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/python/#pgdelta.DependencyResolutionError","title":"<code>pgdelta.DependencyResolutionError</code>","text":"<p>               Bases: <code>PgDeltaError</code></p> <p>Exception raised when dependency resolution fails.</p> <p>This exception is raised when the dependency resolver encounters an unresolvable situation, such as cyclic dependencies between database objects.</p> Source code in <code>src/pgdelta/exceptions.py</code> <pre><code>class DependencyResolutionError(PgDeltaError):\n    \"\"\"\n    Exception raised when dependency resolution fails.\n\n    This exception is raised when the dependency resolver encounters\n    an unresolvable situation, such as cyclic dependencies between\n    database objects.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/python/#pgdelta.CyclicDependencyError","title":"<code>pgdelta.CyclicDependencyError</code>","text":"<p>               Bases: <code>DependencyResolutionError</code></p> <p>Exception raised when a cyclic dependency is detected.</p> <p>This specific exception is raised when the dependency resolver detects a cycle in the dependency graph that cannot be resolved through standard topological sorting.</p> Source code in <code>src/pgdelta/exceptions.py</code> <pre><code>class CyclicDependencyError(DependencyResolutionError):\n    \"\"\"\n    Exception raised when a cyclic dependency is detected.\n\n    This specific exception is raised when the dependency resolver\n    detects a cycle in the dependency graph that cannot be resolved\n    through standard topological sorting.\n    \"\"\"\n\n    def __init__(self, message: str = \"Cyclic dependency detected in DDL operations\"):\n        \"\"\"\n        Initialize the cyclic dependency error.\n\n        Args:\n            message: Custom error message describing the cyclic dependency\n        \"\"\"\n        super().__init__(message)\n        self.message = message\n</code></pre>"},{"location":"api/python/#pgdelta.CyclicDependencyError.__init__","title":"<code>__init__(message='Cyclic dependency detected in DDL operations')</code>","text":"<p>Initialize the cyclic dependency error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Custom error message describing the cyclic dependency</p> <code>'Cyclic dependency detected in DDL operations'</code> Source code in <code>src/pgdelta/exceptions.py</code> <pre><code>def __init__(self, message: str = \"Cyclic dependency detected in DDL operations\"):\n    \"\"\"\n    Initialize the cyclic dependency error.\n\n    Args:\n        message: Custom error message describing the cyclic dependency\n    \"\"\"\n    super().__init__(message)\n    self.message = message\n</code></pre>"},{"location":"api/python/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/python/#custom-connection-handling","title":"Custom Connection Handling","text":"<pre><code>from sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session\nfrom pgdelta.catalog import extract_catalog\n\ndef extract_from_database(connection_string: str) -&gt; PgCatalog:\n    \"\"\"Extract catalog from a database connection string.\"\"\"\n    engine = create_engine(connection_string)\n    with Session(engine) as session:\n        return extract_catalog(session)\n\n# Usage\nsource_catalog = extract_from_database(\"postgresql://user:pass@localhost/db1\")\ntarget_catalog = extract_from_database(\"postgresql://user:pass@localhost/db2\")\n</code></pre>"},{"location":"api/python/#filtering-changes","title":"Filtering Changes","text":"<pre><code>from pgdelta.changes import CreateTable, DropTable\n\n# Extract catalogs\nsource_catalog = extract_catalog(source_session)\ntarget_catalog = extract_catalog(target_session)\n\n# Get all changes\nall_changes = target_catalog.diff(source_catalog)\n\n# Filter only table creation changes\ntable_creates = [change for change in all_changes if isinstance(change, CreateTable)]\n\n# Filter only table drops\ntable_drops = [change for change in all_changes if isinstance(change, DropTable)]\n\n# Generate SQL for specific changes\ncreate_sql = [generate_sql(change) for change in table_creates]\n</code></pre>"},{"location":"api/python/#semantic-equality-checking","title":"Semantic Equality Checking","text":"<pre><code># Check if two catalogs are semantically identical\nif source_catalog.semantically_equals(target_catalog):\n    print(\"Schemas are identical\")\nelse:\n    print(\"Schemas differ\")\n    changes = source_catalog.diff(target_catalog)\n    print(f\"Found {len(changes)} changes\")\n</code></pre>"},{"location":"api/python/#working-with-individual-models","title":"Working with Individual Models","text":"<pre><code>from pgdelta.model import PgClass, PgAttribute\n\n# Access individual tables\nfor table in source_catalog.tables:\n    print(f\"Table: {table.schema}.{table.name}\")\n\n    # Access columns\n    for column in table.columns:\n        print(f\"  Column: {column.name} ({column.type_name})\")\n\n        # Check column properties\n        if not column.is_nullable:\n            print(f\"    NOT NULL\")\n        if column.has_default:\n            print(f\"    DEFAULT {column.default}\")\n</code></pre>"},{"location":"api/python/#error-handling","title":"Error Handling","text":"<pre><code>from pgdelta import DependencyResolutionError, CyclicDependencyError\n\ntry:\n    changes = source_catalog.diff(target_catalog)\n    sql_statements = [generate_sql(change) for change in changes]\n\nexcept DependencyResolutionError as e:\n    print(f\"Could not resolve dependencies: {e}\")\n\nexcept CyclicDependencyError as e:\n    print(f\"Cyclic dependency detected: {e}\")\n\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/python/#integration-examples","title":"Integration Examples","text":""},{"location":"api/python/#flask-application","title":"Flask Application","text":"<pre><code>from flask import Flask, request, jsonify\nfrom pgdelta import extract_catalog, generate_sql\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session\n\napp = Flask(__name__)\n\n@app.route('/diff', methods=['POST'])\ndef generate_diff():\n    \"\"\"Generate schema diff between two databases.\"\"\"\n    data = request.json\n    source_url = data['source_url']\n    target_url = data['target_url']\n\n    try:\n        source_engine = create_engine(source_url)\n        target_engine = create_engine(target_url)\n\n        with Session(source_engine) as source_session, \\\n             Session(target_engine) as target_session:\n\n            source_catalog = extract_catalog(source_session)\n            target_catalog = extract_catalog(target_session)\n\n            changes = source_catalog.diff(target_catalog)\n            sql_statements = [generate_sql(change) for change in changes]\n\n            return jsonify({\n                'success': True,\n                'sql': sql_statements,\n                'change_count': len(changes)\n            })\n\n    except Exception as e:\n        return jsonify({\n            'success': False,\n            'error': str(e)\n        }), 500\n</code></pre>"},{"location":"api/python/#django-management-command","title":"Django Management Command","text":"<pre><code>from django.core.management.base import BaseCommand\nfrom django.db import connection\nfrom pgdelta.catalog import extract_catalog\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import Session\n\nclass Command(BaseCommand):\n    help = 'Generate schema diff'\n\n    def add_arguments(self, parser):\n        parser.add_argument('--target-url', required=True)\n        parser.add_argument('--output', required=False)\n\n    def handle(self, *args, **options):\n        # Use Django's database connection for source\n        django_url = f\"postgresql://{connection.settings_dict['USER']}:\" \\\n                    f\"{connection.settings_dict['PASSWORD']}@\" \\\n                    f\"{connection.settings_dict['HOST']}:\" \\\n                    f\"{connection.settings_dict['PORT']}/\" \\\n                    f\"{connection.settings_dict['NAME']}\"\n\n        source_engine = create_engine(django_url)\n        target_engine = create_engine(options['target_url'])\n\n        with Session(source_engine) as source_session, \\\n             Session(target_engine) as target_session:\n\n            source_catalog = extract_catalog(source_session)\n            target_catalog = extract_catalog(target_session)\n\n            changes = source_catalog.diff(target_catalog)\n\n            if not changes:\n                self.stdout.write(\"No changes detected\")\n                return\n\n            sql_statements = [generate_sql(change) for change in changes]\n\n            if options['output']:\n                with open(options['output'], 'w') as f:\n                    f.write('\\n'.join(sql_statements))\n                self.stdout.write(f\"Wrote {len(sql_statements)} statements to {options['output']}\")\n            else:\n                for sql in sql_statements:\n                    self.stdout.write(sql)\n</code></pre>"},{"location":"api/python/#async-usage","title":"Async Usage","text":"<pre><code>import asyncio\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom pgdelta.catalog import extract_catalog\n\nasync def async_diff():\n    \"\"\"Example of using pgdelta with async SQLAlchemy.\"\"\"\n\n    # Note: extract_catalog currently requires sync sessions\n    # This is a pattern for working with async engines\n\n    source_engine = create_async_engine(\"postgresql+asyncpg://user:pass@localhost/db1\")\n    target_engine = create_async_engine(\"postgresql+asyncpg://user:pass@localhost/db2\")\n\n    # Convert to sync for extraction\n    source_sync = source_engine.sync_engine\n    target_sync = target_engine.sync_engine\n\n    with Session(source_sync) as source_session, \\\n         Session(target_sync) as target_session:\n\n        source_catalog = extract_catalog(source_session)\n        target_catalog = extract_catalog(target_session)\n\n        changes = source_catalog.diff(target_catalog)\n        sql_statements = [generate_sql(change) for change in changes]\n\n        return sql_statements\n\n# Usage\nasync def main():\n    statements = await async_diff()\n    for sql in statements:\n        print(sql)\n\nasyncio.run(main())\n</code></pre>"},{"location":"api/python/#testing","title":"Testing","text":""},{"location":"api/python/#unit-testing-with-pytest","title":"Unit Testing with pytest","text":"<pre><code>import pytest\nfrom pgdelta import PgCatalog, generate_sql\nfrom pgdelta.catalog import extract_catalog\nfrom sqlalchemy import create_engine, text\nfrom sqlalchemy.orm import Session\nfrom testcontainers.postgres import PostgresContainer\n\n@pytest.fixture\ndef postgres_container():\n    \"\"\"Pytest fixture providing a PostgreSQL container.\"\"\"\n    with PostgresContainer(\"postgres:17\") as container:\n        yield container\n\ndef test_table_creation_diff(postgres_container):\n    \"\"\"Test that table creation is detected correctly.\"\"\"\n\n    # Get connection URL\n    url = postgres_container.get_connection_url()\n    engine = create_engine(url)\n\n    with Session(engine) as session:\n        # Create initial schema\n        session.execute(text(\"CREATE SCHEMA test\"))\n        session.commit()\n\n        # Extract empty catalog\n        empty_catalog = extract_catalog(session)\n\n        # Add a table\n        session.execute(text(\"\"\"\n            CREATE TABLE test.users (\n                id SERIAL PRIMARY KEY,\n                email TEXT NOT NULL\n            )\n        \"\"\"))\n        session.commit()\n\n        # Extract catalog with table\n        table_catalog = extract_catalog(session)\n\n        # Generate diff\n        changes = empty_catalog.diff(table_catalog)\n\n        # Should have one CREATE TABLE change\n        assert len(changes) == 1\n        assert \"CREATE TABLE\" in generate_sql(changes[0])\n        assert \"test\" in generate_sql(changes[0])\n        assert \"users\" in generate_sql(changes[0])\n</code></pre>"},{"location":"api/python/#integration-testing","title":"Integration Testing","text":"<pre><code>from pgdelta.catalog import extract_catalog\nfrom sqlalchemy import create_engine, text\nfrom sqlalchemy.orm import Session\n\ndef test_roundtrip_fidelity():\n    \"\"\"Test that Extract \u2192 Diff \u2192 Generate \u2192 Apply produces identical schemas.\"\"\"\n\n    # Setup two identical databases\n    source_engine = create_engine(\"postgresql://user:pass@localhost/source\")\n    target_engine = create_engine(\"postgresql://user:pass@localhost/target\")\n\n    with Session(source_engine) as source_session, \\\n         Session(target_engine) as target_session:\n\n        # Apply initial schema to source\n        source_session.execute(text(\"\"\"\n            CREATE SCHEMA app;\n            CREATE TABLE app.users (\n                id SERIAL PRIMARY KEY,\n                email TEXT UNIQUE NOT NULL\n            );\n        \"\"\"))\n        source_session.commit()\n\n        # Extract catalogs\n        source_catalog = extract_catalog(source_session)\n        target_catalog = extract_catalog(target_session)\n\n        # Generate migration\n        changes = target_catalog.diff(source_catalog)\n        sql_statements = [generate_sql(change) for change in changes]\n\n        # Apply migration to target\n        for sql in sql_statements:\n            target_session.execute(text(sql))\n        target_session.commit()\n\n        # Extract final catalog\n        final_catalog = extract_catalog(target_session)\n\n        # Should be semantically identical\n        assert source_catalog.semantically_equals(final_catalog)\n</code></pre>"},{"location":"api/python/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/python/#large-schema-handling","title":"Large Schema Handling","text":"<pre><code># For very large schemas, consider extracting only specific schemas\nfrom pgdelta.catalog import extract_catalog\n\n# Extract only specific schemas (not yet implemented, but planned)\n# catalog = extract_catalog(session, schema_filter=['public', 'app'])\n\n# Current approach - extract all and filter\ncatalog = extract_catalog(session)\nfiltered_tables = [t for t in catalog.tables if t.schema in ['public', 'app']]\n</code></pre>"},{"location":"api/python/#best-practices","title":"Best Practices","text":"<ol> <li>Use context managers for database connections</li> <li>Handle exceptions appropriately for production code</li> <li>Test with real databases using testcontainers</li> <li>Validate generated SQL before applying to production</li> <li>Use semantic equality to verify migrations</li> <li>Filter changes when you only need specific types</li> <li>Test with representative data before production use</li> </ol>"},{"location":"contributing/adding-entities/","title":"Adding New Entity Types","text":"<p>This guide walks through the process of adding support for a new PostgreSQL entity type to pgdelta, using indexes as a comprehensive example.</p>"},{"location":"contributing/adding-entities/#overview","title":"Overview","text":"<p>Adding a new entity type to pgdelta involves several steps:</p> <ol> <li>Model Creation: Define the PostgreSQL object model</li> <li>Extraction: Extract objects from PostgreSQL catalogs</li> <li>Diffing: Compare objects between catalogs</li> <li>Change Types: Define create/drop/alter operations</li> <li>SQL Generation: Generate DDL from change objects</li> <li>Testing: Comprehensive test coverage</li> <li>Documentation: Update entity documentation</li> </ol>"},{"location":"contributing/adding-entities/#step-by-step-guide-adding-index-support","title":"Step-by-Step Guide: Adding Index Support","text":"<p>Let's walk through adding index support to demonstrate the complete process.</p>"},{"location":"contributing/adding-entities/#step-1-model-creation","title":"Step 1: Model Creation","text":"<p>Create the PostgreSQL model in <code>src/pgdelta/model/pg_index.py</code>:</p> <pre><code>\"\"\"PostgreSQL index model.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING\n\nfrom sqlalchemy import text\n\nfrom .base import BasePgModel\n\nif TYPE_CHECKING:\n    from sqlalchemy.orm import Session\n\n\n@dataclass(frozen=True)\nclass PgIndex(BasePgModel):\n    \"\"\"PostgreSQL index model.\"\"\"\n\n    # Identity fields (uniquely identify the index)\n    indexname: str = field(metadata={\"tag\": \"identity\"})\n    schemaname: str = field(metadata={\"tag\": \"identity\"})\n\n    # Data fields (index properties)\n    tablename: str = field(metadata={\"tag\": \"data\"})\n    index_definition: str = field(metadata={\"tag\": \"data\"})\n    is_unique: bool = field(metadata={\"tag\": \"data\"})\n    is_primary: bool = field(metadata={\"tag\": \"data\"})\n    is_exclusion: bool = field(metadata={\"tag\": \"data\"})\n\n    # Internal fields (PostgreSQL internals)\n    oid: int = field(metadata={\"tag\": \"internal\"})\n\n    @property\n    def stable_id(self) -&gt; str:\n        \"\"\"Cross-database portable identifier.\"\"\"\n        return f\"i:{self.schemaname}.{self.indexname}\"\n\n    @property\n    def table_stable_id(self) -&gt; str:\n        \"\"\"Stable ID of the table this index is on.\"\"\"\n        return f\"t:{self.schemaname}.{self.tablename}\"\n\n\ndef extract_indexes(session: Session) -&gt; list[PgIndex]:\n    \"\"\"Extract indexes from PostgreSQL.\"\"\"\n\n    # Use PostgreSQL's information_schema and pg_* tables\n    query = text(\"\"\"\n        SELECT\n            i.indexname,\n            i.schemaname,\n            i.tablename,\n            pg_get_indexdef(pi.oid) as index_definition,\n            pi.indisunique as is_unique,\n            pi.indisprimary as is_primary,\n            pi.indisexclusion as is_exclusion,\n            pi.oid\n        FROM pg_indexes i\n        JOIN pg_class pc ON pc.relname = i.tablename\n        JOIN pg_namespace pn ON pn.nspname = i.schemaname AND pn.oid = pc.relnamespace\n        JOIN pg_index pi ON pi.indexrelid = (\n            SELECT oid FROM pg_class\n            WHERE relname = i.indexname\n            AND relnamespace = pn.oid\n        )\n        WHERE i.schemaname NOT IN ('information_schema', 'pg_catalog', 'pg_toast')\n        ORDER BY i.schemaname, i.tablename, i.indexname\n    \"\"\")\n\n    result = session.execute(query)\n    indexes = []\n\n    for row in result:\n        index = PgIndex(\n            indexname=row.indexname,\n            schemaname=row.schemaname,\n            tablename=row.tablename,\n            index_definition=row.index_definition,\n            is_unique=row.is_unique,\n            is_primary=row.is_primary,\n            is_exclusion=row.is_exclusion,\n            oid=row.oid,\n        )\n        indexes.append(index)\n\n    return indexes\n</code></pre> <p>Key Points: - Use <code>@dataclass(frozen=True)</code> for immutability - Tag fields with metadata: <code>identity</code>, <code>data</code>, or <code>internal</code> - Implement <code>stable_id</code> property for cross-database identification - Use PostgreSQL system catalogs for extraction - Filter out system schemas</p>"},{"location":"contributing/adding-entities/#step-2-catalog-integration","title":"Step 2: Catalog Integration","text":"<p>Update <code>src/pgdelta/catalog.py</code> to include indexes:</p> <pre><code>from .model.pg_index import PgIndex, extract_indexes\n\n@dataclass(frozen=True)\nclass PgCatalog:\n    \"\"\"Immutable PostgreSQL catalog snapshot.\"\"\"\n\n    # ... existing fields ...\n    indexes: dict[str, PgIndex]  # Keyed by stable_id\n    # ... rest of fields ...\n\ndef extract_catalog(session: Session) -&gt; PgCatalog:\n    \"\"\"Extract complete catalog from PostgreSQL session.\"\"\"\n\n    # Extract all object types\n    namespaces = extract_namespaces(session)\n    classes = extract_classes(session)\n    attributes = extract_attributes(session)\n    constraints = extract_constraints(session)\n    indexes = extract_indexes(session)  # Add this line\n    # ... other extractions ...\n\n    return PgCatalog(\n        namespaces={ns.stable_id: ns for ns in namespaces},\n        classes={cls.stable_id: cls for cls in classes},\n        attributes={attr.stable_id: attr for attr in attributes},\n        constraints={cons.stable_id: cons for cons in constraints},\n        indexes={idx.stable_id: idx for idx in indexes},  # Add this line\n        # ... other collections ...\n    )\n</code></pre>"},{"location":"contributing/adding-entities/#step-3-diffing-logic","title":"Step 3: Diffing Logic","text":"<p>Create diff logic in <code>src/pgdelta/diff/orchestrator.py</code>:</p> <pre><code>def diff_catalogs(master: PgCatalog, branch: PgCatalog) -&gt; list[DDL]:\n    \"\"\"Generate changes to transform master to branch.\"\"\"\n    changes = []\n\n    # ... existing diffs ...\n\n    # Add index diffing\n    changes.extend(diff_indexes(master.indexes, branch.indexes))\n\n    # ... rest of diffs ...\n\n    return changes\n\ndef diff_indexes(\n    master_indexes: dict[str, PgIndex],\n    branch_indexes: dict[str, PgIndex],\n) -&gt; list[DDL]:\n    \"\"\"Diff indexes between catalogs.\"\"\"\n    changes = []\n\n    # Find indexes to create (in branch but not master)\n    for stable_id, branch_index in branch_indexes.items():\n        if stable_id not in master_indexes:\n            changes.append(CreateIndex(\n                stable_id=stable_id,\n                index=branch_index\n            ))\n\n    # Find indexes to drop (in master but not branch)\n    for stable_id, master_index in master_indexes.items():\n        if stable_id not in branch_indexes:\n            changes.append(DropIndex(\n                stable_id=stable_id,\n                index=master_index\n            ))\n\n    # Find indexes to alter (in both but different)\n    for stable_id, master_index in master_indexes.items():\n        if stable_id in branch_indexes:\n            branch_index = branch_indexes[stable_id]\n            if not master_index.semantic_equality(branch_index):\n                # For indexes, we typically drop and recreate\n                changes.append(DropIndex(\n                    stable_id=stable_id,\n                    index=master_index\n                ))\n                changes.append(CreateIndex(\n                    stable_id=stable_id,\n                    index=branch_index\n                ))\n\n    return changes\n</code></pre>"},{"location":"contributing/adding-entities/#step-4-change-types","title":"Step 4: Change Types","text":"<p>Create change types in <code>src/pgdelta/changes/index/</code>:</p>"},{"location":"contributing/adding-entities/#srcpgdeltachangesindex__init__py","title":"<code>src/pgdelta/changes/index/__init__.py</code>","text":"<pre><code>\"\"\"Index change types.\"\"\"\n\nfrom .create import CreateIndex\nfrom .drop import DropIndex\n\n__all__ = [\"CreateIndex\", \"DropIndex\"]\n</code></pre>"},{"location":"contributing/adding-entities/#srcpgdeltachangesindexcreatepy","title":"<code>src/pgdelta/changes/index/create.py</code>","text":"<pre><code>\"\"\"Create index change type and SQL generation.\"\"\"\n\nfrom dataclasses import dataclass\n\nfrom ...model.pg_index import PgIndex\n\n\n@dataclass(frozen=True)\nclass CreateIndex:\n    \"\"\"Create index change.\"\"\"\n\n    stable_id: str  # i:namespace.index_name\n    index: PgIndex\n\n\ndef generate_create_index_sql(change: CreateIndex) -&gt; str:\n    \"\"\"Generate CREATE INDEX SQL from the stored index definition.\"\"\"\n    # PostgreSQL's pg_get_indexdef() returns the complete CREATE INDEX statement\n    index_def = change.index.index_definition\n\n    # Ensure it ends with a semicolon for consistency\n    if not index_def.endswith(\";\"):\n        index_def += \";\"\n\n    return index_def\n</code></pre>"},{"location":"contributing/adding-entities/#srcpgdeltachangesindexdroppy","title":"<code>src/pgdelta/changes/index/drop.py</code>","text":"<pre><code>\"\"\"Drop index change type and SQL generation.\"\"\"\n\nfrom dataclasses import dataclass\n\nfrom ...model.pg_index import PgIndex\n\n\n@dataclass(frozen=True)\nclass DropIndex:\n    \"\"\"Drop index change.\"\"\"\n\n    stable_id: str  # i:namespace.index_name\n    index: PgIndex\n\n\ndef generate_drop_index_sql(change: DropIndex) -&gt; str:\n    \"\"\"Generate DROP INDEX SQL.\"\"\"\n    quoted_schema = f'\"{change.index.schemaname}\"'\n    quoted_index = f'\"{change.index.indexname}\"'\n\n    return f\"DROP INDEX {quoted_schema}.{quoted_index};\"\n</code></pre>"},{"location":"contributing/adding-entities/#step-5-sql-generation-integration","title":"Step 5: SQL Generation Integration","text":"<p>Update <code>src/pgdelta/changes/dispatcher.py</code>:</p> <pre><code>from .index import CreateIndex, DropIndex\nfrom .index.create import generate_create_index_sql\nfrom .index.drop import generate_drop_index_sql\n\ndef generate_sql(change: DDL) -&gt; str:\n    \"\"\"Generate SQL for a change object using structural pattern matching.\"\"\"\n\n    match change:\n        # ... existing cases ...\n\n        case CreateIndex() as create_index:\n            return generate_create_index_sql(create_index)\n\n        case DropIndex() as drop_index:\n            return generate_drop_index_sql(drop_index)\n\n        # ... rest of cases ...\n\n        case _:\n            msg = f\"Unsupported change type: {type(change)}\"\n            raise NotImplementedError(msg)\n</code></pre>"},{"location":"contributing/adding-entities/#step-6-dependencies","title":"Step 6: Dependencies","text":"<p>Update dependency tracking in <code>src/pgdelta/model/pg_depend.py</code>:</p> <pre><code>def extract_depends(\n    session: Session,\n    namespaces: list[PgNamespace],\n    classes: list[PgClass],\n    constraints: list[PgConstraint],\n    indexes: list[PgIndex],  # Add this parameter\n    # ... other parameters ...\n) -&gt; list[PgDepend]:\n    \"\"\"Extract dependencies from pg_depend.\"\"\"\n\n    # ... existing OID mappings ...\n\n    # Map index OIDs (indexes also use pg_class)\n    for index in indexes:\n        oid_to_stable_id[(\"pg_class\", index.oid)] = index.stable_id\n\n    # ... rest of function ...\n</code></pre>"},{"location":"contributing/adding-entities/#step-7-testing","title":"Step 7: Testing","text":"<p>Create comprehensive tests in <code>tests/</code>:</p>"},{"location":"contributing/adding-entities/#unit-tests-testsunittest_indexpy","title":"Unit Tests: <code>tests/unit/test_index.py</code>","text":"<pre><code>\"\"\"Unit tests for index functionality.\"\"\"\n\nimport pytest\nfrom pgdelta.model.pg_index import PgIndex\nfrom pgdelta.changes.index import CreateIndex, DropIndex\nfrom pgdelta.changes.index.create import generate_create_index_sql\nfrom pgdelta.changes.index.drop import generate_drop_index_sql\n\n\ndef test_pg_index_stable_id():\n    \"\"\"Test PgIndex stable_id property.\"\"\"\n    index = PgIndex(\n        indexname=\"idx_users_email\",\n        schemaname=\"public\",\n        tablename=\"users\",\n        index_definition=\"CREATE INDEX ...\",\n        is_unique=False,\n        is_primary=False,\n        is_exclusion=False,\n        oid=12345,\n    )\n\n    assert index.stable_id == \"i:public.idx_users_email\"\n\n\ndef test_pg_index_table_stable_id():\n    \"\"\"Test PgIndex table_stable_id property.\"\"\"\n    index = PgIndex(\n        indexname=\"idx_users_email\",\n        schemaname=\"public\",\n        tablename=\"users\",\n        index_definition=\"CREATE INDEX ...\",\n        is_unique=False,\n        is_primary=False,\n        is_exclusion=False,\n        oid=12345,\n    )\n\n    assert index.table_stable_id == \"t:public.users\"\n\n\ndef test_create_index_sql_generation():\n    \"\"\"Test CREATE INDEX SQL generation.\"\"\"\n    index = PgIndex(\n        indexname=\"idx_users_email\",\n        schemaname=\"public\",\n        tablename=\"users\",\n        index_definition='CREATE INDEX \"idx_users_email\" ON \"public\".\"users\" (\"email\")',\n        is_unique=False,\n        is_primary=False,\n        is_exclusion=False,\n        oid=12345,\n    )\n\n    change = CreateIndex(\n        stable_id=\"i:public.idx_users_email\",\n        index=index\n    )\n\n    sql = generate_create_index_sql(change)\n    assert 'CREATE INDEX \"idx_users_email\"' in sql\n    assert 'ON \"public\".\"users\"' in sql\n    assert sql.endswith(\";\")\n\n\ndef test_drop_index_sql_generation():\n    \"\"\"Test DROP INDEX SQL generation.\"\"\"\n    index = PgIndex(\n        indexname=\"idx_users_email\",\n        schemaname=\"public\",\n        tablename=\"users\",\n        index_definition='CREATE INDEX \"idx_users_email\" ON \"public\".\"users\" (\"email\")',\n        is_unique=False,\n        is_primary=False,\n        is_exclusion=False,\n        oid=12345,\n    )\n\n    change = DropIndex(\n        stable_id=\"i:public.idx_users_email\",\n        index=index\n    )\n\n    sql = generate_drop_index_sql(change)\n    assert sql == 'DROP INDEX \"public\".\"idx_users_email\";'\n</code></pre>"},{"location":"contributing/adding-entities/#integration-tests-testsintegrationtest_index_roundtrippy","title":"Integration Tests: <code>tests/integration/test_index_roundtrip.py</code>","text":"<pre><code>\"\"\"Integration tests for index roundtrip fidelity.\"\"\"\n\nimport pytest\nfrom sqlalchemy import text\nfrom pgdelta.catalog import extract_catalog\nfrom pgdelta.changes.dispatcher import generate_sql\n\n\ndef test_index_creation_roundtrip(postgres_session):\n    \"\"\"Test index creation roundtrip fidelity.\"\"\"\n    # Create table and index\n    postgres_session.execute(text(\"\"\"\n        CREATE TABLE test_table (\n            id SERIAL PRIMARY KEY,\n            email TEXT NOT NULL\n        );\n        CREATE INDEX idx_test_email ON test_table (email);\n    \"\"\"))\n    postgres_session.commit()\n\n    # Extract catalog\n    catalog = extract_catalog(postgres_session)\n\n    # Find the index\n    index = None\n    for idx in catalog.indexes.values():\n        if idx.indexname == \"idx_test_email\":\n            index = idx\n            break\n\n    assert index is not None\n    assert index.tablename == \"test_table\"\n    assert index.schemaname == \"public\"\n    assert not index.is_unique\n    assert not index.is_primary\n\n\ndef test_unique_index_roundtrip(postgres_session):\n    \"\"\"Test unique index roundtrip fidelity.\"\"\"\n    # Create table with unique index\n    postgres_session.execute(text(\"\"\"\n        CREATE TABLE test_table (\n            id SERIAL PRIMARY KEY,\n            email TEXT NOT NULL\n        );\n        CREATE UNIQUE INDEX idx_test_email_unique ON test_table (email);\n    \"\"\"))\n    postgres_session.commit()\n\n    # Extract catalog\n    catalog = extract_catalog(postgres_session)\n\n    # Find the unique index\n    index = None\n    for idx in catalog.indexes.values():\n        if idx.indexname == \"idx_test_email_unique\":\n            index = idx\n            break\n\n    assert index is not None\n    assert index.is_unique\n    assert not index.is_primary\n\n\ndef test_index_diff_and_generation(postgres_session):\n    \"\"\"Test index diff and SQL generation.\"\"\"\n    # Create initial table\n    postgres_session.execute(text(\"\"\"\n        CREATE TABLE test_table (\n            id SERIAL PRIMARY KEY,\n            email TEXT NOT NULL\n        );\n    \"\"\"))\n    postgres_session.commit()\n\n    # Extract master catalog\n    master_catalog = extract_catalog(postgres_session)\n\n    # Add index\n    postgres_session.execute(text(\"\"\"\n        CREATE INDEX idx_test_email ON test_table (email);\n    \"\"\"))\n    postgres_session.commit()\n\n    # Extract branch catalog\n    branch_catalog = extract_catalog(postgres_session)\n\n    # Generate diff\n    changes = master_catalog.diff(branch_catalog)\n\n    # Should have one CreateIndex change\n    assert len(changes) == 1\n    assert isinstance(changes[0], CreateIndex)\n\n    # Generate SQL\n    sql = generate_sql(changes[0])\n    assert \"CREATE INDEX\" in sql\n    assert \"idx_test_email\" in sql\n    assert \"test_table\" in sql\n</code></pre>"},{"location":"contributing/adding-entities/#step-8-documentation","title":"Step 8: Documentation","text":"<p>Update entity documentation in <code>docs/entities/indexes.md</code>:</p> <pre><code># Indexes\n\nPostgreSQL indexes improve query performance by providing faster data access paths.\n\n## PostgreSQL Specification\n\n### CREATE INDEX Syntax\n```sql\nCREATE [ UNIQUE ] INDEX [ CONCURRENTLY ] [ [ IF NOT EXISTS ] name ]\nON [ ONLY ] table_name [ USING method ]\n( { column_name | ( expression ) } [ ... ] )\n[ WHERE predicate ]\n</code></pre>"},{"location":"contributing/adding-entities/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"contributing/adding-entities/#currently-supported","title":"\u2705 Currently Supported","text":"<ul> <li>CREATE INDEX (regular and unique)</li> <li>DROP INDEX</li> <li>All index types (B-tree, Hash, GIN, GiST, SP-GiST, BRIN)</li> <li>Partial indexes with WHERE clause</li> <li>Functional indexes with expressions</li> <li>Multi-column indexes</li> </ul>"},{"location":"contributing/adding-entities/#not-yet-supported","title":"\u274c Not Yet Supported","text":"<ul> <li>ALTER INDEX operations</li> <li>CONCURRENTLY option (not needed for schema migration)</li> </ul>"},{"location":"contributing/adding-entities/#usage-examples","title":"Usage Examples","text":""},{"location":"contributing/adding-entities/#basic-index-creation","title":"Basic Index Creation","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT NOT NULL\n);\nCREATE INDEX idx_users_email ON users (email);\n\"\"\"\n</code></pre>"},{"location":"contributing/adding-entities/#implementation-details","title":"Implementation Details","text":""},{"location":"contributing/adding-entities/#index-model","title":"Index Model","text":"<pre><code>@dataclass(frozen=True)\nclass PgIndex:\n    indexname: str\n    schemaname: str\n    tablename: str\n    index_definition: str\n    is_unique: bool\n    is_primary: bool\n    is_exclusion: bool\n    oid: int\n</code></pre>"},{"location":"contributing/adding-entities/#sql-generation","title":"SQL Generation","text":"<p><pre><code>def generate_create_index_sql(change: CreateIndex) -&gt; str:\n    \"\"\"Generate CREATE INDEX SQL from stored definition.\"\"\"\n    return change.index.index_definition + \";\"\n</code></pre> <pre><code>### Step 9: Update Public API\n\nUpdate `src/pgdelta/__init__.py` to expose new types:\n\n```python\nfrom .changes.index import CreateIndex, DropIndex\n\n__all__ = [\n    # ... existing exports ...\n    \"CreateIndex\",\n    \"DropIndex\",\n    # ... rest of exports ...\n]\n</code></pre></p>"},{"location":"contributing/adding-entities/#common-patterns","title":"Common Patterns","text":""},{"location":"contributing/adding-entities/#1-model-design-pattern","title":"1. Model Design Pattern","text":"<pre><code>@dataclass(frozen=True)\nclass PgEntity(BasePgModel):\n    \"\"\"Template for new entity models.\"\"\"\n\n    # Identity fields (what makes this object unique)\n    name: str = field(metadata={\"tag\": \"identity\"})\n    schema: str = field(metadata={\"tag\": \"identity\"})\n\n    # Data fields (object properties that matter for DDL)\n    property1: str = field(metadata={\"tag\": \"data\"})\n    property2: bool = field(metadata={\"tag\": \"data\"})\n\n    # Internal fields (PostgreSQL implementation details)\n    oid: int = field(metadata={\"tag\": \"internal\"})\n\n    @property\n    def stable_id(self) -&gt; str:\n        \"\"\"Cross-database portable identifier.\"\"\"\n        return f\"prefix:{self.schema}.{self.name}\"\n</code></pre>"},{"location":"contributing/adding-entities/#2-extraction-pattern","title":"2. Extraction Pattern","text":"<pre><code>def extract_entities(session: Session) -&gt; list[PgEntity]:\n    \"\"\"Extract entities from PostgreSQL.\"\"\"\n\n    # Use appropriate PostgreSQL system catalogs\n    query = text(\"\"\"\n        SELECT\n            entity_name,\n            schema_name,\n            entity_property1,\n            entity_property2,\n            entity_oid\n        FROM pg_entities e\n        JOIN pg_namespace n ON n.oid = e.schema_oid\n        WHERE n.nspname NOT IN ('information_schema', 'pg_catalog')\n        ORDER BY schema_name, entity_name\n    \"\"\")\n\n    result = session.execute(query)\n    entities = []\n\n    for row in result:\n        entity = PgEntity(\n            name=row.entity_name,\n            schema=row.schema_name,\n            property1=row.entity_property1,\n            property2=row.entity_property2,\n            oid=row.entity_oid,\n        )\n        entities.append(entity)\n\n    return entities\n</code></pre>"},{"location":"contributing/adding-entities/#3-diffing-pattern","title":"3. Diffing Pattern","text":"<pre><code>def diff_entities(\n    master_entities: dict[str, PgEntity],\n    branch_entities: dict[str, PgEntity],\n) -&gt; list[DDL]:\n    \"\"\"Diff entities between catalogs.\"\"\"\n    changes = []\n\n    # Create entities that exist in branch but not master\n    for stable_id, branch_entity in branch_entities.items():\n        if stable_id not in master_entities:\n            changes.append(CreateEntity(\n                stable_id=stable_id,\n                entity=branch_entity\n            ))\n\n    # Drop entities that exist in master but not branch\n    for stable_id, master_entity in master_entities.items():\n        if stable_id not in branch_entities:\n            changes.append(DropEntity(\n                stable_id=stable_id,\n                entity=master_entity\n            ))\n\n    # Alter entities that exist in both but are different\n    for stable_id, master_entity in master_entities.items():\n        if stable_id in branch_entities:\n            branch_entity = branch_entities[stable_id]\n            if not master_entity.semantic_equality(branch_entity):\n                changes.append(AlterEntity(\n                    stable_id=stable_id,\n                    old_entity=master_entity,\n                    new_entity=branch_entity\n                ))\n\n    return changes\n</code></pre>"},{"location":"contributing/adding-entities/#4-sql-generation-pattern","title":"4. SQL Generation Pattern","text":"<pre><code>def generate_create_entity_sql(change: CreateEntity) -&gt; str:\n    \"\"\"Generate CREATE ENTITY SQL.\"\"\"\n    quoted_schema = f'\"{change.entity.schema}\"'\n    quoted_name = f'\"{change.entity.name}\"'\n\n    sql_parts = [f\"CREATE ENTITY {quoted_schema}.{quoted_name}\"]\n\n    # Add entity-specific properties\n    if change.entity.property1:\n        sql_parts.append(f\"WITH PROPERTY1 = '{change.entity.property1}'\")\n\n    if change.entity.property2:\n        sql_parts.append(\"WITH PROPERTY2\")\n\n    return \" \".join(sql_parts) + \";\"\n</code></pre>"},{"location":"contributing/adding-entities/#5-testing-pattern","title":"5. Testing Pattern","text":"<pre><code>def test_entity_roundtrip(postgres_session):\n    \"\"\"Test entity roundtrip fidelity.\"\"\"\n    # Create entity\n    postgres_session.execute(text(\"\"\"\n        CREATE ENTITY test_entity WITH PROPERTY1 = 'value';\n    \"\"\"))\n    postgres_session.commit()\n\n    # Extract catalog\n    catalog = extract_catalog(postgres_session)\n\n    # Verify entity exists\n    entity_id = \"prefix:public.test_entity\"\n    assert entity_id in catalog.entities\n\n    # Verify properties\n    entity = catalog.entities[entity_id]\n    assert entity.name == \"test_entity\"\n    assert entity.schema == \"public\"\n    assert entity.property1 == \"value\"\n    assert entity.property2 is True\n\n    # Test SQL generation\n    changes = empty_catalog.diff(catalog)\n    sql = generate_sql(changes[0])\n    assert \"CREATE ENTITY\" in sql\n    assert \"test_entity\" in sql\n</code></pre>"},{"location":"contributing/adding-entities/#best-practices","title":"Best Practices","text":""},{"location":"contributing/adding-entities/#1-model-design","title":"1. Model Design","text":"<ul> <li>Immutable: Always use <code>@dataclass(frozen=True)</code></li> <li>Field tagging: Tag all fields with appropriate metadata</li> <li>Stable IDs: Use consistent, cross-database identifiers</li> <li>Inheritance: Extend <code>BasePgModel</code> for semantic equality</li> </ul>"},{"location":"contributing/adding-entities/#2-extraction","title":"2. Extraction","text":"<ul> <li>System catalogs: Use PostgreSQL's system catalogs</li> <li>Schema filtering: Exclude system schemas</li> <li>Ordering: Order results for consistent output</li> <li>Error handling: Handle missing or invalid objects</li> </ul>"},{"location":"contributing/adding-entities/#3-diffing","title":"3. Diffing","text":"<ul> <li>Semantic equality: Use the model's <code>semantic_equality</code> method</li> <li>Change types: Create appropriate change objects</li> <li>Completeness: Handle create, drop, and alter operations</li> </ul>"},{"location":"contributing/adding-entities/#4-sql-generation","title":"4. SQL Generation","text":"<ul> <li>Quoting: Always quote identifiers</li> <li>Formatting: Use consistent SQL formatting</li> <li>Completeness: Generate complete, valid SQL</li> <li>Error handling: Validate inputs and handle edge cases</li> </ul>"},{"location":"contributing/adding-entities/#5-testing","title":"5. Testing","text":"<ul> <li>Real PostgreSQL: Use actual PostgreSQL instances</li> <li>Roundtrip fidelity: Test extract \u2192 diff \u2192 generate \u2192 apply</li> <li>Edge cases: Test unusual but valid scenarios</li> <li>Performance: Test with realistic data volumes</li> </ul>"},{"location":"contributing/adding-entities/#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributing/adding-entities/#common-issues","title":"Common Issues","text":""},{"location":"contributing/adding-entities/#model-not-found","title":"Model Not Found","text":"<pre><code># Error: Model not imported in catalog\nfrom .model.pg_entity import PgEntity, extract_entities\n\n# Solution: Add to catalog.py imports\n</code></pre>"},{"location":"contributing/adding-entities/#sql-generation-errors","title":"SQL Generation Errors","text":"<pre><code># Error: Change type not handled in dispatcher\nmatch change:\n    case CreateEntity() as create_entity:\n        return generate_create_entity_sql(create_entity)\n\n# Solution: Add case to generate_sql() function\n</code></pre>"},{"location":"contributing/adding-entities/#dependency-issues","title":"Dependency Issues","text":"<pre><code># Error: Objects created in wrong order\n# Solution: Ensure dependencies are properly tracked in pg_depend.py\n\n# Map entity OIDs\nfor entity in entities:\n    oid_to_stable_id[(\"pg_entity\", entity.oid)] = entity.stable_id\n</code></pre>"},{"location":"contributing/adding-entities/#validation-checklist","title":"Validation Checklist","text":"<ul> <li>[ ] Model extends <code>BasePgModel</code></li> <li>[ ] All fields have appropriate metadata tags</li> <li>[ ] <code>stable_id</code> property is implemented</li> <li>[ ] Extraction function queries system catalogs</li> <li>[ ] Catalog integration includes new entity type</li> <li>[ ] Diff logic handles create/drop/alter operations</li> <li>[ ] Change types are immutable dataclasses</li> <li>[ ] SQL generation produces valid DDL</li> <li>[ ] Dispatcher handles all change types</li> <li>[ ] Dependencies are tracked in pg_depend.py</li> <li>[ ] Comprehensive unit tests exist</li> <li>[ ] Integration tests with real PostgreSQL</li> <li>[ ] Roundtrip fidelity tests pass</li> <li>[ ] Documentation is updated</li> <li>[ ] Public API exports new types</li> </ul>"},{"location":"contributing/adding-entities/#summary","title":"Summary","text":"<p>Adding a new entity type to pgdelta requires:</p> <ol> <li>Defining the model with proper field metadata</li> <li>Implementing extraction from PostgreSQL catalogs</li> <li>Creating diff logic to detect changes</li> <li>Defining change types for operations</li> <li>Implementing SQL generation for each operation</li> <li>Adding dependency tracking for proper ordering</li> <li>Writing comprehensive tests with real PostgreSQL</li> <li>Updating documentation and public API</li> </ol> <p>This systematic approach ensures that new entity types integrate seamlessly with pgdelta's architecture while maintaining correctness, performance, and maintainability.</p>"},{"location":"contributing/setup/","title":"Contributing Setup","text":"<p>This guide will help you set up a development environment for contributing to pgdelta.</p>"},{"location":"contributing/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+</li> <li>Docker (for running PostgreSQL test containers)</li> <li>Git</li> </ul>"},{"location":"contributing/setup/#development-setup","title":"Development Setup","text":""},{"location":"contributing/setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/olirice/pgdelta.git\ncd pgdelta\n</code></pre>"},{"location":"contributing/setup/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\n\n# Activate virtual environment\n# On Linux/macOS:\nsource venv/bin/activate\n# On Windows:\nvenv\\Scripts\\activate\n</code></pre>"},{"location":"contributing/setup/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Install pgdelta in editable mode with development dependencies\npip install -e \".[dev]\"\n\n# Install documentation dependencies (optional)\npip install -e \".[docs]\"\n</code></pre>"},{"location":"contributing/setup/#4-install-pre-commit-hooks","title":"4. Install Pre-commit Hooks","text":"<pre><code># Install pre-commit hooks\npre-commit install\n\n# Test pre-commit hooks\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/setup/#5-verify-installation","title":"5. Verify Installation","text":"<pre><code># Run tests to verify everything works\npytest\n\n# Run specific test categories\npytest -m \"not slow\"              # Skip slow tests\npytest -m integration             # Run integration tests\npytest -m roundtrip              # Run roundtrip tests\n\n# Check code quality\nmypy src/pgdelta\nruff check .\nruff format .\n</code></pre>"},{"location":"contributing/setup/#development-tools","title":"Development Tools","text":""},{"location":"contributing/setup/#testing","title":"Testing","text":"<pre><code># Run all tests\npytest\n\n# Run tests with coverage\npytest --cov=src/pgdelta --cov-report=html\n\n# Run tests in parallel (faster)\npytest -n auto\n\n# Run specific test file\npytest tests/unit/test_catalog.py\n\n# Run specific test\npytest tests/unit/test_catalog.py::test_extract_catalog_basic\n</code></pre>"},{"location":"contributing/setup/#code-quality","title":"Code Quality","text":"<pre><code># Type checking\nmypy src/pgdelta\n\n# Linting\nruff check .\n\n# Auto-fix linting issues\nruff check . --fix\n\n# Code formatting\nruff format .\n\n# Run all pre-commit hooks\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/setup/#documentation","title":"Documentation","text":"<pre><code># Install documentation dependencies\npip install -e \".[docs]\"\n\n# Build documentation\nmkdocs build\n\n# Serve documentation locally\nmkdocs serve\n# Visit http://localhost:8000\n</code></pre>"},{"location":"contributing/setup/#architecture-overview","title":"Architecture Overview","text":"<p>Before contributing, familiarize yourself with pgdelta's architecture:</p>"},{"location":"contributing/setup/#three-phase-design","title":"Three-Phase Design","text":"<ol> <li>Extract: Extract schema from PostgreSQL into immutable dataclasses</li> <li>Diff: Compare catalogs to generate change objects</li> <li>Generate: Generate SQL DDL from change objects with dependency resolution</li> </ol>"},{"location":"contributing/setup/#key-directories","title":"Key Directories","text":"<pre><code>src/pgdelta/\n\u251c\u2500\u2500 model/          # PostgreSQL object models\n\u251c\u2500\u2500 diff/           # Diff algorithms\n\u251c\u2500\u2500 changes/        # Change types and SQL generation\n\u251c\u2500\u2500 cli/            # Command-line interface\n\u251c\u2500\u2500 catalog.py      # Catalog extraction\n\u2514\u2500\u2500 dependency_resolution.py  # Dependency resolution\n</code></pre>"},{"location":"contributing/setup/#testing-philosophy","title":"Testing Philosophy","text":"<ul> <li>Real PostgreSQL: All tests use actual PostgreSQL instances</li> <li>Roundtrip fidelity: Extract \u2192 Diff \u2192 Generate \u2192 Apply produces identical schemas</li> <li>Comprehensive coverage: 85% minimum test coverage required</li> </ul>"},{"location":"contributing/setup/#contributing-guidelines","title":"Contributing Guidelines","text":""},{"location":"contributing/setup/#code-style","title":"Code Style","text":""},{"location":"contributing/setup/#python-code","title":"Python Code","text":"<ul> <li>Follow PEP 8 (enforced by ruff)</li> <li>Use type hints for all functions and methods</li> <li>Write docstrings for public functions</li> <li>Use descriptive variable names</li> </ul> <pre><code># Good\ndef extract_catalog(session: Session) -&gt; PgCatalog:\n    \"\"\"Extract complete catalog from PostgreSQL session.\"\"\"\n    pass\n\n# Bad\ndef extract(s):\n    pass\n</code></pre>"},{"location":"contributing/setup/#sql-code","title":"SQL Code","text":"<ul> <li>Use double quotes for identifiers</li> <li>Format SQL for readability</li> <li>Include comments for complex queries</li> </ul> <pre><code>-- Good\nCREATE TABLE \"public\".\"users\" (\n  \"id\" serial PRIMARY KEY,\n  \"email\" text NOT NULL,\n  \"created_at\" timestamp DEFAULT now()\n);\n\n-- Bad\ncreate table users(id serial primary key,email text not null,created_at timestamp default now());\n</code></pre>"},{"location":"contributing/setup/#commit-messages","title":"Commit Messages","text":"<p>Use conventional commit format:</p> <pre><code># Feature\nfeat(tables): add support for partitioned tables\n\n# Bug fix\nfix(constraints): handle foreign key constraint dependencies correctly\n\n# Documentation\ndocs(api): add examples for Python API usage\n\n# Refactoring\nrefactor(diff): simplify table diffing algorithm\n\n# Tests\ntest(integration): add roundtrip tests for complex schemas\n</code></pre>"},{"location":"contributing/setup/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch <pre><code>git checkout -b feature/add-partitioned-tables\n</code></pre></li> <li>Make your changes</li> <li>Add tests for any new functionality</li> <li>Run the test suite <pre><code>pytest\nmypy src/pgdelta\nruff check .\n</code></pre></li> <li>Update documentation if needed</li> <li>Commit your changes</li> <li>Push to your fork</li> <li>Create a pull request</li> </ol>"},{"location":"contributing/setup/#pull-request-checklist","title":"Pull Request Checklist","text":"<ul> <li>[ ] Code follows style guidelines</li> <li>[ ] Tests added for new functionality</li> <li>[ ] All tests pass</li> <li>[ ] Documentation updated</li> <li>[ ] Type hints added</li> <li>[ ] Pre-commit hooks pass</li> <li>[ ] Commit messages follow convention</li> </ul>"},{"location":"contributing/setup/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/setup/#writing-tests","title":"Writing Tests","text":""},{"location":"contributing/setup/#unit-tests","title":"Unit Tests","text":"<p>Test individual components in isolation:</p> <pre><code>def test_create_table_sql_generation():\n    \"\"\"Test CREATE TABLE SQL generation.\"\"\"\n    change = CreateTable(\n        stable_id=\"t:public.users\",\n        namespace=\"public\",\n        relname=\"users\",\n        columns=[\n            PgAttribute(attname=\"id\", type_name=\"integer\", attnotnull=True),\n        ]\n    )\n\n    sql = generate_create_table_sql(change)\n    assert \"CREATE TABLE \\\"public\\\".\\\"users\\\"\" in sql\n    assert \"\\\"id\\\" integer NOT NULL\" in sql\n</code></pre>"},{"location":"contributing/setup/#integration-tests","title":"Integration Tests","text":"<p>Test with real PostgreSQL:</p> <pre><code>def test_table_creation_roundtrip(postgres_session):\n    \"\"\"Test table creation roundtrip fidelity.\"\"\"\n    # Create table\n    postgres_session.execute(text(\"\"\"\n        CREATE TABLE test_table (\n            id SERIAL PRIMARY KEY,\n            name TEXT NOT NULL\n        )\n    \"\"\"))\n    postgres_session.commit()\n\n    # Extract catalog\n    catalog = extract_catalog(postgres_session)\n\n    # Verify table exists\n    assert \"t:public.test_table\" in catalog.classes\n\n    # Verify columns\n    table = catalog.classes[\"t:public.test_table\"]\n    columns = catalog.get_class_attributes(table.stable_id)\n    assert len(columns) == 2\n    assert columns[0].attname == \"id\"\n    assert columns[1].attname == \"name\"\n</code></pre>"},{"location":"contributing/setup/#roundtrip-tests","title":"Roundtrip Tests","text":"<p>Test complete workflows:</p> <pre><code>def test_complex_schema_roundtrip(postgres_session):\n    \"\"\"Test roundtrip fidelity with complex schema.\"\"\"\n    # Create complex schema\n    setup_complex_schema(postgres_session)\n\n    # Extract original catalog\n    original_catalog = extract_catalog(postgres_session)\n\n    # Generate changes to recreate schema\n    empty_catalog = create_empty_catalog()\n    changes = empty_catalog.diff(original_catalog)\n\n    # Apply changes to empty database\n    apply_changes_to_empty_database(changes)\n\n    # Extract final catalog\n    final_catalog = extract_catalog(empty_postgres_session)\n\n    # Verify catalogs are semantically identical\n    assert original_catalog.semantically_equals(final_catalog)\n</code></pre>"},{"location":"contributing/setup/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/              # Unit tests\n\u2502   \u251c\u2500\u2500 test_catalog.py\n\u2502   \u251c\u2500\u2500 test_diff.py\n\u2502   \u2514\u2500\u2500 test_sql_generation.py\n\u251c\u2500\u2500 integration/       # Integration tests\n\u2502   \u251c\u2500\u2500 test_extract.py\n\u2502   \u251c\u2500\u2500 test_roundtrip.py\n\u2502   \u2514\u2500\u2500 test_dependency_resolution.py\n\u251c\u2500\u2500 cli/              # CLI tests\n\u2502   \u2514\u2500\u2500 test_main.py\n\u2514\u2500\u2500 conftest.py       # Test fixtures\n</code></pre>"},{"location":"contributing/setup/#test-fixtures","title":"Test Fixtures","text":"<pre><code># conftest.py\n@pytest.fixture\ndef postgres_container():\n    \"\"\"PostgreSQL test container.\"\"\"\n    with PostgresContainer(\"postgres:17\") as container:\n        yield container\n\n@pytest.fixture\ndef postgres_session(postgres_container):\n    \"\"\"PostgreSQL session for testing.\"\"\"\n    engine = create_engine(postgres_container.get_connection_url())\n    with Session(engine) as session:\n        yield session\n\n@pytest.fixture\ndef empty_catalog():\n    \"\"\"Empty catalog for testing.\"\"\"\n    return PgCatalog(\n        namespaces={},\n        classes={},\n        attributes={},\n        constraints={},\n        indexes={},\n        sequences={},\n        policies={},\n        procedures={},\n        triggers={},\n        types={},\n        depends=[],\n    )\n</code></pre>"},{"location":"contributing/setup/#documentation-guidelines","title":"Documentation Guidelines","text":""},{"location":"contributing/setup/#code-documentation","title":"Code Documentation","text":""},{"location":"contributing/setup/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def generate_sql(change: DDL) -&gt; str:\n    \"\"\"Generate SQL DDL from a change object.\n\n    Args:\n        change: The change object to generate SQL for.\n\n    Returns:\n        SQL DDL statement as a string.\n\n    Raises:\n        NotImplementedError: If the change type is not supported.\n\n    Example:\n        &gt;&gt;&gt; change = CreateTable(stable_id=\"t:public.users\", ...)\n        &gt;&gt;&gt; sql = generate_sql(change)\n        &gt;&gt;&gt; print(sql)\n        CREATE TABLE \"public\".\"users\" (...);\n    \"\"\"\n</code></pre>"},{"location":"contributing/setup/#type-hints","title":"Type Hints","text":"<p>Use comprehensive type hints:</p> <pre><code>from typing import Dict, List, Optional, Union\n\ndef diff_objects(\n    master_objects: dict[str, T],\n    branch_objects: dict[str, T],\n    create_fn: Callable[[T], DDL],\n    drop_fn: Callable[[T], DDL],\n) -&gt; list[DDL]:\n    \"\"\"Diff two object collections.\"\"\"\n    pass\n</code></pre>"},{"location":"contributing/setup/#documentation-files","title":"Documentation Files","text":""},{"location":"contributing/setup/#structure","title":"Structure","text":"<ul> <li>Use clear headings and sections</li> <li>Include code examples</li> <li>Add links to related documentation</li> <li>Keep examples up-to-date</li> </ul>"},{"location":"contributing/setup/#examples","title":"Examples","text":"<p>Include practical examples:</p> <pre><code># Good - practical example\nfrom pgdelta import extract_catalog, generate_sql\n\n# Extract catalogs\nsource_catalog = extract_catalog(source_session)\ntarget_catalog = extract_catalog(target_session)\n\n# Generate changes\nchanges = source_catalog.diff(target_catalog)\n\n# Generate SQL\nsql_statements = [generate_sql(change) for change in changes]\n</code></pre>"},{"location":"contributing/setup/#debugging","title":"Debugging","text":""},{"location":"contributing/setup/#common-issues","title":"Common Issues","text":""},{"location":"contributing/setup/#test-failures","title":"Test Failures","text":"<pre><code># Run failed tests with verbose output\npytest -v --tb=short\n\n# Run specific failed test\npytest tests/unit/test_catalog.py::test_extract_catalog_basic -v\n\n# Run tests with pdb on failure\npytest --pdb\n</code></pre>"},{"location":"contributing/setup/#import-errors","title":"Import Errors","text":"<pre><code># Check Python path\npython -c \"import sys; print(sys.path)\"\n\n# Reinstall in editable mode\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/setup/#docker-issues","title":"Docker Issues","text":"<pre><code># Check Docker is running\ndocker ps\n\n# Pull PostgreSQL image\ndocker pull postgres:17\n\n# Clean up containers\ndocker container prune\n</code></pre>"},{"location":"contributing/setup/#debugging-tools","title":"Debugging Tools","text":""},{"location":"contributing/setup/#pytest","title":"pytest","text":"<pre><code># Run with verbose output\npytest -v\n\n# Run with coverage\npytest --cov=src/pgdelta --cov-report=html\n\n# Run specific markers\npytest -m integration\npytest -m \"not slow\"\n</code></pre>"},{"location":"contributing/setup/#mypy","title":"mypy","text":"<pre><code># Check specific file\nmypy src/pgdelta/catalog.py\n\n# Check with verbose output\nmypy --verbose src/pgdelta/\n\n# Generate HTML report\nmypy --html-report mypy-report src/pgdelta/\n</code></pre>"},{"location":"contributing/setup/#ruff","title":"ruff","text":"<pre><code># Check specific file\nruff check src/pgdelta/catalog.py\n\n# Auto-fix issues\nruff check --fix .\n\n# Show all rules\nruff linter\n</code></pre>"},{"location":"contributing/setup/#getting-help","title":"Getting Help","text":""},{"location":"contributing/setup/#communication-channels","title":"Communication Channels","text":"<ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>GitHub Discussions: Questions and general discussion</li> <li>Pull Requests: Code review and collaboration</li> </ul>"},{"location":"contributing/setup/#resources","title":"Resources","text":"<ul> <li>Architecture Documentation</li> <li>API Documentation</li> <li>Supported Entities</li> <li>Dependency Resolution</li> </ul>"},{"location":"contributing/setup/#asking-questions","title":"Asking Questions","text":"<p>When asking for help: 1. Describe what you're trying to do 2. Include relevant code snippets 3. Provide error messages 4. Share your environment details 5. Mention what you've already tried</p>"},{"location":"contributing/setup/#license","title":"License","text":"<p>By contributing to pgdelta, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"contributing/setup/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - GitHub contributor list - Release notes - Documentation acknowledgments</p> <p>Thank you for contributing to pgdelta!</p>"},{"location":"entities/constraints/","title":"Constraints","text":"<p>PostgreSQL constraints ensure data integrity by restricting the values that can be stored in tables.</p>"},{"location":"entities/constraints/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/constraints/#constraint-types","title":"Constraint Types","text":""},{"location":"entities/constraints/#primary-key","title":"PRIMARY KEY","text":"<pre><code>ALTER TABLE table_name ADD CONSTRAINT constraint_name PRIMARY KEY (column_name [, ...]);\n</code></pre>"},{"location":"entities/constraints/#foreign-key","title":"FOREIGN KEY","text":"<pre><code>ALTER TABLE table_name ADD CONSTRAINT constraint_name\nFOREIGN KEY (column_name [, ...]) REFERENCES referenced_table (column_name [, ...])\n[ MATCH FULL | MATCH PARTIAL | MATCH SIMPLE ]\n[ ON DELETE action ] [ ON UPDATE action ]\n[ [ NOT ] DEFERRABLE ] [ INITIALLY DEFERRED | INITIALLY IMMEDIATE ];\n</code></pre>"},{"location":"entities/constraints/#unique","title":"UNIQUE","text":"<pre><code>ALTER TABLE table_name ADD CONSTRAINT constraint_name UNIQUE (column_name [, ...]);\n</code></pre>"},{"location":"entities/constraints/#check","title":"CHECK","text":"<pre><code>ALTER TABLE table_name ADD CONSTRAINT constraint_name CHECK (expression);\n</code></pre>"},{"location":"entities/constraints/#exclude","title":"EXCLUDE","text":"<pre><code>ALTER TABLE table_name ADD CONSTRAINT constraint_name\nEXCLUDE [ USING method ] ( element WITH operator [, ...] )\n[ WHERE predicate ];\n</code></pre> <p>References: - PostgreSQL 17 ALTER TABLE - PostgreSQL 17 CREATE TABLE</p>"},{"location":"entities/constraints/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/constraints/#currently-supported","title":"\u2705 Currently Supported","text":""},{"location":"entities/constraints/#primary-key-constraints","title":"PRIMARY KEY Constraints","text":"<ul> <li>Single column primary keys</li> <li>Multi-column composite primary keys</li> <li>Automatic index creation</li> </ul> <pre><code>ALTER TABLE \"public\".\"users\" ADD CONSTRAINT \"users_pkey\" PRIMARY KEY (\"id\");\n</code></pre>"},{"location":"entities/constraints/#foreign-key-constraints","title":"FOREIGN KEY Constraints","text":"<ul> <li>Single column foreign keys</li> <li>Multi-column foreign keys</li> <li>ON DELETE/UPDATE actions (CASCADE, RESTRICT, SET NULL, SET DEFAULT)</li> <li>Constraint deferrability options</li> </ul> <pre><code>ALTER TABLE \"public\".\"orders\" ADD CONSTRAINT \"orders_user_id_fkey\"\nFOREIGN KEY (\"user_id\") REFERENCES \"public\".\"users\" (\"id\") ON DELETE CASCADE;\n</code></pre>"},{"location":"entities/constraints/#unique-constraints","title":"UNIQUE Constraints","text":"<ul> <li>Single column unique constraints</li> <li>Multi-column unique constraints</li> <li>Partial unique constraints with WHERE clause</li> </ul> <pre><code>ALTER TABLE \"public\".\"users\" ADD CONSTRAINT \"users_email_key\" UNIQUE (\"email\");\n</code></pre>"},{"location":"entities/constraints/#check-constraints","title":"CHECK Constraints","text":"<ul> <li>Column-level check constraints</li> <li>Table-level check constraints</li> <li>Complex expressions</li> </ul> <pre><code>ALTER TABLE \"public\".\"users\" ADD CONSTRAINT \"users_age_check\" CHECK (age &gt;= 0);\n</code></pre>"},{"location":"entities/constraints/#exclusion-constraints","title":"EXCLUSION Constraints","text":"<ul> <li>Basic exclusion constraints</li> <li>Custom operator specifications</li> <li>Spatial exclusion constraints</li> </ul> <pre><code>ALTER TABLE \"public\".\"reservations\" ADD CONSTRAINT \"reservations_overlap_excl\"\nEXCLUDE USING gist (room_id WITH =, during WITH &amp;&amp;);\n</code></pre>"},{"location":"entities/constraints/#drop-constraint","title":"DROP CONSTRAINT","text":"<ul> <li>Constraint deletion</li> <li>Cascade behavior through dependency resolution</li> </ul> <pre><code>ALTER TABLE \"public\".\"users\" DROP CONSTRAINT \"users_email_key\";\n</code></pre>"},{"location":"entities/constraints/#not-yet-supported","title":"\u274c Not Yet Supported","text":""},{"location":"entities/constraints/#alter-constraint","title":"ALTER CONSTRAINT","text":"<ul> <li>Constraint modifications</li> <li>Deferrability changes</li> <li>Constraint validation</li> </ul>"},{"location":"entities/constraints/#advanced-features","title":"Advanced Features","text":"<ul> <li>MATCH FULL/PARTIAL for foreign keys</li> <li>Complex exclusion constraint expressions</li> </ul>"},{"location":"entities/constraints/#intentionally-not-supported","title":"\ud83d\udeab Intentionally Not Supported","text":""},{"location":"entities/constraints/#environment-specific-features","title":"Environment-Specific Features","text":"<ul> <li>Constraint timing (some aspects are runtime)</li> <li>Performance-related constraint options</li> </ul>"},{"location":"entities/constraints/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/constraints/#primary-key-constraints_1","title":"Primary Key Constraints","text":"<pre><code># Single column primary key\ntarget_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL,\n    email TEXT\n);\nALTER TABLE users ADD CONSTRAINT users_pkey PRIMARY KEY (id);\n\"\"\"\n\n# Multi-column primary key\ntarget_sql = \"\"\"\nCREATE TABLE order_items (\n    order_id INTEGER,\n    product_id INTEGER,\n    quantity INTEGER\n);\nALTER TABLE order_items ADD CONSTRAINT order_items_pkey\nPRIMARY KEY (order_id, product_id);\n\"\"\"\n</code></pre>"},{"location":"entities/constraints/#foreign-key-constraints_1","title":"Foreign Key Constraints","text":"<pre><code># Basic foreign key\ntarget_sql = \"\"\"\nCREATE TABLE users (id SERIAL PRIMARY KEY, email TEXT);\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER,\n    total DECIMAL(10,2)\n);\nALTER TABLE orders ADD CONSTRAINT orders_user_id_fkey\nFOREIGN KEY (user_id) REFERENCES users (id);\n\"\"\"\n\n# Foreign key with cascade\ntarget_sql = \"\"\"\nALTER TABLE orders ADD CONSTRAINT orders_user_id_fkey\nFOREIGN KEY (user_id) REFERENCES users (id) ON DELETE CASCADE;\n\"\"\"\n</code></pre>"},{"location":"entities/constraints/#unique-constraints_1","title":"Unique Constraints","text":"<pre><code># Single column unique\ntarget_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT\n);\nALTER TABLE users ADD CONSTRAINT users_email_key UNIQUE (email);\n\"\"\"\n\n# Multi-column unique\ntarget_sql = \"\"\"\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name TEXT,\n    category TEXT\n);\nALTER TABLE products ADD CONSTRAINT products_name_category_key\nUNIQUE (name, category);\n\"\"\"\n</code></pre>"},{"location":"entities/constraints/#check-constraints_1","title":"Check Constraints","text":"<pre><code># Simple check constraint\ntarget_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    age INTEGER\n);\nALTER TABLE users ADD CONSTRAINT users_age_check CHECK (age &gt;= 0);\n\"\"\"\n\n# Complex check constraint\ntarget_sql = \"\"\"\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    price DECIMAL(10,2),\n    discount_price DECIMAL(10,2)\n);\nALTER TABLE products ADD CONSTRAINT products_price_check\nCHECK (price &gt;= 0 AND discount_price &gt;= 0 AND discount_price &lt;= price);\n\"\"\"\n</code></pre>"},{"location":"entities/constraints/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/constraints/#constraint-models","title":"Constraint Models","text":"<pre><code>@dataclass(frozen=True)\nclass CreateConstraint:\n    stable_id: str              # Format: \"c:schema.constraint_name\"\n    constraint: PgConstraint    # Constraint definition\n\n@dataclass(frozen=True)\nclass PgConstraint:\n    conname: str               # Constraint name\n    contype: str               # Constraint type (p, f, u, c, x)\n    table_name: str            # Table name\n    schema_name: str           # Schema name\n    constraint_definition: str # Complete constraint definition\n\n    @property\n    def constraint_type_name(self) -&gt; str:\n        type_map = {\n            'p': 'PRIMARY KEY',\n            'f': 'FOREIGN KEY',\n            'u': 'UNIQUE',\n            'c': 'CHECK',\n            'x': 'EXCLUDE'\n        }\n        return type_map.get(self.contype, 'UNKNOWN')\n</code></pre>"},{"location":"entities/constraints/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_constraint_sql(change: CreateConstraint) -&gt; str:\n    \"\"\"Generate ADD CONSTRAINT SQL.\"\"\"\n    constraint = change.constraint\n\n    quoted_schema = f'\"{constraint.schema_name}\"'\n    quoted_table = f'\"{constraint.table_name}\"'\n    quoted_constraint = f'\"{constraint.conname}\"'\n\n    return (\n        f\"ALTER TABLE {quoted_schema}.{quoted_table} \"\n        f\"ADD CONSTRAINT {quoted_constraint} \"\n        f\"{constraint.constraint_definition};\"\n    )\n</code></pre>"},{"location":"entities/constraints/#constraint-types-detail","title":"Constraint Types Detail","text":""},{"location":"entities/constraints/#primary-key-constraints_2","title":"Primary Key Constraints","text":"<pre><code>-- Single column\nALTER TABLE users ADD CONSTRAINT users_pkey PRIMARY KEY (id);\n\n-- Multi-column\nALTER TABLE order_items ADD CONSTRAINT order_items_pkey\nPRIMARY KEY (order_id, product_id);\n</code></pre> <p>Characteristics: - Automatically creates unique B-tree index - Implies NOT NULL on all columns - Only one per table - Referenced by foreign keys</p>"},{"location":"entities/constraints/#foreign-key-constraints_2","title":"Foreign Key Constraints","text":"<pre><code>-- Basic foreign key\nALTER TABLE orders ADD CONSTRAINT orders_user_id_fkey\nFOREIGN KEY (user_id) REFERENCES users (id);\n\n-- With actions\nALTER TABLE orders ADD CONSTRAINT orders_user_id_fkey\nFOREIGN KEY (user_id) REFERENCES users (id)\nON DELETE CASCADE ON UPDATE RESTRICT;\n\n-- Multi-column\nALTER TABLE order_items ADD CONSTRAINT order_items_product_fkey\nFOREIGN KEY (product_id, variant_id) REFERENCES products (id, variant_id);\n</code></pre> <p>Actions: - <code>CASCADE</code>: Delete/update dependent rows - <code>RESTRICT</code>: Prevent delete/update if dependents exist - <code>SET NULL</code>: Set foreign key to NULL - <code>SET DEFAULT</code>: Set foreign key to default value - <code>NO ACTION</code>: Same as RESTRICT (default)</p>"},{"location":"entities/constraints/#unique-constraints_2","title":"Unique Constraints","text":"<pre><code>-- Single column\nALTER TABLE users ADD CONSTRAINT users_email_key UNIQUE (email);\n\n-- Multi-column\nALTER TABLE products ADD CONSTRAINT products_name_category_key\nUNIQUE (name, category);\n\n-- Partial unique (via partial unique index)\nCREATE UNIQUE INDEX users_active_email_key ON users (email) WHERE is_active = true;\n</code></pre> <p>Characteristics: - Automatically creates unique B-tree index - Allows multiple NULL values (unless NOT NULL constraint) - Can be referenced by foreign keys</p>"},{"location":"entities/constraints/#check-constraints_2","title":"Check Constraints","text":"<pre><code>-- Simple check\nALTER TABLE users ADD CONSTRAINT users_age_check CHECK (age &gt;= 0);\n\n-- Complex check\nALTER TABLE products ADD CONSTRAINT products_price_check\nCHECK (price &gt;= 0 AND discount_price &gt;= 0 AND discount_price &lt;= price);\n\n-- Check with function\nALTER TABLE users ADD CONSTRAINT users_email_check\nCHECK (email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$');\n</code></pre> <p>Characteristics: - Evaluated on INSERT/UPDATE - Can reference multiple columns - Can use functions and operators - NOT NULL is a special case of CHECK</p>"},{"location":"entities/constraints/#exclusion-constraints_1","title":"Exclusion Constraints","text":"<pre><code>-- Basic exclusion\nALTER TABLE reservations ADD CONSTRAINT reservations_overlap_excl\nEXCLUDE USING gist (room_id WITH =, during WITH &amp;&amp;);\n\n-- With WHERE clause\nALTER TABLE reservations ADD CONSTRAINT reservations_active_overlap_excl\nEXCLUDE USING gist (room_id WITH =, during WITH &amp;&amp;) WHERE (is_active = true);\n</code></pre> <p>Characteristics: - Prevents overlapping values - Uses GiST or SP-GiST indexes - Common for time ranges and geometric data - Can combine multiple operators</p>"},{"location":"entities/constraints/#testing","title":"Testing","text":""},{"location":"entities/constraints/#unit-tests","title":"Unit Tests","text":"<pre><code>def test_create_primary_key():\n    \"\"\"Test primary key constraint creation.\"\"\"\n    constraint = PgConstraint(\n        conname=\"users_pkey\",\n        contype=\"p\",\n        table_name=\"users\",\n        schema_name=\"public\",\n        constraint_definition=\"PRIMARY KEY (id)\"\n    )\n\n    change = CreateConstraint(\n        stable_id=\"c:public.users_pkey\",\n        constraint=constraint\n    )\n\n    sql = generate_create_constraint_sql(change)\n    assert 'ALTER TABLE \"public\".\"users\"' in sql\n    assert 'ADD CONSTRAINT \"users_pkey\"' in sql\n    assert 'PRIMARY KEY (id)' in sql\n</code></pre>"},{"location":"entities/constraints/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_foreign_key_roundtrip(postgres_session):\n    \"\"\"Test foreign key constraint roundtrip fidelity.\"\"\"\n    # Create tables and foreign key\n    postgres_session.execute(text(\"\"\"\n        CREATE TABLE users (id SERIAL PRIMARY KEY, email TEXT);\n        CREATE TABLE orders (\n            id SERIAL PRIMARY KEY,\n            user_id INTEGER,\n            total DECIMAL(10,2)\n        );\n        ALTER TABLE orders ADD CONSTRAINT orders_user_id_fkey\n        FOREIGN KEY (user_id) REFERENCES users (id) ON DELETE CASCADE;\n    \"\"\"))\n    postgres_session.commit()\n\n    # Extract catalog\n    catalog = extract_catalog(postgres_session)\n\n    # Find constraint\n    constraint = next(c for c in catalog.constraints\n                     if c.conname == \"orders_user_id_fkey\")\n    assert constraint.contype == \"f\"\n    assert \"ON DELETE CASCADE\" in constraint.constraint_definition\n</code></pre>"},{"location":"entities/constraints/#error-handling","title":"Error Handling","text":""},{"location":"entities/constraints/#common-errors","title":"Common Errors","text":"<pre><code># Constraint violations\ntry:\n    # Foreign key reference to non-existent table\n    sql = \"\"\"\n    ALTER TABLE orders ADD CONSTRAINT orders_user_id_fkey\n    FOREIGN KEY (user_id) REFERENCES nonexistent_table (id);\n    \"\"\"\n    # pgdelta validates references during extraction\nexcept Exception as e:\n    pass\n\n# Name conflicts\ntry:\n    # Duplicate constraint names\n    sql = \"\"\"\n    ALTER TABLE users ADD CONSTRAINT users_check CHECK (age &gt;= 0);\n    ALTER TABLE users ADD CONSTRAINT users_check CHECK (email IS NOT NULL);\n    \"\"\"\n    # pgdelta tracks constraint names\nexcept Exception as e:\n    pass\n</code></pre>"},{"location":"entities/constraints/#validation","title":"Validation","text":"<pre><code>def validate_constraint_name(name: str) -&gt; bool:\n    \"\"\"Validate constraint name.\"\"\"\n    # Must be valid identifier\n    if not name.replace('_', '').isalnum():\n        return False\n\n    # Reasonable length\n    if len(name) &gt; 63:  # PostgreSQL limit\n        return False\n\n    return True\n\ndef validate_check_expression(expression: str) -&gt; bool:\n    \"\"\"Validate check constraint expression.\"\"\"\n    # No subqueries\n    if \"SELECT\" in expression.upper():\n        return False\n\n    # Balanced parentheses\n    if expression.count(\"(\") != expression.count(\")\"):\n        return False\n\n    return True\n</code></pre>"},{"location":"entities/constraints/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/constraints/#planned-features-v020","title":"Planned Features (v0.2.0)","text":""},{"location":"entities/constraints/#alter-constraint_1","title":"ALTER CONSTRAINT","text":"<pre><code>-- Constraint modifications\nALTER TABLE users ALTER CONSTRAINT users_age_check NOT DEFERRABLE;\nALTER TABLE users RENAME CONSTRAINT old_name TO new_name;\n</code></pre>"},{"location":"entities/constraints/#validate-constraint","title":"VALIDATE CONSTRAINT","text":"<pre><code>-- Validate constraints added as NOT VALID\nALTER TABLE users VALIDATE CONSTRAINT users_age_check;\n</code></pre>"},{"location":"entities/constraints/#enhanced-foreign-keys","title":"Enhanced Foreign Keys","text":"<pre><code>-- MATCH options\nALTER TABLE orders ADD CONSTRAINT orders_user_id_fkey\nFOREIGN KEY (user_id) REFERENCES users (id) MATCH FULL;\n</code></pre>"},{"location":"entities/constraints/#best-practices","title":"Best Practices","text":""},{"location":"entities/constraints/#constraint-naming","title":"Constraint Naming","text":"<pre><code># Good constraint names\nnaming_patterns = {\n    \"primary_key\": \"{table}_pkey\",\n    \"foreign_key\": \"{table}_{column}_fkey\",\n    \"unique\": \"{table}_{column}_key\",\n    \"check\": \"{table}_{column}_{purpose}_check\",\n    \"exclude\": \"{table}_{purpose}_excl\"\n}\n\n# Examples\nexamples = {\n    \"users_pkey\": \"Primary key on users table\",\n    \"orders_user_id_fkey\": \"Foreign key from orders.user_id to users.id\",\n    \"users_email_key\": \"Unique constraint on users.email\",\n    \"users_age_positive_check\": \"Check that age is positive\",\n    \"reservations_overlap_excl\": \"Exclude overlapping reservations\"\n}\n</code></pre>"},{"location":"entities/constraints/#performance-considerations","title":"Performance Considerations","text":"<pre><code># Constraint performance impact\nperformance_notes = {\n    \"primary_key\": \"Minimal impact, creates efficient index\",\n    \"foreign_key\": \"Requires index on referenced columns\",\n    \"unique\": \"Creates index, minimal impact\",\n    \"check\": \"Evaluated on every INSERT/UPDATE\",\n    \"exclude\": \"Uses GiST index, moderate impact\"\n}\n\n# Optimization tips\noptimization_tips = {\n    \"foreign_keys\": \"Ensure referenced columns are indexed\",\n    \"check_constraints\": \"Use simple expressions when possible\",\n    \"exclusion_constraints\": \"Consider partial constraints with WHERE\",\n    \"deferrability\": \"Use DEFERRABLE for complex transactions\"\n}\n</code></pre>"},{"location":"entities/constraints/#data-integrity-strategy","title":"Data Integrity Strategy","text":"<pre><code># Constraint hierarchy\nintegrity_levels = {\n    \"essential\": [\"NOT NULL\", \"PRIMARY KEY\", \"FOREIGN KEY\"],\n    \"important\": [\"UNIQUE\", \"CHECK (basic validations)\"],\n    \"optional\": [\"CHECK (complex business rules)\", \"EXCLUDE\"]\n}\n\n# When to use each type\nuse_cases = {\n    \"primary_key\": \"Every table needs a primary key\",\n    \"foreign_key\": \"Maintain referential integrity\",\n    \"unique\": \"Prevent duplicate values\",\n    \"check\": \"Enforce business rules at database level\",\n    \"exclude\": \"Prevent conflicting reservations/schedules\"\n}\n</code></pre>"},{"location":"entities/functions/","title":"Functions","text":"<p>PostgreSQL functions are reusable code blocks that can be called from SQL queries.</p>"},{"location":"entities/functions/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/functions/#create-function-syntax","title":"CREATE FUNCTION Syntax","text":"<pre><code>CREATE [ OR REPLACE ] FUNCTION\n    name ( [ [ argmode ] [ argname ] argtype [ { DEFAULT | = } default_expr ] [, ...] ] )\n    [ RETURNS rettype\n      | RETURNS TABLE ( column_name column_type [, ...] ) ]\n  { LANGUAGE lang_name\n    | TRANSFORM { FOR TYPE type_name } [, ... ]\n    | WINDOW\n    | { IMMUTABLE | STABLE | VOLATILE }\n    | [ NOT ] LEAKPROOF\n    | { CALLED ON NULL INPUT | RETURNS NULL ON NULL INPUT | STRICT }\n    | { [ EXTERNAL ] SECURITY INVOKER | [ EXTERNAL ] SECURITY DEFINER }\n    | PARALLEL { UNSAFE | RESTRICTED | SAFE }\n    | COST execution_cost\n    | ROWS result_rows\n    | SUPPORT support_function\n    | SET configuration_parameter { TO value | = value | FROM CURRENT }\n    | AS 'definition'\n    | AS 'obj_file', 'link_symbol'\n    | sql_body\n  } ...\n</code></pre> <p>Reference: PostgreSQL 17 CREATE FUNCTION</p>"},{"location":"entities/functions/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/functions/#currently-supported-via-pg_get_functiondef","title":"\u2705 Currently Supported (via pg_get_functiondef)","text":"<ul> <li>CREATE FUNCTION with complete definition</li> <li>DROP FUNCTION</li> <li>CREATE OR REPLACE FUNCTION</li> <li>CREATE PROCEDURE</li> <li>DROP PROCEDURE</li> <li>All function languages (SQL, PL/pgSQL, Python, etc.)</li> <li>All parameter modes (IN, OUT, INOUT, VARIADIC)</li> <li>All return types including RETURNS TABLE</li> <li>All function attributes (IMMUTABLE, STABLE, VOLATILE, etc.)</li> <li>Security context (SECURITY DEFINER/INVOKER)</li> <li>Cost and row estimates</li> <li>Configuration parameter settings</li> <li>All PostgreSQL function features</li> </ul> <pre><code>CREATE FUNCTION \"public\".\"calculate_tax\"(amount decimal, rate decimal)\nRETURNS decimal\nLANGUAGE sql\nIMMUTABLE\nAS $function$\n    SELECT amount * rate;\n$function$;\n</code></pre>"},{"location":"entities/functions/#not-yet-supported","title":"\u274c Not Yet Supported","text":"<ul> <li>ALTER FUNCTION operations (planned)</li> <li>ALTER PROCEDURE operations (planned)</li> </ul>"},{"location":"entities/functions/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/functions/#basic-sql-function","title":"Basic SQL Function","text":"<pre><code>target_sql = \"\"\"\nCREATE FUNCTION calculate_total(price decimal, tax_rate decimal)\nRETURNS decimal\nLANGUAGE sql\nIMMUTABLE\nAS $$\n    SELECT price * (1 + tax_rate);\n$$;\n\"\"\"\n</code></pre>"},{"location":"entities/functions/#plpgsql-function","title":"PL/pgSQL Function","text":"<pre><code>target_sql = \"\"\"\nCREATE FUNCTION get_user_order_count(user_id integer)\nRETURNS integer\nLANGUAGE plpgsql\nSTABLE\nAS $$\nDECLARE\n    order_count integer;\nBEGIN\n    SELECT COUNT(*) INTO order_count\n    FROM orders\n    WHERE orders.user_id = $1;\n\n    RETURN order_count;\nEND;\n$$;\n\"\"\"\n</code></pre>"},{"location":"entities/functions/#table-returning-function","title":"Table-Returning Function","text":"<pre><code>target_sql = \"\"\"\nCREATE FUNCTION get_recent_orders(days_back integer)\nRETURNS TABLE(id integer, user_id integer, total decimal)\nLANGUAGE sql\nSTABLE\nAS $$\n    SELECT id, user_id, total\n    FROM orders\n    WHERE created_at &gt;= CURRENT_DATE - INTERVAL '1 day' * days_back;\n$$;\n\"\"\"\n</code></pre>"},{"location":"entities/functions/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/functions/#function-models","title":"Function Models","text":"<pre><code>@dataclass(frozen=True)\nclass CreateFunction:\n    stable_id: str          # Format: \"f:schema.function_name\"\n    namespace: str          # Schema name\n    proname: str           # Function name\n    function_definition: str # Complete function definition\n\n@dataclass(frozen=True)\nclass ReplaceFunction:\n    stable_id: str          # Format: \"f:schema.function_name\"\n    namespace: str          # Schema name\n    proname: str           # Function name\n    function_definition: str # New function definition\n</code></pre>"},{"location":"entities/functions/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_function_sql(change: CreateFunction) -&gt; str:\n    \"\"\"Generate CREATE FUNCTION SQL.\"\"\"\n    # Function definition contains the complete CREATE FUNCTION statement\n    return change.function_definition + \";\"\n\ndef generate_replace_function_sql(change: ReplaceFunction) -&gt; str:\n    \"\"\"Generate CREATE OR REPLACE FUNCTION SQL.\"\"\"\n    # Replace CREATE with CREATE OR REPLACE\n    definition = change.function_definition\n    if definition.startswith(\"CREATE FUNCTION\"):\n        definition = definition.replace(\"CREATE FUNCTION\", \"CREATE OR REPLACE FUNCTION\", 1)\n    return definition + \";\"\n</code></pre>"},{"location":"entities/functions/#function-types","title":"Function Types","text":""},{"location":"entities/functions/#sql-functions","title":"SQL Functions","text":"<pre><code>-- Simple calculation\nCREATE FUNCTION add_numbers(a integer, b integer)\nRETURNS integer\nLANGUAGE sql\nIMMUTABLE\nAS $$\n    SELECT a + b;\n$$;\n</code></pre>"},{"location":"entities/functions/#plpgsql-functions","title":"PL/pgSQL Functions","text":"<pre><code>-- Complex logic with control structures\nCREATE FUNCTION fibonacci(n integer)\nRETURNS integer\nLANGUAGE plpgsql\nIMMUTABLE\nAS $$\nDECLARE\n    a integer := 0;\n    b integer := 1;\n    temp integer;\nBEGIN\n    FOR i IN 1..n LOOP\n        temp := a + b;\n        a := b;\n        b := temp;\n    END LOOP;\n    RETURN a;\nEND;\n$$;\n</code></pre>"},{"location":"entities/functions/#trigger-functions","title":"Trigger Functions","text":"<pre><code>-- Function for trigger use\nCREATE FUNCTION update_modified_time()\nRETURNS trigger\nLANGUAGE plpgsql\nAS $$\nBEGIN\n    NEW.modified_at = NOW();\n    RETURN NEW;\nEND;\n$$;\n</code></pre>"},{"location":"entities/functions/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/functions/#planned-features-v020","title":"Planned Features (v0.2.0)","text":"<ul> <li>Enhanced function overloading support</li> <li>Better parameter type resolution</li> <li>Function security context tracking</li> </ul>"},{"location":"entities/functions/#best-practices","title":"Best Practices","text":""},{"location":"entities/functions/#function-naming","title":"Function Naming","text":"<pre><code># Good function names\ngood_names = [\n    \"calculate_tax\",          # Verb + noun\n    \"get_user_orders\",        # Clear action\n    \"validate_email\",         # Descriptive purpose\n    \"format_currency\",        # Transformation function\n]\n\n# Function categories\ncategories = {\n    \"calculations\": \"calculate_*, compute_*\",\n    \"queries\": \"get_*, find_*, search_*\",\n    \"validations\": \"validate_*, check_*, verify_*\",\n    \"transformations\": \"format_*, convert_*, transform_*\"\n}\n</code></pre>"},{"location":"entities/functions/#performance-considerations","title":"Performance Considerations","text":"<pre><code># Function volatility\nvolatility_levels = {\n    \"IMMUTABLE\": \"Same input always produces same output\",\n    \"STABLE\": \"Output doesn't change within single statement\",\n    \"VOLATILE\": \"Output can change between calls (default)\"\n}\n\n# Performance impact\nperformance_tips = {\n    \"mark_immutable\": \"Allows aggressive optimization\",\n    \"use_stable\": \"For functions reading database state\",\n    \"avoid_volatile\": \"Only when function has side effects\"\n}\n</code></pre>"},{"location":"entities/indexes/","title":"Indexes","text":"<p>PostgreSQL indexes improve query performance by providing faster data access paths.</p>"},{"location":"entities/indexes/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/indexes/#create-index-syntax","title":"CREATE INDEX Syntax","text":"<pre><code>CREATE [ UNIQUE ] INDEX [ CONCURRENTLY ] [ [ IF NOT EXISTS ] name ]\nON [ ONLY ] table_name [ USING method ]\n(\n    { column_name | ( expression ) }\n    [ COLLATE collation ]\n    [ opclass [ ( opclass_parameter = value [, ...] ) ] ]\n    [ ASC | DESC ]\n    [ NULLS { FIRST | LAST } ]\n    [, ...]\n)\n[ INCLUDE ( column_name [, ...] ) ]\n[ NULLS [ NOT ] DISTINCT ]\n[ WITH ( storage_parameter = value [, ...] ) ]\n[ TABLESPACE tablespace_name ]\n[ WHERE predicate ]\n</code></pre> <p>Reference: PostgreSQL 17 CREATE INDEX</p>"},{"location":"entities/indexes/#drop-index-syntax","title":"DROP INDEX Syntax","text":"<pre><code>DROP INDEX [ CONCURRENTLY ] [ IF EXISTS ] name [, ...] [ CASCADE | RESTRICT ]\n</code></pre> <p>Reference: PostgreSQL 17 DROP INDEX</p>"},{"location":"entities/indexes/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/indexes/#currently-supported","title":"\u2705 Currently Supported","text":""},{"location":"entities/indexes/#create-index","title":"CREATE INDEX","text":"<ul> <li>Regular indexes (B-tree, Hash, GIN, GiST, SP-GiST, BRIN)</li> <li>Unique indexes</li> <li>Partial indexes with WHERE clause</li> <li>Functional indexes with expressions</li> <li>Multi-column indexes</li> <li>Custom operator classes</li> <li>Collation specifications</li> <li>ASC/DESC ordering options</li> <li>NULLS FIRST/LAST options</li> <li>Storage parameters (WITH clause)</li> </ul> <pre><code>-- Regular index\nCREATE INDEX \"idx_users_email\" ON \"public\".\"users\" (\"email\");\n\n-- Unique index\nCREATE UNIQUE INDEX \"idx_users_email_unique\" ON \"public\".\"users\" (\"email\");\n\n-- Partial index\nCREATE INDEX \"idx_active_users\" ON \"public\".\"users\" (\"email\") WHERE is_active = true;\n\n-- Functional index\nCREATE INDEX \"idx_users_email_lower\" ON \"public\".\"users\" (lower(\"email\"));\n\n-- Multi-column index\nCREATE INDEX \"idx_users_name\" ON \"public\".\"users\" (\"last_name\", \"first_name\");\n</code></pre>"},{"location":"entities/indexes/#drop-index","title":"DROP INDEX","text":"<ul> <li>Index deletion</li> <li>Cascade behavior through dependency resolution</li> </ul> <pre><code>DROP INDEX \"public\".\"idx_users_email\";\n</code></pre>"},{"location":"entities/indexes/#not-yet-supported","title":"\u274c Not Yet Supported","text":""},{"location":"entities/indexes/#create-index-options","title":"CREATE INDEX Options","text":"<ul> <li>INCLUDE columns (covering indexes)</li> <li>NULLS [NOT] DISTINCT option</li> <li>ONLY modifier for inheritance</li> <li>TABLESPACE clause</li> <li>Operator class parameters</li> </ul>"},{"location":"entities/indexes/#alter-index-operations","title":"ALTER INDEX Operations","text":"<ul> <li>Index renaming</li> <li>Storage parameter modifications</li> <li>Tablespace changes</li> </ul>"},{"location":"entities/indexes/#intentionally-not-supported","title":"\ud83d\udeab Intentionally Not Supported","text":""},{"location":"entities/indexes/#operational-features","title":"Operational Features","text":"<ul> <li>CONCURRENTLY option (not needed for schema migration)</li> <li>IF NOT EXISTS (pgdelta tracks existence)</li> <li>IF EXISTS (pgdelta tracks existence)</li> </ul>"},{"location":"entities/indexes/#environment-specific-features","title":"Environment-Specific Features","text":"<ul> <li>TABLESPACE clause (file system layout)</li> </ul>"},{"location":"entities/indexes/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/indexes/#basic-index-creation","title":"Basic Index Creation","text":"<pre><code>from pgdelta import extract_catalog, generate_sql\n\n# Target schema with new index\ntarget_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT NOT NULL\n);\nCREATE INDEX idx_users_email ON users (email);\n\"\"\"\n\n# Generate diff\nchanges = source_catalog.diff(target_catalog)\n# Results in:\n# 1. CREATE TABLE \"public\".\"users\" (...)\n# 2. CREATE INDEX \"idx_users_email\" ON \"public\".\"users\" (\"email\");\n</code></pre>"},{"location":"entities/indexes/#unique-index","title":"Unique Index","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT NOT NULL\n);\nCREATE UNIQUE INDEX idx_users_email_unique ON users (email);\n\"\"\"\n\n# pgdelta generates:\n# CREATE UNIQUE INDEX \"idx_users_email_unique\" ON \"public\".\"users\" (\"email\");\n</code></pre>"},{"location":"entities/indexes/#partial-index","title":"Partial Index","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT NOT NULL,\n    is_active BOOLEAN DEFAULT true\n);\nCREATE INDEX idx_active_users ON users (email) WHERE is_active = true;\n\"\"\"\n\n# pgdelta generates:\n# CREATE INDEX \"idx_active_users\" ON \"public\".\"users\" (\"email\") WHERE is_active = true;\n</code></pre>"},{"location":"entities/indexes/#functional-index","title":"Functional Index","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT NOT NULL\n);\nCREATE INDEX idx_users_email_lower ON users (lower(email));\n\"\"\"\n\n# pgdelta generates:\n# CREATE INDEX \"idx_users_email_lower\" ON \"public\".\"users\" (lower(email));\n</code></pre>"},{"location":"entities/indexes/#multi-column-index","title":"Multi-Column Index","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    first_name TEXT,\n    last_name TEXT,\n    email TEXT NOT NULL\n);\nCREATE INDEX idx_users_name ON users (last_name, first_name);\n\"\"\"\n\n# pgdelta generates:\n# CREATE INDEX \"idx_users_name\" ON \"public\".\"users\" (\"last_name\", \"first_name\");\n</code></pre>"},{"location":"entities/indexes/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/indexes/#index-model","title":"Index Model","text":"<pre><code>@dataclass(frozen=True)\nclass CreateIndex:\n    stable_id: str      # Format: \"i:schema.index_name\"\n    index: PgIndex      # Index definition\n</code></pre>"},{"location":"entities/indexes/#pgindex-model","title":"PgIndex Model","text":"<pre><code>@dataclass(frozen=True)\nclass PgIndex:\n    indexname: str                 # Index name\n    tablename: str                 # Table name\n    schemaname: str               # Schema name\n    index_definition: str          # Complete CREATE INDEX statement\n    is_unique: bool               # Unique index flag\n    is_primary: bool              # Primary key index flag\n    is_exclusion: bool            # Exclusion constraint index flag\n\n    @property\n    def stable_id(self) -&gt; str:\n        \"\"\"Stable identifier for cross-database comparison.\"\"\"\n        return f\"i:{self.schemaname}.{self.indexname}\"\n</code></pre>"},{"location":"entities/indexes/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_index_sql(change: CreateIndex) -&gt; str:\n    \"\"\"Generate CREATE INDEX SQL from the stored index definition.\"\"\"\n    # PostgreSQL's pg_get_indexdef() returns the complete CREATE INDEX statement\n    index_def = change.index.index_definition\n\n    # Ensure it ends with a semicolon\n    if not index_def.endswith(\";\"):\n        index_def += \";\"\n\n    return index_def\n</code></pre>"},{"location":"entities/indexes/#index-types","title":"Index Types","text":""},{"location":"entities/indexes/#b-tree-indexes-default","title":"B-tree Indexes (Default)","text":"<pre><code>-- Implicit B-tree\nCREATE INDEX idx_users_id ON users (id);\n\n-- Explicit B-tree\nCREATE INDEX idx_users_id ON users USING btree (id);\n</code></pre> <p>Use cases: - Equality and range queries - ORDER BY operations - &lt; &gt; &lt;= &gt;= operations</p>"},{"location":"entities/indexes/#hash-indexes","title":"Hash Indexes","text":"<pre><code>CREATE INDEX idx_users_email ON users USING hash (email);\n</code></pre> <p>Use cases: - Equality queries only - Faster than B-tree for equality - No ordering support</p>"},{"location":"entities/indexes/#gin-indexes","title":"GIN Indexes","text":"<pre><code>-- For JSON data\nCREATE INDEX idx_users_metadata ON users USING gin (metadata);\n\n-- For full-text search\nCREATE INDEX idx_users_search ON users USING gin (to_tsvector('english', name));\n\n-- For arrays\nCREATE INDEX idx_users_tags ON users USING gin (tags);\n</code></pre> <p>Use cases: - JSON/JSONB data - Full-text search - Array operations - Composite values</p>"},{"location":"entities/indexes/#gist-indexes","title":"GiST Indexes","text":"<pre><code>-- For geometric data\nCREATE INDEX idx_locations_point ON locations USING gist (location);\n\n-- For full-text search\nCREATE INDEX idx_users_search ON users USING gist (to_tsvector('english', name));\n</code></pre> <p>Use cases: - Geometric data types - Full-text search - Range types - Custom data types</p>"},{"location":"entities/indexes/#sp-gist-indexes","title":"SP-GiST Indexes","text":"<pre><code>-- For geometric data\nCREATE INDEX idx_locations_point ON locations USING spgist (location);\n\n-- For IP addresses\nCREATE INDEX idx_logs_ip ON logs USING spgist (ip_address);\n</code></pre> <p>Use cases: - Non-balanced data structures - Geometric data - Text patterns - IP addresses</p>"},{"location":"entities/indexes/#brin-indexes","title":"BRIN Indexes","text":"<pre><code>-- For time-series data\nCREATE INDEX idx_logs_timestamp ON logs USING brin (created_at);\n\n-- For sequential data\nCREATE INDEX idx_orders_date ON orders USING brin (order_date);\n</code></pre> <p>Use cases: - Very large tables - Sequential data - Time-series data - Minimal storage overhead</p>"},{"location":"entities/indexes/#testing","title":"Testing","text":""},{"location":"entities/indexes/#unit-tests","title":"Unit Tests","text":"<pre><code>def test_create_index_basic():\n    \"\"\"Test basic index creation.\"\"\"\n    index = PgIndex(\n        indexname=\"idx_users_email\",\n        tablename=\"users\",\n        schemaname=\"public\",\n        index_definition='CREATE INDEX \"idx_users_email\" ON \"public\".\"users\" (\"email\")',\n        is_unique=False,\n        is_primary=False,\n        is_exclusion=False\n    )\n\n    change = CreateIndex(\n        stable_id=\"i:public.idx_users_email\",\n        index=index\n    )\n\n    sql = generate_create_index_sql(change)\n    assert 'CREATE INDEX \"idx_users_email\"' in sql\n    assert 'ON \"public\".\"users\"' in sql\n</code></pre>"},{"location":"entities/indexes/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_index_roundtrip(postgres_session):\n    \"\"\"Test index creation roundtrip fidelity.\"\"\"\n    # Create table and index\n    postgres_session.execute(text(\"\"\"\n        CREATE TABLE test_table (\n            id SERIAL PRIMARY KEY,\n            email TEXT NOT NULL\n        );\n        CREATE INDEX idx_test_email ON test_table (email);\n    \"\"\"))\n    postgres_session.commit()\n\n    # Extract catalog\n    catalog = extract_catalog(postgres_session)\n\n    # Find index\n    index = next(i for i in catalog.indexes if i.indexname == \"idx_test_email\")\n    assert index.tablename == \"test_table\"\n    assert index.schemaname == \"public\"\n    assert not index.is_unique\n</code></pre>"},{"location":"entities/indexes/#performance-tests","title":"Performance Tests","text":"<pre><code>def test_index_performance():\n    \"\"\"Test that indexes improve query performance.\"\"\"\n    # Create table with data\n    postgres_session.execute(text(\"\"\"\n        CREATE TABLE perf_test (\n            id SERIAL PRIMARY KEY,\n            value INTEGER\n        );\n        INSERT INTO perf_test (value)\n        SELECT generate_series(1, 100000);\n    \"\"\"))\n\n    # Test query without index\n    explain_result = postgres_session.execute(text(\"\"\"\n        EXPLAIN (FORMAT JSON) SELECT * FROM perf_test WHERE value = 50000\n    \"\"\"))\n\n    # Should use sequential scan\n    plan = explain_result.fetchone()[0]\n    assert \"Seq Scan\" in str(plan)\n\n    # Add index\n    postgres_session.execute(text(\"\"\"\n        CREATE INDEX idx_perf_value ON perf_test (value);\n    \"\"\"))\n\n    # Test query with index\n    explain_result = postgres_session.execute(text(\"\"\"\n        EXPLAIN (FORMAT JSON) SELECT * FROM perf_test WHERE value = 50000\n    \"\"\"))\n\n    # Should use index scan\n    plan = explain_result.fetchone()[0]\n    assert \"Index Scan\" in str(plan)\n</code></pre>"},{"location":"entities/indexes/#advanced-features","title":"Advanced Features","text":""},{"location":"entities/indexes/#operator-classes","title":"Operator Classes","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    content TEXT\n);\n\n-- Use specific operator class\nCREATE INDEX idx_documents_content ON documents USING gin (content gin_trgm_ops);\n\"\"\"\n\n# pgdelta preserves operator class specifications\n</code></pre>"},{"location":"entities/indexes/#collation-support","title":"Collation Support","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    name TEXT\n);\n\n-- Index with collation\nCREATE INDEX idx_users_name_collate ON users (name COLLATE \"C\");\n\"\"\"\n\n# pgdelta preserves collation specifications\n</code></pre>"},{"location":"entities/indexes/#storage-parameters","title":"Storage Parameters","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE large_table (\n    id BIGSERIAL PRIMARY KEY,\n    data TEXT\n);\n\n-- Index with storage parameters\nCREATE INDEX idx_large_data ON large_table (data) WITH (fillfactor = 70);\n\"\"\"\n\n# pgdelta preserves storage parameters\n</code></pre>"},{"location":"entities/indexes/#performance-considerations","title":"Performance Considerations","text":""},{"location":"entities/indexes/#index-selection-guidelines","title":"Index Selection Guidelines","text":"<pre><code># When to use each index type\nindex_guidelines = {\n    \"btree\": \"Default choice for most queries\",\n    \"hash\": \"Equality queries only, faster than btree\",\n    \"gin\": \"JSON, arrays, full-text search\",\n    \"gist\": \"Geometric data, range types\",\n    \"spgist\": \"Non-balanced data, IP addresses\",\n    \"brin\": \"Very large tables, sequential data\"\n}\n</code></pre>"},{"location":"entities/indexes/#index-maintenance","title":"Index Maintenance","text":"<pre><code># Monitor index usage\nmonitor_queries = {\n    \"unused_indexes\": \"\"\"\n        SELECT schemaname, tablename, indexname, idx_scan\n        FROM pg_stat_user_indexes\n        WHERE idx_scan = 0\n        ORDER BY schemaname, tablename, indexname;\n    \"\"\",\n\n    \"index_size\": \"\"\"\n        SELECT schemaname, tablename, indexname, pg_size_pretty(pg_relation_size(indexrelid))\n        FROM pg_stat_user_indexes\n        ORDER BY pg_relation_size(indexrelid) DESC;\n    \"\"\",\n\n    \"duplicate_indexes\": \"\"\"\n        SELECT i1.schemaname, i1.tablename, i1.indexname, i2.indexname\n        FROM pg_stat_user_indexes i1\n        JOIN pg_stat_user_indexes i2 ON i1.tablename = i2.tablename\n        WHERE i1.indexname &lt; i2.indexname;\n    \"\"\"\n}\n</code></pre>"},{"location":"entities/indexes/#error-handling","title":"Error Handling","text":""},{"location":"entities/indexes/#common-errors","title":"Common Errors","text":"<pre><code># Index name conflicts\ntry:\n    sql = 'CREATE INDEX \"existing_index\" ON table1 (column1);'\n    # pgdelta avoids this by tracking existence\nexcept Exception as e:\n    pass\n\n# Invalid expressions\ntry:\n    sql = 'CREATE INDEX idx_invalid ON table1 (invalid_function(column1));'\n    # pgdelta validates expressions during extraction\nexcept Exception as e:\n    pass\n</code></pre>"},{"location":"entities/indexes/#validation","title":"Validation","text":"<pre><code>def validate_index_expression(expression: str) -&gt; bool:\n    \"\"\"Validate index expression syntax.\"\"\"\n    # Check for common issues\n    if \"SELECT\" in expression.upper():\n        return False  # Subqueries not allowed\n\n    if expression.count(\"(\") != expression.count(\")\"):\n        return False  # Unmatched parentheses\n\n    return True\n</code></pre>"},{"location":"entities/indexes/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/indexes/#planned-features-v020","title":"Planned Features (v0.2.0)","text":""},{"location":"entities/indexes/#include-columns","title":"INCLUDE Columns","text":"<pre><code>-- Covering indexes\nCREATE INDEX idx_users_email_include ON users (email) INCLUDE (first_name, last_name);\n</code></pre>"},{"location":"entities/indexes/#nulls-distinct","title":"NULLS DISTINCT","text":"<pre><code>-- Unique index with null handling\nCREATE UNIQUE INDEX idx_users_email_nulls ON users (email) NULLS NOT DISTINCT;\n</code></pre>"},{"location":"entities/indexes/#alter-index","title":"ALTER INDEX","text":"<pre><code>-- Index modifications\nALTER INDEX idx_users_email RENAME TO idx_users_email_old;\nALTER INDEX idx_users_email SET (fillfactor = 80);\n</code></pre>"},{"location":"entities/indexes/#implementation-challenges","title":"Implementation Challenges","text":""},{"location":"entities/indexes/#covering-indexes","title":"Covering Indexes","text":"<ul> <li>Complex dependency tracking</li> <li>Storage optimization considerations</li> <li>Query planner integration</li> </ul>"},{"location":"entities/indexes/#concurrent-operations","title":"Concurrent Operations","text":"<ul> <li>Lock management during creation</li> <li>Online index rebuilding</li> <li>Minimal downtime requirements</li> </ul>"},{"location":"entities/indexes/#best-practices","title":"Best Practices","text":""},{"location":"entities/indexes/#index-naming","title":"Index Naming","text":"<pre><code># Good index names\ngood_names = [\n    \"idx_users_email\",              # Single column\n    \"idx_users_last_first\",         # Multiple columns\n    \"idx_orders_date_status\",       # Composite\n    \"idx_active_users\",             # Partial index\n    \"idx_users_email_lower\",        # Functional index\n    \"uniq_users_email\",             # Unique index\n]\n\n# Naming patterns\nnaming_patterns = {\n    \"regular\": \"idx_{table}_{columns}\",\n    \"unique\": \"uniq_{table}_{columns}\",\n    \"partial\": \"idx_{table}_{condition}\",\n    \"functional\": \"idx_{table}_{function}\",\n}\n</code></pre>"},{"location":"entities/indexes/#index-strategy","title":"Index Strategy","text":"<pre><code># Index creation strategy\nstrategy = {\n    \"primary_keys\": \"Automatic B-tree indexes\",\n    \"foreign_keys\": \"Usually need indexes for joins\",\n    \"where_clauses\": \"Index columns in WHERE conditions\",\n    \"order_by\": \"Index columns in ORDER BY\",\n    \"group_by\": \"Index columns in GROUP BY\",\n    \"json_queries\": \"GIN indexes for JSON operations\",\n    \"text_search\": \"GIN/GiST for full-text search\",\n}\n\n# Avoid over-indexing\navoid_patterns = {\n    \"too_many_indexes\": \"More than 5-10 indexes per table\",\n    \"unused_indexes\": \"Monitor pg_stat_user_indexes\",\n    \"duplicate_indexes\": \"Same column combinations\",\n    \"very_wide_indexes\": \"More than 6 columns\",\n}\n</code></pre>"},{"location":"entities/materialized-views/","title":"Materialized Views","text":"<p>PostgreSQL materialized views are physical copies of query results that can be refreshed periodically.</p>"},{"location":"entities/materialized-views/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/materialized-views/#create-materialized-view-syntax","title":"CREATE MATERIALIZED VIEW Syntax","text":"<pre><code>CREATE MATERIALIZED VIEW [ IF NOT EXISTS ] table_name\n    [ (column_name [, ...] ) ]\n    [ USING method ]\n    [ WITH ( storage_parameter [= value] [, ... ] ) ]\n    [ TABLESPACE tablespace_name ]\n    AS query\n    [ WITH [ NO ] DATA ]\n</code></pre>"},{"location":"entities/materialized-views/#drop-materialized-view-syntax","title":"DROP MATERIALIZED VIEW Syntax","text":"<pre><code>DROP MATERIALIZED VIEW [ IF EXISTS ] name [, ...] [ CASCADE | RESTRICT ]\n</code></pre> <p>Reference: PostgreSQL 17 CREATE MATERIALIZED VIEW</p>"},{"location":"entities/materialized-views/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/materialized-views/#currently-supported","title":"\u2705 Currently Supported","text":"<ul> <li>CREATE MATERIALIZED VIEW with AS query</li> <li>DROP MATERIALIZED VIEW</li> <li>Schema-qualified names</li> <li>Basic materialized view lifecycle</li> </ul> <pre><code>CREATE MATERIALIZED VIEW \"public\".\"user_stats\" AS\nSELECT\n    DATE_TRUNC('month', created_at) as month,\n    COUNT(*) as user_count\nFROM users\nGROUP BY DATE_TRUNC('month', created_at);\n</code></pre>"},{"location":"entities/materialized-views/#not-yet-supported","title":"\u274c Not Yet Supported","text":"<ul> <li>ALTER MATERIALIZED VIEW operations (planned)</li> <li>Storage parameters</li> <li>USING method</li> <li>Explicit column names</li> </ul>"},{"location":"entities/materialized-views/#intentionally-not-supported","title":"\ud83d\udeab Intentionally Not Supported","text":"<ul> <li>WITH DATA option (always uses NO DATA for safety)</li> <li>TABLESPACE clause (not applicable)</li> <li>IF EXISTS/IF NOT EXISTS (pgdelta tracks existence)</li> <li>REFRESH MATERIALIZED VIEW (not applicable for DDL)</li> </ul>"},{"location":"entities/materialized-views/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/materialized-views/#basic-materialized-view","title":"Basic Materialized View","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER,\n    total DECIMAL(10,2),\n    created_at TIMESTAMP DEFAULT now()\n);\n\nCREATE MATERIALIZED VIEW monthly_sales AS\nSELECT\n    DATE_TRUNC('month', created_at) as month,\n    SUM(total) as total_sales,\n    COUNT(*) as order_count\nFROM orders\nGROUP BY DATE_TRUNC('month', created_at);\n\"\"\"\n</code></pre>"},{"location":"entities/materialized-views/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/materialized-views/#materialized-view-models","title":"Materialized View Models","text":"<pre><code>@dataclass(frozen=True)\nclass CreateMaterializedView:\n    stable_id: str      # Format: \"m:schema.matview_name\"\n    namespace: str      # Schema name\n    relname: str        # Materialized view name\n    definition: str     # Complete AS query\n</code></pre>"},{"location":"entities/materialized-views/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_materialized_view_sql(change: CreateMaterializedView) -&gt; str:\n    \"\"\"Generate CREATE MATERIALIZED VIEW SQL.\"\"\"\n    quoted_schema = f'\"{change.namespace}\"'\n    quoted_matview = f'\"{change.relname}\"'\n\n    return f'CREATE MATERIALIZED VIEW {quoted_schema}.{quoted_matview} AS {change.definition};'\n</code></pre>"},{"location":"entities/materialized-views/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/materialized-views/#planned-features-v020","title":"Planned Features (v0.2.0)","text":"<ul> <li>WITH/WITHOUT DATA options</li> <li>Storage parameters</li> <li>Refresh strategies</li> <li>Index management on materialized views</li> </ul>"},{"location":"entities/materialized-views/#best-practices","title":"Best Practices","text":""},{"location":"entities/materialized-views/#when-to-use-materialized-views","title":"When to Use Materialized Views","text":"<pre><code>use_cases = {\n    \"expensive_aggregations\": \"Pre-compute complex GROUP BY queries\",\n    \"reporting\": \"Snapshot data for reports\",\n    \"denormalization\": \"Flatten normalized data for performance\",\n    \"external_data\": \"Cache data from foreign data wrappers\"\n}\n\navoid_cases = {\n    \"frequently_updated\": \"High update frequency makes refresh costly\",\n    \"simple_queries\": \"Regular views are sufficient\",\n    \"real_time_data\": \"Staleness is unacceptable\"\n}\n</code></pre>"},{"location":"entities/materialized-views/#refresh-strategies","title":"Refresh Strategies","text":"<pre><code># Manual refresh patterns (outside pgdelta scope)\nrefresh_patterns = {\n    \"scheduled\": \"CRON job with REFRESH MATERIALIZED VIEW\",\n    \"triggered\": \"Refresh on base table changes\",\n    \"concurrent\": \"REFRESH MATERIALIZED VIEW CONCURRENTLY\",\n    \"partial\": \"Incremental refresh patterns\"\n}\n</code></pre>"},{"location":"entities/overview/","title":"Supported Entities Overview","text":"<p>This page provides a comprehensive overview of PostgreSQL entities supported by pgdelta, organized by operation type.</p>"},{"location":"entities/overview/#support-status-legend","title":"Support Status Legend","text":"<ul> <li>\u2705 Supported: Full implementation with comprehensive test coverage</li> <li>\u274c Not Supported: Not yet implemented, may be planned for future release</li> <li>\ud83d\udeab Not Applicable: Operation doesn't apply to this entity type</li> <li>\ud83d\udd04 Partial: Limited implementation, see entity details below</li> </ul>"},{"location":"entities/overview/#entity-support-matrix","title":"Entity Support Matrix","text":"Entity CREATE DROP ALTER REPLACE Notes Schemas \u2705 \u2705 \u274c \ud83d\udeab Basic schema lifecycle Tables \u2705 \u2705 \u2705 \ud83d\udeab Full table support with columns Constraints \u2705 \u2705 \ud83d\udd04 \ud83d\udeab All constraint types Indexes \u2705 \u2705 \u274c \ud83d\udeab All index types and methods Views \u2705 \u2705 \u274c \u2705 Basic view support Materialized Views \u2705 \u2705 \u274c \u2705 Created with NO DATA Functions \u2705 \u2705 \u274c \u2705 All function types via pg_get_functiondef Triggers \u2705 \u2705 \u274c \u274c All trigger types via pg_get_triggerdef Sequences \u2705 \u2705 \u2705 \ud83d\udeab Sequence support with OWNED BY Types \u2705 \u2705 \u2705 \ud83d\udeab Enum, composite, and domain types Policies \u2705 \u2705 \u2705 \ud83d\udeab Row Level Security policies Comments \u274c \u274c \u274c \ud83d\udeab Roles \u274c \u274c \u274c \ud83d\udeab Grants \u274c \u274c \ud83d\udeab \ud83d\udeab Default Privileges \ud83d\udeab \ud83d\udeab \u274c \ud83d\udeab"},{"location":"entities/overview/#contributing","title":"Contributing","text":"<p>To contribute support for new entity features: 1. Review the Contributing Guide 2. Follow the Adding New Entities guide 3. Ensure comprehensive test coverage with real PostgreSQL 4. Maintain roundtrip fidelity</p> <p>The project prioritizes correctness and completeness over speed of implementation.</p>"},{"location":"entities/policies/","title":"Policies","text":"<p>PostgreSQL Row Level Security (RLS) policies control which rows users can access in tables.</p>"},{"location":"entities/policies/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/policies/#create-policy-syntax","title":"CREATE POLICY Syntax","text":"<pre><code>CREATE POLICY name ON table_name\n    [ AS { PERMISSIVE | RESTRICTIVE } ]\n    [ FOR { ALL | SELECT | INSERT | UPDATE | DELETE } ]\n    [ TO { role_name | PUBLIC | CURRENT_ROLE | CURRENT_USER | SESSION_USER } [, ...] ]\n    [ USING ( using_expression ) ]\n    [ WITH CHECK ( check_expression ) ]\n</code></pre> <p>Reference: PostgreSQL 17 CREATE POLICY</p>"},{"location":"entities/policies/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/policies/#currently-supported","title":"\u2705 Currently Supported","text":"<ul> <li>CREATE POLICY with all options</li> <li>DROP POLICY</li> <li>ALTER POLICY</li> <li>All policy types (PERMISSIVE, RESTRICTIVE)</li> <li>All policy commands (SELECT, INSERT, UPDATE, DELETE, ALL)</li> </ul> <pre><code>CREATE POLICY \"users_own_data\" ON \"public\".\"users\"\n    FOR ALL\n    TO authenticated\n    USING (auth.uid() = id);\n</code></pre>"},{"location":"entities/policies/#not-yet-supported","title":"\u274c Not Yet Supported","text":"<ul> <li>Complex policy dependency optimization</li> <li>Policy inheritance patterns</li> </ul>"},{"location":"entities/policies/#intentionally-not-supported","title":"\ud83d\udeab Intentionally Not Supported","text":"<ul> <li>Role-based security (environment-specific)</li> </ul>"},{"location":"entities/policies/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/policies/#basic-row-level-security","title":"Basic Row-Level Security","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT NOT NULL,\n    is_active BOOLEAN DEFAULT true\n);\n\n-- Enable RLS\nALTER TABLE users ENABLE ROW LEVEL SECURITY;\n\n-- Policy for users to see their own data\nCREATE POLICY users_own_data ON users\n    FOR ALL\n    TO authenticated\n    USING (auth.uid() = id);\n\"\"\"\n</code></pre>"},{"location":"entities/policies/#restrictive-policy","title":"Restrictive Policy","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE sensitive_data (\n    id SERIAL PRIMARY KEY,\n    user_id INTEGER,\n    data TEXT\n);\n\nALTER TABLE sensitive_data ENABLE ROW LEVEL SECURITY;\n\n-- Only allow access to active users\nCREATE POLICY active_users_only ON sensitive_data\n    AS RESTRICTIVE\n    FOR ALL\n    TO authenticated\n    USING (EXISTS (SELECT 1 FROM users WHERE users.id = user_id AND is_active = true));\n\"\"\"\n</code></pre>"},{"location":"entities/policies/#command-specific-policies","title":"Command-Specific Policies","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE posts (\n    id SERIAL PRIMARY KEY,\n    author_id INTEGER,\n    title TEXT,\n    content TEXT,\n    published BOOLEAN DEFAULT false\n);\n\nALTER TABLE posts ENABLE ROW LEVEL SECURITY;\n\n-- Users can read published posts or their own posts\nCREATE POLICY posts_select ON posts\n    FOR SELECT\n    TO authenticated\n    USING (published = true OR auth.uid() = author_id);\n\n-- Users can only update their own posts\nCREATE POLICY posts_update ON posts\n    FOR UPDATE\n    TO authenticated\n    USING (auth.uid() = author_id);\n\"\"\"\n</code></pre>"},{"location":"entities/policies/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/policies/#policy-models","title":"Policy Models","text":"<pre><code>@dataclass(frozen=True)\nclass CreatePolicy:\n    stable_id: str              # Format: \"p:schema.table.policy_name\"\n    namespace: str              # Schema name\n    table_name: str             # Table name\n    policy_name: str            # Policy name\n    policy_definition: str      # Complete policy definition\n\n@dataclass(frozen=True)\nclass AlterPolicy:\n    stable_id: str              # Format: \"p:schema.table.policy_name\"\n    namespace: str              # Schema name\n    table_name: str             # Table name\n    policy_name: str            # Policy name\n    policy_definition: str      # New policy definition\n</code></pre>"},{"location":"entities/policies/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_policy_sql(change: CreatePolicy) -&gt; str:\n    \"\"\"Generate CREATE POLICY SQL.\"\"\"\n    return change.policy_definition + \";\"\n\ndef generate_alter_policy_sql(change: AlterPolicy) -&gt; str:\n    \"\"\"Generate ALTER POLICY SQL.\"\"\"\n    return change.policy_definition + \";\"\n</code></pre>"},{"location":"entities/policies/#policy-types","title":"Policy Types","text":""},{"location":"entities/policies/#permissive-policies-default","title":"Permissive Policies (Default)","text":"<pre><code>-- Allow access when condition is true\nCREATE POLICY permissive_policy ON table_name\n    AS PERMISSIVE\n    FOR ALL\n    USING (condition);\n</code></pre>"},{"location":"entities/policies/#restrictive-policies","title":"Restrictive Policies","text":"<pre><code>-- Restrict access unless condition is true\nCREATE POLICY restrictive_policy ON table_name\n    AS RESTRICTIVE\n    FOR ALL\n    USING (condition);\n</code></pre>"},{"location":"entities/policies/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/policies/#planned-features-v020","title":"Planned Features (v0.2.0)","text":"<ul> <li>Policy inheritance tracking</li> <li>Complex policy dependency resolution</li> <li>Policy performance analysis</li> </ul>"},{"location":"entities/policies/#best-practices","title":"Best Practices","text":""},{"location":"entities/policies/#policy-naming","title":"Policy Naming","text":"<pre><code># Good policy names\ngood_names = [\n    \"users_own_data\",         # Clear ownership\n    \"published_posts_read\",   # Specific action\n    \"admin_full_access\",      # Role-based\n    \"active_users_only\",      # Status-based\n]\n\n# Naming patterns\npatterns = {\n    \"ownership\": \"entity_own_data\",\n    \"action_based\": \"entity_action_condition\",\n    \"role_based\": \"role_access_level\",\n    \"status_based\": \"condition_only\"\n}\n</code></pre>"},{"location":"entities/policies/#security-considerations","title":"Security Considerations","text":"<pre><code># RLS security guidelines\nsecurity_guidelines = {\n    \"enable_rls\": \"Always enable RLS on sensitive tables\",\n    \"default_deny\": \"Start with restrictive policies\",\n    \"test_policies\": \"Test with different user contexts\",\n    \"performance\": \"Keep policy conditions efficient\"\n}\n\n# Common patterns\ncommon_patterns = {\n    \"user_isolation\": \"Users can only access their own data\",\n    \"role_based\": \"Different access levels for different roles\",\n    \"status_based\": \"Access based on record status\",\n    \"time_based\": \"Access based on time conditions\"\n}\n</code></pre>"},{"location":"entities/schemas/","title":"Schemas","text":"<p>PostgreSQL schemas are namespaces that contain database objects like tables, views, functions, and types.</p>"},{"location":"entities/schemas/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/schemas/#create-schema-syntax","title":"CREATE SCHEMA Syntax","text":"<pre><code>CREATE SCHEMA [ IF NOT EXISTS ] schema_name [ AUTHORIZATION role_specification ] [ schema_element [ ... ] ]\nCREATE SCHEMA [ IF NOT EXISTS ] AUTHORIZATION role_specification [ schema_element [ ... ] ]\n</code></pre> <p>Where <code>role_specification</code> is: <pre><code>[ GROUP ] role_name\n| CURRENT_ROLE\n| CURRENT_USER\n| SESSION_USER\n</code></pre></p> <p>Reference: PostgreSQL 17 CREATE SCHEMA</p>"},{"location":"entities/schemas/#drop-schema-syntax","title":"DROP SCHEMA Syntax","text":"<pre><code>DROP SCHEMA [ IF EXISTS ] name [, ...] [ CASCADE | RESTRICT ]\n</code></pre> <p>Reference: PostgreSQL 17 DROP SCHEMA</p>"},{"location":"entities/schemas/#alter-schema-syntax","title":"ALTER SCHEMA Syntax","text":"<pre><code>ALTER SCHEMA name RENAME TO new_name\nALTER SCHEMA name OWNER TO { new_owner | CURRENT_ROLE | CURRENT_USER | SESSION_USER }\n</code></pre> <p>Reference: PostgreSQL 17 ALTER SCHEMA</p>"},{"location":"entities/schemas/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/schemas/#currently-supported","title":"\u2705 Currently Supported","text":""},{"location":"entities/schemas/#create-schema","title":"CREATE SCHEMA","text":"<ul> <li>Basic schema creation with schema name</li> <li>Schema dependency resolution</li> <li>Automatic ordering with other DDL statements</li> </ul> <pre><code>CREATE SCHEMA \"analytics\";\n</code></pre>"},{"location":"entities/schemas/#drop-schema","title":"DROP SCHEMA","text":"<ul> <li>Schema deletion</li> <li>Dependency-aware ordering (drops contents first)</li> <li>Cascade behavior handled by dependency resolution</li> </ul> <pre><code>DROP SCHEMA \"analytics\";\n</code></pre>"},{"location":"entities/schemas/#not-yet-supported","title":"\u274c Not Yet Supported","text":""},{"location":"entities/schemas/#create-schema-options","title":"CREATE SCHEMA Options","text":"<ul> <li><code>AUTHORIZATION</code> clause for ownership</li> <li>Schema elements (inline object creation)</li> </ul>"},{"location":"entities/schemas/#alter-schema","title":"ALTER SCHEMA","text":"<ul> <li>Owner changes (<code>OWNER TO</code>) (planned)</li> </ul>"},{"location":"entities/schemas/#intentionally-not-supported","title":"\ud83d\udeab Intentionally Not Supported","text":""},{"location":"entities/schemas/#create-schema_1","title":"CREATE SCHEMA","text":"<ul> <li><code>IF NOT EXISTS</code> clause (pgdelta tracks existence)</li> </ul>"},{"location":"entities/schemas/#alter-schema_1","title":"ALTER SCHEMA","text":"<ul> <li>Schema renaming (<code>RENAME TO</code>) - uses drop/recreate pattern</li> </ul>"},{"location":"entities/schemas/#security-and-ownership","title":"Security and Ownership","text":"<ul> <li>Schema-level privileges (security context)</li> </ul>"},{"location":"entities/schemas/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/schemas/#basic-schema-creation","title":"Basic Schema Creation","text":"<pre><code>from pgdelta import extract_catalog, generate_sql\n\n# Source schema (empty)\nsource_sql = \"\"\n\n# Target schema with new schema\ntarget_sql = \"CREATE SCHEMA analytics;\"\n\n# Generate diff\nchanges = source_catalog.diff(target_catalog)\nsql = generate_sql(changes[0])\n# Result: CREATE SCHEMA \"analytics\";\n</code></pre>"},{"location":"entities/schemas/#schema-with-objects","title":"Schema with Objects","text":"<pre><code># Schema with table\ntarget_sql = \"\"\"\nCREATE SCHEMA analytics;\nCREATE TABLE analytics.metrics (\n    id SERIAL PRIMARY KEY,\n    value NUMERIC\n);\n\"\"\"\n\n# pgdelta will generate:\n# 1. CREATE SCHEMA \"analytics\";\n# 2. CREATE TABLE \"analytics\".\"metrics\" (...);\n</code></pre>"},{"location":"entities/schemas/#schema-deletion","title":"Schema Deletion","text":"<pre><code># Source has schema, target doesn't\nsource_sql = \"\"\"\nCREATE SCHEMA analytics;\nCREATE TABLE analytics.metrics (id SERIAL PRIMARY KEY);\n\"\"\"\n\ntarget_sql = \"\"\n\n# pgdelta will generate (in correct order):\n# 1. DROP TABLE \"analytics\".\"metrics\";\n# 2. DROP SCHEMA \"analytics\";\n</code></pre>"},{"location":"entities/schemas/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/schemas/#schema-model","title":"Schema Model","text":"<pre><code>@dataclass(frozen=True)\nclass CreateSchema:\n    stable_id: str      # Format: \"s:schema_name\"\n    nspname: str        # Schema name\n</code></pre>"},{"location":"entities/schemas/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_schema_sql(change: CreateSchema) -&gt; str:\n    \"\"\"Generate CREATE SCHEMA SQL.\"\"\"\n    quoted_schema = f'\"{change.nspname}\"'\n    return f\"CREATE SCHEMA {quoted_schema};\"\n</code></pre>"},{"location":"entities/schemas/#dependency-resolution","title":"Dependency Resolution","text":"<p>Schemas have dependencies with their contained objects: - CREATE: Schema must be created before its objects - DROP: Schema objects must be dropped before the schema</p> <pre><code># Dependencies for CREATE SCHEMA\ndepends_on = []  # Schemas have no dependencies\n\n# Dependencies for DROP SCHEMA\ndepends_on = [all_objects_in_schema]  # All contained objects\n</code></pre>"},{"location":"entities/schemas/#testing","title":"Testing","text":""},{"location":"entities/schemas/#unit-tests","title":"Unit Tests","text":"<pre><code>def test_create_schema_basic():\n    \"\"\"Test basic schema creation.\"\"\"\n    change = CreateSchema(\n        stable_id=\"s:test_schema\",\n        nspname=\"test_schema\"\n    )\n\n    sql = generate_create_schema_sql(change)\n    assert sql == 'CREATE SCHEMA \"test_schema\";'\n\ndef test_create_schema_quoted():\n    \"\"\"Test schema with special characters.\"\"\"\n    change = CreateSchema(\n        stable_id=\"s:test-schema\",\n        nspname=\"test-schema\"\n    )\n\n    sql = generate_create_schema_sql(change)\n    assert sql == 'CREATE SCHEMA \"test-schema\";'\n</code></pre>"},{"location":"entities/schemas/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_schema_roundtrip(postgres_session):\n    \"\"\"Test schema creation roundtrip fidelity.\"\"\"\n    # Create schema\n    postgres_session.execute(text('CREATE SCHEMA \"analytics\"'))\n    postgres_session.commit()\n\n    # Extract catalog\n    catalog = extract_catalog(postgres_session)\n\n    # Find schema\n    schema = next(s for s in catalog.schemas if s.nspname == \"analytics\")\n    assert schema.nspname == \"analytics\"\n\n    # Generate SQL\n    change = CreateSchema(\n        stable_id=f\"s:{schema.nspname}\",\n        nspname=schema.nspname\n    )\n\n    sql = generate_create_schema_sql(change)\n    assert 'CREATE SCHEMA \"analytics\"' in sql\n</code></pre>"},{"location":"entities/schemas/#dependency-tests","title":"Dependency Tests","text":"<pre><code>def test_schema_object_dependencies():\n    \"\"\"Test schema dependencies with contained objects.\"\"\"\n    source_sql = \"\"\n    target_sql = \"\"\"\n    CREATE SCHEMA app;\n    CREATE TABLE app.users (id SERIAL PRIMARY KEY);\n    CREATE INDEX idx_users_id ON app.users (id);\n    \"\"\"\n\n    changes = generate_changes(source_sql, target_sql)\n\n    # Should generate in correct order:\n    # 1. CREATE SCHEMA \"app\";\n    # 2. CREATE TABLE \"app\".\"users\" (...);\n    # 3. CREATE INDEX \"idx_users_id\" ON \"app\".\"users\" (...);\n\n    assert isinstance(changes[0], CreateSchema)\n    assert isinstance(changes[1], CreateTable)\n    assert isinstance(changes[2], CreateIndex)\n</code></pre>"},{"location":"entities/schemas/#error-handling","title":"Error Handling","text":""},{"location":"entities/schemas/#common-errors","title":"Common Errors","text":"<pre><code># Schema already exists\ntry:\n    session.execute(text('CREATE SCHEMA \"existing_schema\"'))\nexcept Exception as e:\n    # pgdelta avoids this by tracking existence\n    pass\n\n# Schema name conflicts\ntry:\n    session.execute(text('CREATE SCHEMA \"public\"'))  # Reserved name\nexcept Exception as e:\n    # pgdelta validates schema names\n    pass\n</code></pre>"},{"location":"entities/schemas/#validation","title":"Validation","text":"<pre><code>def validate_schema_name(name: str) -&gt; bool:\n    \"\"\"Validate schema name according to PostgreSQL rules.\"\"\"\n    # Must be valid identifier\n    if not name.isidentifier():\n        return False\n\n    # Check for reserved words\n    reserved = {'public', 'information_schema', 'pg_catalog'}\n    if name.lower() in reserved:\n        return False\n\n    return True\n</code></pre>"},{"location":"entities/schemas/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/schemas/#planned-features-v020","title":"Planned Features (v0.2.0)","text":""},{"location":"entities/schemas/#alter-schema-support","title":"ALTER SCHEMA Support","text":"<pre><code>-- Schema renaming\nALTER SCHEMA \"old_name\" RENAME TO \"new_name\";\n\n-- Owner changes (if we add role support)\nALTER SCHEMA \"analytics\" OWNER TO analytics_user;\n</code></pre>"},{"location":"entities/schemas/#create-schema-options_1","title":"CREATE SCHEMA Options","text":"<pre><code>-- With authorization\nCREATE SCHEMA \"analytics\" AUTHORIZATION analytics_user;\n\n-- With inline objects\nCREATE SCHEMA \"analytics\"\n    CREATE TABLE metrics (id SERIAL PRIMARY KEY)\n    CREATE VIEW metric_summary AS SELECT COUNT(*) FROM metrics;\n</code></pre>"},{"location":"entities/schemas/#implementation-notes","title":"Implementation Notes","text":""},{"location":"entities/schemas/#schema-renaming","title":"Schema Renaming","text":"<p>Schema renaming is complex because: - All dependent objects must be updated - Cross-schema references must be maintained - May be better handled as DROP/CREATE pattern</p>"},{"location":"entities/schemas/#authorization","title":"Authorization","text":"<p>Authorization support requires: - Role tracking and resolution - Environment-specific security context - May be environment-specific configuration</p>"},{"location":"entities/schemas/#best-practices","title":"Best Practices","text":""},{"location":"entities/schemas/#naming-conventions","title":"Naming Conventions","text":"<pre><code># Good schema names\ngood_names = [\n    \"public\",           # Default schema\n    \"app\",              # Application schema\n    \"analytics\",        # Analytics schema\n    \"reporting\",        # Reporting schema\n    \"staging\",          # Staging schema\n]\n\n# Avoid special characters\navoid_names = [\n    \"app-schema\",       # Hyphens require quoting\n    \"123schema\",        # Starting with numbers\n    \"schema name\",      # Spaces require quoting\n]\n</code></pre>"},{"location":"entities/schemas/#schema-organization","title":"Schema Organization","text":"<pre><code># Organize by function\nschemas = {\n    \"app\": \"Core application objects\",\n    \"analytics\": \"Analytics and reporting\",\n    \"audit\": \"Audit and logging\",\n    \"staging\": \"ETL staging area\",\n    \"archive\": \"Historical data\",\n}\n\n# Use consistent naming\nprefixes = {\n    \"app_\": \"Application schemas\",\n    \"rpt_\": \"Reporting schemas\",\n    \"tmp_\": \"Temporary schemas\",\n}\n</code></pre>"},{"location":"entities/schemas/#migration-patterns","title":"Migration Patterns","text":"<pre><code># Schema creation pattern\ndef create_schema_with_objects():\n    \"\"\"Create schema and its objects in correct order.\"\"\"\n    return [\n        \"CREATE SCHEMA \\\"new_schema\\\";\",\n        \"CREATE TABLE \\\"new_schema\\\".\\\"table1\\\" (...);\",\n        \"CREATE INDEX \\\"idx_table1\\\" ON \\\"new_schema\\\".\\\"table1\\\" (...);\",\n    ]\n\n# Schema deletion pattern\ndef drop_schema_with_objects():\n    \"\"\"Drop schema objects in reverse dependency order.\"\"\"\n    return [\n        \"DROP INDEX \\\"new_schema\\\".\\\"idx_table1\\\";\",\n        \"DROP TABLE \\\"new_schema\\\".\\\"table1\\\";\",\n        \"DROP SCHEMA \\\"new_schema\\\";\",\n    ]\n</code></pre>"},{"location":"entities/sequences/","title":"Sequences","text":"<p>PostgreSQL sequences generate unique numeric identifiers, commonly used for auto-incrementing primary keys.</p>"},{"location":"entities/sequences/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/sequences/#create-sequence-syntax","title":"CREATE SEQUENCE Syntax","text":"<pre><code>CREATE [ TEMPORARY | TEMP ] SEQUENCE [ IF NOT EXISTS ] name\n    [ AS data_type ]\n    [ INCREMENT [ BY ] increment ]\n    [ MINVALUE minvalue | NO MINVALUE ]\n    [ MAXVALUE maxvalue | NO MAXVALUE ]\n    [ START [ WITH ] start ]\n    [ CACHE cache ]\n    [ [ NO ] CYCLE ]\n    [ OWNED BY { table_name.column_name | NONE } ]\n</code></pre> <p>Reference: PostgreSQL 17 CREATE SEQUENCE</p>"},{"location":"entities/sequences/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/sequences/#currently-supported","title":"\u2705 Currently Supported","text":"<ul> <li>CREATE SEQUENCE with all options</li> <li>DROP SEQUENCE</li> <li>ALTER SEQUENCE OWNED BY</li> <li>All sequence parameters (INCREMENT, MINVALUE, MAXVALUE, START, CACHE, CYCLE)</li> </ul> <pre><code>CREATE SEQUENCE \"public\".\"users_id_seq\"\nAS bigint\nINCREMENT BY 1\nMINVALUE 1\nMAXVALUE 9223372036854775807\nSTART WITH 1\nCACHE 1\nNO CYCLE;\n\nALTER SEQUENCE \"public\".\"users_id_seq\" OWNED BY \"public\".\"users\".\"id\";\n</code></pre>"},{"location":"entities/sequences/#not-yet-supported","title":"\u274c Not Yet Supported","text":"<ul> <li>ALTER SEQUENCE parameter modifications (planned)</li> <li>RESTART operations (planned)</li> </ul>"},{"location":"entities/sequences/#intentionally-not-supported","title":"\ud83d\udeab Intentionally Not Supported","text":"<ul> <li>TEMPORARY sequences (not persistent schema objects)</li> <li>IF EXISTS/IF NOT EXISTS (pgdelta tracks existence)</li> </ul>"},{"location":"entities/sequences/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/sequences/#basic-sequence","title":"Basic Sequence","text":"<pre><code>target_sql = \"\"\"\nCREATE SEQUENCE user_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    NO MINVALUE\n    NO MAXVALUE\n    CACHE 1;\n\"\"\"\n</code></pre>"},{"location":"entities/sequences/#sequence-with-table","title":"Sequence with Table","text":"<pre><code>target_sql = \"\"\"\nCREATE SEQUENCE user_id_seq;\n\nCREATE TABLE users (\n    id BIGINT DEFAULT nextval('user_id_seq') PRIMARY KEY,\n    email TEXT NOT NULL\n);\n\nALTER SEQUENCE user_id_seq OWNED BY users.id;\n\"\"\"\n</code></pre>"},{"location":"entities/sequences/#serial-columns-auto-generated-sequences","title":"SERIAL Columns (Auto-generated Sequences)","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,  -- Creates users_id_seq automatically\n    email TEXT NOT NULL\n);\n\"\"\"\n# pgdelta handles the auto-generated sequence\n</code></pre>"},{"location":"entities/sequences/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/sequences/#sequence-models","title":"Sequence Models","text":"<pre><code>@dataclass(frozen=True)\nclass CreateSequence:\n    stable_id: str          # Format: \"s:schema.sequence_name\"\n    namespace: str          # Schema name\n    seqname: str           # Sequence name\n    sequence_definition: str # Complete sequence definition\n    owned_by: str | None    # Table.column ownership\n</code></pre>"},{"location":"entities/sequences/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_sequence_sql(change: CreateSequence) -&gt; str:\n    \"\"\"Generate CREATE SEQUENCE SQL.\"\"\"\n    sql_parts = [change.sequence_definition]\n\n    # Add ownership if specified\n    if change.owned_by:\n        quoted_seq = f'\"{change.namespace}\".\"{change.seqname}\"'\n        sql_parts.append(f\"ALTER SEQUENCE {quoted_seq} OWNED BY {change.owned_by};\")\n\n    return \"\\n\".join(sql_parts)\n</code></pre>"},{"location":"entities/sequences/#sequence-parameters","title":"Sequence Parameters","text":""},{"location":"entities/sequences/#data-types","title":"Data Types","text":"<pre><code>-- Integer sequences (default)\nCREATE SEQUENCE int_seq AS integer;\n\n-- Bigint sequences (for large ranges)\nCREATE SEQUENCE bigint_seq AS bigint;\n\n-- Smallint sequences (for small ranges)\nCREATE SEQUENCE smallint_seq AS smallint;\n</code></pre>"},{"location":"entities/sequences/#increment-options","title":"Increment Options","text":"<pre><code>-- Increment by 1 (default)\nCREATE SEQUENCE normal_seq INCREMENT BY 1;\n\n-- Increment by 10\nCREATE SEQUENCE skip_seq INCREMENT BY 10;\n\n-- Decrement (negative increment)\nCREATE SEQUENCE countdown_seq INCREMENT BY -1 START WITH 1000;\n</code></pre>"},{"location":"entities/sequences/#range-options","title":"Range Options","text":"<pre><code>-- Custom range\nCREATE SEQUENCE custom_range_seq\n    MINVALUE 1000\n    MAXVALUE 9999\n    START WITH 1000;\n\n-- No limits\nCREATE SEQUENCE unlimited_seq\n    NO MINVALUE\n    NO MAXVALUE;\n</code></pre>"},{"location":"entities/sequences/#caching-and-cycling","title":"Caching and Cycling","text":"<pre><code>-- High performance with caching\nCREATE SEQUENCE cached_seq CACHE 100;\n\n-- Cycling sequence\nCREATE SEQUENCE cycling_seq\n    MINVALUE 1\n    MAXVALUE 100\n    CYCLE;\n</code></pre>"},{"location":"entities/sequences/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/sequences/#planned-features-v020","title":"Planned Features (v0.2.0)","text":"<ul> <li>ALTER SEQUENCE support</li> <li>Sequence value synchronization</li> <li>Identity column integration</li> </ul>"},{"location":"entities/sequences/#best-practices","title":"Best Practices","text":""},{"location":"entities/sequences/#sequence-naming","title":"Sequence Naming","text":"<pre><code># Good sequence names\ngood_names = [\n    \"users_id_seq\",           # Table + column + seq\n    \"order_number_seq\",       # Business identifier\n    \"invoice_seq\",            # Short and clear\n]\n\n# Avoid generic names\navoid_names = [\n    \"seq1\", \"sequence\",       # Non-descriptive\n    \"my_seq\", \"temp_seq\",     # Unclear purpose\n]\n</code></pre>"},{"location":"entities/sequences/#sequence-design","title":"Sequence Design","text":"<pre><code># Sequence design considerations\ndesign_considerations = {\n    \"data_type\": \"Use BIGINT for high-volume tables\",\n    \"increment\": \"Usually 1, higher for bulk operations\",\n    \"cache\": \"Higher cache for better performance\",\n    \"cycle\": \"Rarely needed, consider implications\",\n    \"ownership\": \"Always set OWNED BY for dependent sequences\"\n}\n\n# Performance tips\nperformance_tips = {\n    \"cache_size\": \"Increase cache for high-throughput sequences\",\n    \"multiple_sequences\": \"Use separate sequences for different purposes\",\n    \"avoid_gaps\": \"Gaps are normal and expected in sequences\"\n}\n</code></pre>"},{"location":"entities/sequences/#common-patterns","title":"Common Patterns","text":"<pre><code># Standard SERIAL pattern\nserial_pattern = \"\"\"\nCREATE TABLE table_name (\n    id SERIAL PRIMARY KEY,  -- Auto-creates sequence\n    -- other columns\n);\n\"\"\"\n\n# Custom sequence pattern\ncustom_pattern = \"\"\"\nCREATE SEQUENCE table_name_id_seq\n    START WITH 1\n    INCREMENT BY 1\n    CACHE 1;\n\nCREATE TABLE table_name (\n    id BIGINT DEFAULT nextval('table_name_id_seq') PRIMARY KEY,\n    -- other columns\n);\n\nALTER SEQUENCE table_name_id_seq OWNED BY table_name.id;\n\"\"\"\n</code></pre>"},{"location":"entities/tables/","title":"Tables","text":"<p>PostgreSQL tables are the fundamental data storage structures containing rows and columns.</p>"},{"location":"entities/tables/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/tables/#create-table-syntax","title":"CREATE TABLE Syntax","text":"<pre><code>CREATE [ [ GLOBAL | LOCAL ] { TEMPORARY | TEMP } | UNLOGGED ] TABLE [ IF NOT EXISTS ] table_name ( [\n  { column_name data_type [ STORAGE { PLAIN | EXTERNAL | EXTENDED | MAIN | DEFAULT } ] [ COMPRESSION compression_method ] [ COLLATE collation ] [ column_constraint [ ... ] ]\n    | table_constraint\n    | LIKE source_table [ like_option ... ] }\n    [, ... ]\n] )\n[ INHERITS ( parent_table [, ... ] ) ]\n[ PARTITION BY { RANGE | LIST | HASH } ( { column_name | ( expression ) } [ COLLATE collation ] [ opclass ] [, ... ] ) ]\n[ USING method ]\n[ WITH ( storage_parameter [= value] [, ... ] ) | WITHOUT OIDS ]\n[ ON COMMIT { PRESERVE ROWS | DELETE ROWS | DROP } ]\n[ TABLESPACE tablespace_name ]\n</code></pre> <p>Reference: PostgreSQL 17 CREATE TABLE</p>"},{"location":"entities/tables/#alter-table-syntax","title":"ALTER TABLE Syntax","text":"<pre><code>ALTER TABLE [ IF EXISTS ] [ ONLY ] name [ * ]\n    action [, ... ]\n\n-- Where action is one of:\nADD [ COLUMN ] [ IF NOT EXISTS ] column_name data_type [ COLLATE collation ] [ column_constraint [ ... ] ]\nDROP [ COLUMN ] [ IF EXISTS ] column_name [ RESTRICT | CASCADE ]\nALTER [ COLUMN ] column_name [ SET DATA ] TYPE data_type [ COLLATE collation ] [ USING expression ]\nALTER [ COLUMN ] column_name SET DEFAULT expression\nALTER [ COLUMN ] column_name DROP DEFAULT\nALTER [ COLUMN ] column_name { SET | DROP } NOT NULL\n-- ... and many more\n</code></pre> <p>Reference: PostgreSQL 17 ALTER TABLE</p>"},{"location":"entities/tables/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/tables/#currently-supported","title":"\u2705 Currently Supported","text":""},{"location":"entities/tables/#create-table","title":"CREATE TABLE","text":"<ul> <li>Basic table creation with columns</li> <li>Column data types with precision/scale</li> <li>NOT NULL constraints</li> <li>DEFAULT expressions</li> <li>Generated columns (GENERATED ALWAYS AS ... STORED)</li> <li>Table inheritance (INHERITS clause)</li> <li>Storage parameters (WITH clause)</li> <li>Sequence ownership (ALTER SEQUENCE ... OWNED BY)</li> </ul> <pre><code>CREATE TABLE \"public\".\"users\" (\n  \"id\" serial PRIMARY KEY,\n  \"email\" text NOT NULL,\n  \"created_at\" timestamp DEFAULT now(),\n  \"full_name\" text GENERATED ALWAYS AS (first_name || ' ' || last_name) STORED\n);\n</code></pre>"},{"location":"entities/tables/#drop-table","title":"DROP TABLE","text":"<ul> <li>Table deletion</li> <li>Cascade behavior through dependency resolution</li> </ul> <pre><code>DROP TABLE \"public\".\"users\";\n</code></pre>"},{"location":"entities/tables/#alter-table","title":"ALTER TABLE","text":"<ul> <li>ADD COLUMN with all supported column options</li> <li>DROP COLUMN</li> <li>ALTER COLUMN TYPE with USING expression</li> <li>ALTER COLUMN SET/DROP DEFAULT</li> <li>ALTER COLUMN SET/DROP NOT NULL</li> </ul> <pre><code>ALTER TABLE \"public\".\"users\" ADD COLUMN \"phone\" text;\nALTER TABLE \"public\".\"users\" DROP COLUMN \"deprecated_field\";\nALTER TABLE \"public\".\"users\" ALTER COLUMN \"email\" TYPE varchar(255);\n</code></pre>"},{"location":"entities/tables/#not-yet-supported","title":"\u274c Not Yet Supported","text":""},{"location":"entities/tables/#create-table-options","title":"CREATE TABLE Options","text":"<ul> <li>Column STORAGE settings (PLAIN, EXTERNAL, EXTENDED, MAIN)</li> <li>Column COMPRESSION settings</li> <li>Column COLLATE settings</li> <li>LIKE clause for copying table structure</li> <li>PARTITION BY clause (RANGE, LIST, HASH partitioning)</li> <li>USING method for access method</li> <li>TABLESPACE clause</li> <li>Identity columns (GENERATED BY DEFAULT AS IDENTITY)</li> </ul>"},{"location":"entities/tables/#alter-table-operations","title":"ALTER TABLE Operations","text":"<ul> <li>Table renaming (RENAME TO)</li> <li>Schema changes (SET SCHEMA)</li> <li>Owner changes (OWNER TO)</li> <li>Storage parameter modifications</li> <li>Constraint modifications (handled separately)</li> </ul>"},{"location":"entities/tables/#intentionally-not-supported","title":"\ud83d\udeab Intentionally Not Supported","text":""},{"location":"entities/tables/#temporary-objects","title":"Temporary Objects","text":"<ul> <li>TEMPORARY/TEMP tables (not persistent schema objects)</li> <li>UNLOGGED tables (storage detail that changes automatically)</li> </ul>"},{"location":"entities/tables/#conditional-operations","title":"Conditional Operations","text":"<ul> <li>IF NOT EXISTS (pgdelta tracks existence)</li> <li>IF EXISTS (pgdelta tracks existence)</li> </ul>"},{"location":"entities/tables/#environment-specific-features","title":"Environment-Specific Features","text":"<ul> <li>TABLESPACE clause (file system layout)</li> <li>ON COMMIT behavior (session-specific)</li> <li>WITHOUT OIDS (deprecated PostgreSQL feature)</li> </ul>"},{"location":"entities/tables/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/tables/#basic-table-creation","title":"Basic Table Creation","text":"<pre><code>from pgdelta import extract_catalog, generate_sql\n\n# Target schema with new table\ntarget_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT NOT NULL,\n    created_at TIMESTAMP DEFAULT now()\n);\n\"\"\"\n\n# Generate diff\nchanges = source_catalog.diff(target_catalog)\nsql = generate_sql(changes[0])\n# Result: CREATE TABLE \"public\".\"users\" (...)\n</code></pre>"},{"location":"entities/tables/#table-with-generated-columns","title":"Table with Generated Columns","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    price DECIMAL(10,2) NOT NULL,\n    tax_rate DECIMAL(5,4) DEFAULT 0.0825,\n    total_price DECIMAL(10,2) GENERATED ALWAYS AS (price * (1 + tax_rate)) STORED\n);\n\"\"\"\n\n# pgdelta generates:\n# CREATE TABLE \"public\".\"products\" (\n#   \"id\" serial NOT NULL,\n#   \"name\" text NOT NULL,\n#   \"price\" numeric(10,2) NOT NULL,\n#   \"tax_rate\" numeric(5,4) DEFAULT 0.0825,\n#   \"total_price\" numeric(10,2) GENERATED ALWAYS AS (price * (1 + tax_rate)) STORED\n# );\n</code></pre>"},{"location":"entities/tables/#table-inheritance","title":"Table Inheritance","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE base_log (\n    id SERIAL PRIMARY KEY,\n    timestamp TIMESTAMP DEFAULT now(),\n    message TEXT\n);\n\nCREATE TABLE error_log (\n    error_code INTEGER,\n    stack_trace TEXT\n) INHERITS (base_log);\n\"\"\"\n\n# pgdelta generates inheritance relationship\n</code></pre>"},{"location":"entities/tables/#column-modifications","title":"Column Modifications","text":"<pre><code># Add column\nsource_sql = \"CREATE TABLE users (id SERIAL PRIMARY KEY);\"\ntarget_sql = \"CREATE TABLE users (id SERIAL PRIMARY KEY, email TEXT);\"\n\n# Generate: ALTER TABLE \"public\".\"users\" ADD COLUMN \"email\" text;\n\n# Change column type\nsource_sql = \"CREATE TABLE users (id SERIAL, email TEXT);\"\ntarget_sql = \"CREATE TABLE users (id SERIAL, email VARCHAR(255));\"\n\n# Generate: ALTER TABLE \"public\".\"users\" ALTER COLUMN \"email\" TYPE varchar(255);\n</code></pre>"},{"location":"entities/tables/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/tables/#table-model","title":"Table Model","text":"<pre><code>@dataclass(frozen=True)\nclass CreateTable:\n    stable_id: str                          # Format: \"t:schema.table\"\n    namespace: str                          # Schema name\n    relname: str                           # Table name\n    columns: list[PgAttribute]             # Column definitions\n    table_options: dict[str, Any] | None   # Storage parameters\n    inherits_from: list[str] | None        # Parent tables\n</code></pre>"},{"location":"entities/tables/#column-model","title":"Column Model","text":"<pre><code>@dataclass(frozen=True)\nclass PgAttribute:\n    attname: str                # Column name\n    type_name: str             # Data type\n    attnotnull: bool           # NOT NULL constraint\n    default_value: str | None  # DEFAULT expression\n    is_generated: bool         # Generated column flag\n    generated_expression: str | None  # Generation expression\n\n    @property\n    def formatted_type(self) -&gt; str:\n        \"\"\"Format type with precision/scale.\"\"\"\n        # e.g., \"varchar(255)\", \"numeric(10,2)\"\n</code></pre>"},{"location":"entities/tables/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_table_sql(change: CreateTable) -&gt; str:\n    \"\"\"Generate CREATE TABLE SQL.\"\"\"\n    quoted_schema = f'\"{change.namespace}\"'\n    quoted_table = f'\"{change.relname}\"'\n\n    sql_parts = [f\"CREATE TABLE {quoted_schema}.{quoted_table} (\"]\n\n    # Add columns\n    column_defs = []\n    for col in change.columns:\n        col_def = f'  \"{col.attname}\" {col.formatted_type}'\n\n        if col.is_generated:\n            col_def += f\" GENERATED ALWAYS AS ({col.generated_expression}) STORED\"\n            if col.attnotnull:\n                col_def += \" NOT NULL\"\n        else:\n            if col.default_value:\n                col_def += f\" DEFAULT {col.default_value}\"\n            if col.attnotnull:\n                col_def += \" NOT NULL\"\n\n        column_defs.append(col_def)\n\n    sql_parts.append(\"\\n\" + \",\\n\".join(column_defs) + \"\\n\")\n    sql_parts.append(\")\")\n\n    # Add inheritance\n    if change.inherits_from:\n        inherits_tables = [f'\"{table}\"' for table in change.inherits_from]\n        sql_parts.append(f\" INHERITS ({', '.join(inherits_tables)})\")\n\n    return \"\".join(sql_parts) + \";\"\n</code></pre>"},{"location":"entities/tables/#testing","title":"Testing","text":""},{"location":"entities/tables/#unit-tests","title":"Unit Tests","text":"<pre><code>def test_create_table_basic():\n    \"\"\"Test basic table creation.\"\"\"\n    column = PgAttribute(\n        attname=\"id\",\n        type_name=\"integer\",\n        attnotnull=True,\n        default_value=\"nextval('users_id_seq'::regclass)\"\n    )\n\n    change = CreateTable(\n        stable_id=\"t:public.users\",\n        namespace=\"public\",\n        relname=\"users\",\n        columns=[column]\n    )\n\n    sql = generate_create_table_sql(change)\n    assert \"CREATE TABLE \\\"public\\\".\\\"users\\\"\" in sql\n    assert \"\\\"id\\\" integer DEFAULT nextval('users_id_seq'::regclass) NOT NULL\" in sql\n</code></pre>"},{"location":"entities/tables/#integration-tests","title":"Integration Tests","text":"<pre><code>def test_table_roundtrip(postgres_session):\n    \"\"\"Test table creation roundtrip fidelity.\"\"\"\n    # Create table\n    postgres_session.execute(text(\"\"\"\n        CREATE TABLE test_table (\n            id SERIAL PRIMARY KEY,\n            name TEXT NOT NULL DEFAULT 'unnamed',\n            created_at TIMESTAMP DEFAULT now()\n        )\n    \"\"\"))\n    postgres_session.commit()\n\n    # Extract catalog\n    catalog = extract_catalog(postgres_session)\n\n    # Find table\n    table = next(t for t in catalog.tables if t.relname == \"test_table\")\n    assert len(table.columns) == 3\n\n    # Verify column properties\n    id_col = next(c for c in table.columns if c.attname == \"id\")\n    assert id_col.attnotnull\n    assert \"nextval\" in id_col.default_value\n</code></pre>"},{"location":"entities/tables/#alter-table-tests","title":"ALTER TABLE Tests","text":"<pre><code>def test_add_column():\n    \"\"\"Test ADD COLUMN generation.\"\"\"\n    source_sql = \"CREATE TABLE users (id SERIAL PRIMARY KEY);\"\n    target_sql = \"CREATE TABLE users (id SERIAL PRIMARY KEY, email TEXT);\"\n\n    changes = generate_changes(source_sql, target_sql)\n\n    alter_change = next(c for c in changes if isinstance(c, AlterTable))\n    assert alter_change.add_columns\n    assert alter_change.add_columns[0].attname == \"email\"\n\n    sql = generate_alter_table_sql(alter_change)\n    assert \"ALTER TABLE \\\"public\\\".\\\"users\\\" ADD COLUMN \\\"email\\\" text;\" in sql\n</code></pre>"},{"location":"entities/tables/#error-handling","title":"Error Handling","text":""},{"location":"entities/tables/#common-errors","title":"Common Errors","text":"<pre><code># Type conversion errors\ntry:\n    sql = \"ALTER TABLE users ALTER COLUMN id TYPE text;\"\n    # May fail if data cannot be converted\nexcept Exception as e:\n    # pgdelta can generate USING expression for safe conversion\n    safe_sql = \"ALTER TABLE users ALTER COLUMN id TYPE text USING id::text;\"\n</code></pre>"},{"location":"entities/tables/#data-type-validation","title":"Data Type Validation","text":"<pre><code>def validate_type_compatibility(old_type: str, new_type: str) -&gt; bool:\n    \"\"\"Check if type conversion is safe.\"\"\"\n    safe_conversions = {\n        (\"integer\", \"bigint\"),\n        (\"text\", \"varchar\"),\n        (\"varchar\", \"text\"),\n        (\"numeric\", \"text\"),\n    }\n\n    return (old_type, new_type) in safe_conversions\n</code></pre>"},{"location":"entities/tables/#advanced-features","title":"Advanced Features","text":""},{"location":"entities/tables/#generated-columns","title":"Generated Columns","text":"<pre><code># Generated column support\ntarget_sql = \"\"\"\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    subtotal DECIMAL(10,2) NOT NULL,\n    tax_rate DECIMAL(5,4) DEFAULT 0.0825,\n    total DECIMAL(10,2) GENERATED ALWAYS AS (subtotal * (1 + tax_rate)) STORED\n);\n\"\"\"\n\n# pgdelta handles generated column syntax correctly\n</code></pre>"},{"location":"entities/tables/#table-inheritance_1","title":"Table Inheritance","text":"<pre><code># Inheritance support\ntarget_sql = \"\"\"\nCREATE TABLE vehicles (\n    id SERIAL PRIMARY KEY,\n    make TEXT,\n    model TEXT\n);\n\nCREATE TABLE cars (\n    doors INTEGER\n) INHERITS (vehicles);\n\"\"\"\n\n# pgdelta tracks inheritance relationships\n</code></pre>"},{"location":"entities/tables/#storage-parameters","title":"Storage Parameters","text":"<pre><code># Storage parameter support\ntarget_sql = \"\"\"\nCREATE TABLE large_table (\n    id BIGSERIAL PRIMARY KEY,\n    data TEXT\n) WITH (\n    fillfactor = 70,\n    autovacuum_vacuum_scale_factor = 0.1\n);\n\"\"\"\n\n# pgdelta preserves storage parameters\n</code></pre>"},{"location":"entities/tables/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/tables/#planned-features-v020","title":"Planned Features (v0.2.0)","text":""},{"location":"entities/tables/#partitioning-support","title":"Partitioning Support","text":"<pre><code>CREATE TABLE measurements (\n    id SERIAL,\n    measurement_date DATE,\n    value NUMERIC\n) PARTITION BY RANGE (measurement_date);\n\nCREATE TABLE measurements_2024 PARTITION OF measurements\nFOR VALUES FROM ('2024-01-01') TO ('2025-01-01');\n</code></pre>"},{"location":"entities/tables/#identity-columns","title":"Identity Columns","text":"<pre><code>CREATE TABLE products (\n    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,\n    name TEXT NOT NULL\n);\n</code></pre>"},{"location":"entities/tables/#advanced-alter-table","title":"Advanced ALTER TABLE","text":"<pre><code>-- Table renaming\nALTER TABLE old_name RENAME TO new_name;\n\n-- Schema changes\nALTER TABLE public.table1 SET SCHEMA app;\n\n-- Storage parameter changes\nALTER TABLE table1 SET (fillfactor = 80);\n</code></pre>"},{"location":"entities/tables/#implementation-challenges","title":"Implementation Challenges","text":""},{"location":"entities/tables/#partitioning","title":"Partitioning","text":"<ul> <li>Complex dependency relationships</li> <li>Partition constraint management</li> <li>Partition pruning considerations</li> </ul>"},{"location":"entities/tables/#identity-columns_1","title":"Identity Columns","text":"<ul> <li>Sequence management</li> <li>Identity property preservation</li> <li>Conversion between serial and identity</li> </ul>"},{"location":"entities/tables/#best-practices","title":"Best Practices","text":""},{"location":"entities/tables/#column-naming","title":"Column Naming","text":"<pre><code># Good column names\ngood_names = [\n    \"id\", \"user_id\", \"created_at\",\n    \"first_name\", \"last_name\", \"email\",\n    \"is_active\", \"status\", \"version\"\n]\n\n# Avoid reserved words\navoid_names = [\n    \"order\", \"group\", \"user\", \"table\",\n    \"select\", \"where\", \"from\", \"join\"\n]\n</code></pre>"},{"location":"entities/tables/#data-type-selection","title":"Data Type Selection","text":"<pre><code># Recommended type mappings\ntype_mappings = {\n    \"id\": \"SERIAL or BIGSERIAL\",\n    \"text\": \"TEXT (unlimited) or VARCHAR(n)\",\n    \"money\": \"DECIMAL(10,2) not MONEY\",\n    \"boolean\": \"BOOLEAN not CHAR(1)\",\n    \"timestamp\": \"TIMESTAMP WITH TIME ZONE\",\n    \"uuid\": \"UUID with uuid-ossp extension\",\n}\n</code></pre>"},{"location":"entities/tables/#default-values","title":"Default Values","text":"<pre><code># Good default patterns\ngood_defaults = {\n    \"created_at\": \"now()\",\n    \"updated_at\": \"now()\",\n    \"is_active\": \"true\",\n    \"version\": \"1\",\n    \"status\": \"'pending'\",\n}\n\n# Avoid non-deterministic defaults in some cases\navoid_defaults = {\n    \"random()\": \"Use at application level\",\n    \"current_user\": \"May vary by connection\",\n}\n</code></pre>"},{"location":"entities/triggers/","title":"Triggers","text":"<p>PostgreSQL triggers are special functions that automatically execute in response to database events.</p>"},{"location":"entities/triggers/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/triggers/#create-trigger-syntax","title":"CREATE TRIGGER Syntax","text":"<pre><code>CREATE [ OR REPLACE ] [ CONSTRAINT ] TRIGGER name { BEFORE | AFTER | INSTEAD OF } { event [ OR ... ] }\n    ON table_name\n    [ FROM referenced_table_name ]\n    [ NOT DEFERRABLE | [ DEFERRABLE ] [ INITIALLY IMMEDIATE | INITIALLY DEFERRED ] ]\n    [ REFERENCING { { OLD | NEW } TABLE [ AS ] transition_relation_name } [ ... ] ]\n    [ FOR [ EACH ] { ROW | STATEMENT } ]\n    [ WHEN ( condition ) ]\n    EXECUTE { FUNCTION | PROCEDURE } function_name ( arguments )\n</code></pre> <p>Reference: PostgreSQL 17 CREATE TRIGGER</p>"},{"location":"entities/triggers/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/triggers/#currently-supported-via-pg_get_triggerdef","title":"\u2705 Currently Supported (via pg_get_triggerdef)","text":"<ul> <li>CREATE TRIGGER with complete definition</li> <li>DROP TRIGGER</li> <li>All trigger types (BEFORE, AFTER, INSTEAD OF)</li> <li>All trigger events (INSERT, UPDATE, DELETE, TRUNCATE)</li> <li>Row-level and statement-level triggers</li> <li>CONSTRAINT triggers</li> <li>Transition tables (REFERENCING clause)</li> <li>WHEN conditions</li> <li>All trigger features</li> </ul> <pre><code>CREATE TRIGGER \"update_user_modified_time\"\nBEFORE UPDATE ON \"public\".\"users\"\nFOR EACH ROW\nEXECUTE FUNCTION update_modified_time();\n</code></pre>"},{"location":"entities/triggers/#not-yet-supported","title":"\u274c Not Yet Supported","text":"<ul> <li>ALTER TRIGGER operations (planned)</li> <li>ENABLE/DISABLE TRIGGER (planned)</li> </ul>"},{"location":"entities/triggers/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/triggers/#basic-update-trigger","title":"Basic Update Trigger","text":"<pre><code>target_sql = \"\"\"\nCREATE FUNCTION update_modified_time()\nRETURNS trigger\nLANGUAGE plpgsql\nAS $$\nBEGIN\n    NEW.modified_at = NOW();\n    RETURN NEW;\nEND;\n$$;\n\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT,\n    created_at TIMESTAMP DEFAULT NOW(),\n    modified_at TIMESTAMP\n);\n\nCREATE TRIGGER update_user_modified_time\nBEFORE UPDATE ON users\nFOR EACH ROW\nEXECUTE FUNCTION update_modified_time();\n\"\"\"\n</code></pre>"},{"location":"entities/triggers/#audit-trigger","title":"Audit Trigger","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE audit_log (\n    id SERIAL PRIMARY KEY,\n    table_name TEXT,\n    operation TEXT,\n    old_values JSON,\n    new_values JSON,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE FUNCTION audit_changes()\nRETURNS trigger\nLANGUAGE plpgsql\nAS $$\nBEGIN\n    INSERT INTO audit_log (table_name, operation, old_values, new_values)\n    VALUES (\n        TG_TABLE_NAME,\n        TG_OP,\n        CASE WHEN TG_OP = 'DELETE' THEN row_to_json(OLD) ELSE NULL END,\n        CASE WHEN TG_OP = 'INSERT' THEN row_to_json(NEW) ELSE NULL END\n    );\n    RETURN COALESCE(NEW, OLD);\nEND;\n$$;\n\nCREATE TRIGGER audit_users_changes\nAFTER INSERT OR UPDATE OR DELETE ON users\nFOR EACH ROW\nEXECUTE FUNCTION audit_changes();\n\"\"\"\n</code></pre>"},{"location":"entities/triggers/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/triggers/#trigger-models","title":"Trigger Models","text":"<pre><code>@dataclass(frozen=True)\nclass CreateTrigger:\n    stable_id: str              # Format: \"t:schema.trigger_name\"\n    namespace: str              # Schema name\n    tgname: str                 # Trigger name\n    table_name: str             # Table name\n    trigger_definition: str     # Complete trigger definition\n</code></pre>"},{"location":"entities/triggers/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_trigger_sql(change: CreateTrigger) -&gt; str:\n    \"\"\"Generate CREATE TRIGGER SQL.\"\"\"\n    return change.trigger_definition + \";\"\n</code></pre>"},{"location":"entities/triggers/#trigger-types","title":"Trigger Types","text":""},{"location":"entities/triggers/#row-level-triggers","title":"Row-Level Triggers","text":"<pre><code>-- Execute for each affected row\nCREATE TRIGGER row_trigger\nBEFORE INSERT ON table_name\nFOR EACH ROW\nEXECUTE FUNCTION trigger_function();\n</code></pre>"},{"location":"entities/triggers/#statement-level-triggers","title":"Statement-Level Triggers","text":"<pre><code>-- Execute once per statement\nCREATE TRIGGER statement_trigger\nAFTER DELETE ON table_name\nFOR EACH STATEMENT\nEXECUTE FUNCTION trigger_function();\n</code></pre>"},{"location":"entities/triggers/#instead-of-triggers","title":"INSTEAD OF Triggers","text":"<pre><code>-- For views (make them updatable)\nCREATE TRIGGER view_insert_trigger\nINSTEAD OF INSERT ON view_name\nFOR EACH ROW\nEXECUTE FUNCTION handle_view_insert();\n</code></pre>"},{"location":"entities/triggers/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/triggers/#planned-features-v020","title":"Planned Features (v0.2.0)","text":"<ul> <li>CONSTRAINT triggers</li> <li>Transition tables support</li> <li>Better trigger dependency tracking</li> </ul>"},{"location":"entities/triggers/#best-practices","title":"Best Practices","text":""},{"location":"entities/triggers/#trigger-naming","title":"Trigger Naming","text":"<pre><code># Good trigger names\ngood_names = [\n    \"update_modified_time\",       # Action-based\n    \"audit_changes\",              # Purpose-based\n    \"validate_email_format\",      # Validation triggers\n    \"sync_derived_fields\",        # Synchronization triggers\n]\n\n# Naming patterns\npatterns = {\n    \"before_triggers\": \"validate_*, check_*, update_*\",\n    \"after_triggers\": \"audit_*, log_*, sync_*\",\n    \"instead_of\": \"handle_*, process_*\"\n}\n</code></pre>"},{"location":"entities/triggers/#performance-considerations","title":"Performance Considerations","text":"<pre><code># Trigger performance impact\nperformance_impact = {\n    \"row_triggers\": \"Execute for each affected row\",\n    \"statement_triggers\": \"Execute once per statement\",\n    \"complex_logic\": \"Can significantly impact INSERT/UPDATE performance\",\n    \"cascading_triggers\": \"Avoid triggers that fire other triggers\"\n}\n\n# Optimization tips\noptimization_tips = {\n    \"use_when_clause\": \"Limit trigger execution with WHEN conditions\",\n    \"minimize_work\": \"Keep trigger functions lightweight\",\n    \"avoid_exceptions\": \"Exceptions in triggers are expensive\",\n    \"consider_alternatives\": \"Sometimes application logic is better\"\n}\n</code></pre>"},{"location":"entities/types/","title":"Types","text":"<p>PostgreSQL supports custom data types including composite types, enums, domains, and ranges.</p>"},{"location":"entities/types/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/types/#create-type-syntax","title":"CREATE TYPE Syntax","text":"<pre><code>-- Composite type\nCREATE TYPE name AS (\n    [ attribute_name data_type [ COLLATE collation ] [, ... ] ]\n);\n\n-- Enum type\nCREATE TYPE name AS ENUM (\n    [ 'label' [, ... ] ]\n);\n\n-- Domain type\nCREATE DOMAIN name [ AS ] data_type\n    [ COLLATE collation ]\n    [ DEFAULT expression ]\n    [ constraint [ ... ] ]\n\n-- Range type\nCREATE TYPE name AS RANGE (\n    SUBTYPE = subtype\n    [ , SUBTYPE_OPCLASS = subtype_operator_class ]\n    [ , COLLATION = collation ]\n    [ , CANONICAL = canonical_function ]\n    [ , SUBTYPE_DIFF = subtype_diff_function ]\n    [ , MULTIRANGE_TYPE_NAME = multirange_type_name ]\n);\n</code></pre> <p>Reference: PostgreSQL 17 CREATE TYPE</p>"},{"location":"entities/types/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/types/#currently-supported","title":"\u2705 Currently Supported","text":"<ul> <li>CREATE TYPE for composite types</li> <li>CREATE TYPE for enum types</li> <li>CREATE DOMAIN for domain types with base type and constraints</li> <li>DROP TYPE / DROP DOMAIN</li> <li>Type dependency tracking</li> </ul> <pre><code>-- Enum type\nCREATE TYPE \"public\".\"user_status\" AS ENUM ('active', 'inactive', 'pending');\n\n-- Composite type\nCREATE TYPE \"public\".\"address\" AS (\n    street text,\n    city text,\n    state text,\n    zip_code text\n);\n\n-- Domain type\nCREATE DOMAIN \"public\".\"positive_int\" AS INTEGER CHECK (VALUE &gt; 0);\nCREATE DOMAIN \"public\".\"email\" AS TEXT CHECK (VALUE ~ '^[^@]+@[^@]+\\.[^@]+$');\n</code></pre>"},{"location":"entities/types/#not-yet-supported","title":"\u274c Not Yet Supported","text":"<ul> <li>ALTER TYPE operations (planned)</li> <li>Range types</li> <li>Multirange types</li> </ul>"},{"location":"entities/types/#intentionally-not-supported","title":"\ud83d\udeab Intentionally Not Supported","text":"<ul> <li>Base types (requires C code)</li> <li>Shell types (incomplete types)</li> </ul>"},{"location":"entities/types/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/types/#enum-types","title":"Enum Types","text":"<pre><code>target_sql = \"\"\"\nCREATE TYPE user_status AS ENUM ('active', 'inactive', 'pending', 'suspended');\n\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT NOT NULL,\n    status user_status DEFAULT 'pending'\n);\n\"\"\"\n</code></pre>"},{"location":"entities/types/#composite-types","title":"Composite Types","text":"<pre><code>target_sql = \"\"\"\nCREATE TYPE address AS (\n    street TEXT,\n    city TEXT,\n    state TEXT,\n    zip_code TEXT\n);\n\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT NOT NULL,\n    home_address address,\n    work_address address\n);\n\"\"\"\n</code></pre>"},{"location":"entities/types/#domain-types","title":"Domain Types","text":"<pre><code>target_sql = \"\"\"\nCREATE DOMAIN email AS TEXT\n    CHECK (VALUE ~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$');\n\nCREATE DOMAIN positive_int AS INTEGER\n    CHECK (VALUE &gt; 0);\n\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email_address email NOT NULL,\n    age positive_int\n);\n\"\"\"\n</code></pre>"},{"location":"entities/types/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/types/#type-models","title":"Type Models","text":"<pre><code>@dataclass(frozen=True)\nclass CreateType:\n    stable_id: str          # Format: \"typ:schema.type_name\"\n    namespace: str          # Schema name\n    typname: str           # Type name\n    typtype: str           # Type category (e=enum, c=composite, d=domain)\n\n    # Type-specific fields\n    enum_values: list[str] | None = None         # For enum types\n    domain_base_type: str | None = None          # For domain types\n    domain_constraints: list[str] | None = None  # For domain types\n    composite_attributes: list[CompositeAttribute] | None = None  # For composite types\n</code></pre>"},{"location":"entities/types/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_type_sql(change: CreateType) -&gt; str:\n    \"\"\"Generate CREATE TYPE SQL.\"\"\"\n    return change.type_definition + \";\"\n</code></pre>"},{"location":"entities/types/#type-categories","title":"Type Categories","text":""},{"location":"entities/types/#enum-types_1","title":"Enum Types","text":"<pre><code>-- Status enum\nCREATE TYPE order_status AS ENUM ('pending', 'processing', 'shipped', 'delivered');\n\n-- Priority enum\nCREATE TYPE priority AS ENUM ('low', 'medium', 'high', 'urgent');\n\n-- Size enum\nCREATE TYPE size AS ENUM ('small', 'medium', 'large', 'extra_large');\n</code></pre> <p>Use cases: - Status fields with fixed values - Category classifications - Priority levels - Size/grade classifications</p>"},{"location":"entities/types/#composite-types_1","title":"Composite Types","text":"<pre><code>-- Address composite\nCREATE TYPE address AS (\n    street TEXT,\n    city TEXT,\n    state TEXT,\n    zip_code TEXT,\n    country TEXT\n);\n\n-- Money composite\nCREATE TYPE money AS (\n    amount DECIMAL(10,2),\n    currency TEXT\n);\n\n-- Coordinate composite\nCREATE TYPE coordinate AS (\n    x DOUBLE PRECISION,\n    y DOUBLE PRECISION\n);\n</code></pre> <p>Use cases: - Grouping related fields - Reusable data structures - Complex data modeling</p>"},{"location":"entities/types/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/types/#planned-features-v020","title":"Planned Features (v0.2.0)","text":""},{"location":"entities/types/#range-types","title":"Range Types","text":"<pre><code>CREATE TYPE price_range AS RANGE (\n    SUBTYPE = DECIMAL,\n    SUBTYPE_OPCLASS = numeric_ops\n);\n\nCREATE TYPE date_range AS RANGE (\n    SUBTYPE = DATE,\n    SUBTYPE_OPCLASS = date_ops\n);\n</code></pre>"},{"location":"entities/types/#alter-type-operations","title":"ALTER TYPE Operations","text":"<pre><code>-- Add enum values\nALTER TYPE user_status ADD VALUE 'archived' AFTER 'inactive';\n\n-- Rename enum values\nALTER TYPE user_status RENAME VALUE 'pending' TO 'awaiting_activation';\n</code></pre>"},{"location":"entities/types/#best-practices","title":"Best Practices","text":""},{"location":"entities/types/#type-naming","title":"Type Naming","text":"<pre><code># Good type names\ngood_names = [\n    \"user_status\",           # Descriptive enum\n    \"address\",               # Clear composite type\n    \"priority_level\",        # Specific enum\n    \"geographic_point\",      # Descriptive composite\n]\n\n# Naming patterns\npatterns = {\n    \"enums\": \"Use singular nouns describing the category\",\n    \"composites\": \"Use nouns describing the data structure\",\n    \"domains\": \"Use descriptive names with constraints implied\"\n}\n</code></pre>"},{"location":"entities/types/#type-design","title":"Type Design","text":"<pre><code># Enum design guidelines\nenum_guidelines = {\n    \"stable_values\": \"Avoid frequently changing enum values\",\n    \"logical_order\": \"Order values logically (e.g., priority levels)\",\n    \"future_expansion\": \"Consider future values when designing\",\n    \"avoid_numbers\": \"Use descriptive labels, not numeric codes\"\n}\n\n# Composite type guidelines\ncomposite_guidelines = {\n    \"related_fields\": \"Group truly related fields together\",\n    \"avoid_large_types\": \"Keep composite types reasonably sized\",\n    \"consider_normalization\": \"Sometimes separate tables are better\",\n    \"null_handling\": \"Consider NULL behavior in composite fields\"\n}\n</code></pre>"},{"location":"entities/types/#performance-considerations","title":"Performance Considerations","text":"<pre><code># Type performance impact\nperformance_impact = {\n    \"enums\": \"Very efficient, stored as integers internally\",\n    \"composites\": \"Some overhead compared to separate columns\",\n    \"domains\": \"Constraint checking on every value\",\n    \"ranges\": \"Efficient for range queries and operations\"\n}\n\n# Optimization tips\noptimization_tips = {\n    \"enum_ordering\": \"Order enum values by frequency of use\",\n    \"composite_indexing\": \"Index individual fields, not entire composite\",\n    \"domain_constraints\": \"Keep domain constraints simple and fast\"\n}\n</code></pre>"},{"location":"entities/types/#common-patterns","title":"Common Patterns","text":"<pre><code># Status enum pattern\nstatus_pattern = \"\"\"\nCREATE TYPE entity_status AS ENUM ('active', 'inactive', 'pending', 'archived');\n\nCREATE TABLE entities (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    status entity_status DEFAULT 'pending',\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\"\"\"\n\n# Address composite pattern\naddress_pattern = \"\"\"\nCREATE TYPE address AS (\n    street TEXT,\n    city TEXT,\n    state TEXT,\n    zip_code TEXT,\n    country TEXT DEFAULT 'USA'\n);\n\nCREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    billing_address address,\n    shipping_address address\n);\n\"\"\"\n</code></pre>"},{"location":"entities/views/","title":"Views","text":"<p>PostgreSQL views are virtual tables that provide a dynamic window into data from one or more tables.</p>"},{"location":"entities/views/#postgresql-specification","title":"PostgreSQL Specification","text":""},{"location":"entities/views/#create-view-syntax","title":"CREATE VIEW Syntax","text":"<pre><code>CREATE [ OR REPLACE ] [ TEMP | TEMPORARY ] [ RECURSIVE ] VIEW name [ ( column_name [, ...] ) ]\n    [ WITH ( view_option_name [= view_option_value] [, ... ] ) ]\n    AS query\n    [ WITH [ CASCADED | LOCAL ] CHECK OPTION ]\n</code></pre>"},{"location":"entities/views/#drop-view-syntax","title":"DROP VIEW Syntax","text":"<pre><code>DROP VIEW [ IF EXISTS ] name [, ...] [ CASCADE | RESTRICT ]\n</code></pre> <p>Reference: PostgreSQL 17 CREATE VIEW</p>"},{"location":"entities/views/#pgdelta-support","title":"pgdelta Support","text":""},{"location":"entities/views/#currently-supported","title":"\u2705 Currently Supported","text":"<ul> <li>CREATE VIEW with AS query</li> <li>DROP VIEW</li> <li>CREATE OR REPLACE VIEW</li> <li>Schema-qualified view names</li> </ul> <pre><code>CREATE VIEW \"public\".\"active_users\" AS\nSELECT id, email, created_at\nFROM users\nWHERE is_active = true;\n</code></pre>"},{"location":"entities/views/#not-yet-supported","title":"\u274c Not Yet Supported","text":"<ul> <li>ALTER VIEW operations (planned)</li> <li>RECURSIVE views (planned)</li> <li>Explicit column names (planned)</li> <li>WITH CHECK OPTION (planned)</li> <li>View options (security_barrier, check_option)</li> </ul>"},{"location":"entities/views/#intentionally-not-supported","title":"\ud83d\udeab Intentionally Not Supported","text":"<ul> <li>TEMPORARY views (not persistent schema objects)</li> <li>IF EXISTS/IF NOT EXISTS (pgdelta tracks existence)</li> </ul>"},{"location":"entities/views/#usage-examples","title":"Usage Examples","text":""},{"location":"entities/views/#basic-view-creation","title":"Basic View Creation","text":"<pre><code>target_sql = \"\"\"\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email TEXT,\n    is_active BOOLEAN DEFAULT true\n);\n\nCREATE VIEW active_users AS\nSELECT id, email FROM users WHERE is_active = true;\n\"\"\"\n</code></pre>"},{"location":"entities/views/#view-replacement","title":"View Replacement","text":"<pre><code># Change view definition\nsource_sql = \"\"\"\nCREATE VIEW user_summary AS\nSELECT id, email FROM users;\n\"\"\"\n\ntarget_sql = \"\"\"\nCREATE VIEW user_summary AS\nSELECT id, email, created_at FROM users;\n\"\"\"\n# Results in: CREATE OR REPLACE VIEW \"public\".\"user_summary\" AS ...\n</code></pre>"},{"location":"entities/views/#implementation-details","title":"Implementation Details","text":""},{"location":"entities/views/#view-models","title":"View Models","text":"<pre><code>@dataclass(frozen=True)\nclass CreateView:\n    stable_id: str      # Format: \"v:schema.view_name\"\n    namespace: str      # Schema name\n    relname: str        # View name\n    definition: str     # Complete view definition (AS query)\n\n@dataclass(frozen=True)\nclass ReplaceView:\n    stable_id: str      # Format: \"v:schema.view_name\"\n    namespace: str      # Schema name\n    relname: str        # View name\n    definition: str     # New view definition\n</code></pre>"},{"location":"entities/views/#sql-generation","title":"SQL Generation","text":"<pre><code>def generate_create_view_sql(change: CreateView) -&gt; str:\n    \"\"\"Generate CREATE VIEW SQL.\"\"\"\n    quoted_schema = f'\"{change.namespace}\"'\n    quoted_view = f'\"{change.relname}\"'\n\n    return f'CREATE VIEW {quoted_schema}.{quoted_view} AS {change.definition};'\n\ndef generate_replace_view_sql(change: ReplaceView) -&gt; str:\n    \"\"\"Generate CREATE OR REPLACE VIEW SQL.\"\"\"\n    quoted_schema = f'\"{change.namespace}\"'\n    quoted_view = f'\"{change.relname}\"'\n\n    return f'CREATE OR REPLACE VIEW {quoted_schema}.{quoted_view} AS {change.definition};'\n</code></pre>"},{"location":"entities/views/#future-enhancements","title":"Future Enhancements","text":""},{"location":"entities/views/#planned-features-v020","title":"Planned Features (v0.2.0)","text":"<ul> <li>RECURSIVE views</li> <li>View column aliases</li> <li>WITH CHECK OPTION</li> <li>View dependencies tracking</li> </ul>"},{"location":"entities/views/#best-practices","title":"Best Practices","text":""},{"location":"entities/views/#view-naming","title":"View Naming","text":"<pre><code># Good view names\ngood_names = [\n    \"active_users\",           # Descriptive of content\n    \"user_summary\",           # Summarizes data\n    \"recent_orders\",          # Time-based views\n    \"monthly_sales_report\",   # Reporting views\n]\n\n# Avoid table-like names\navoid_names = [\n    \"users_view\",             # Redundant suffix\n    \"vw_users\",               # Hungarian notation\n    \"temp_users\",             # Confusing with temporary\n]\n</code></pre>"},{"location":"entities/views/#performance-considerations","title":"Performance Considerations","text":"<pre><code># View performance guidelines\nperformance_tips = {\n    \"simple_views\": \"Fast, just query rewriting\",\n    \"complex_joins\": \"Consider materialized views\",\n    \"aggregations\": \"May benefit from indexes on base tables\",\n    \"nested_views\": \"Avoid views of views when possible\"\n}\n</code></pre>"}]}